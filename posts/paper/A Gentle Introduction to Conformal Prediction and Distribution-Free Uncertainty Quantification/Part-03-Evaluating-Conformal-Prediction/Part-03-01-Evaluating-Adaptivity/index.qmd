---
title: "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 3.1)"
subtitle: "3.1. Evaluating Adaptivity"
author: "유성현"
date: "2026-01-16"
categories: [Paper Review]
format:
  html:
    toc: true
    number-sections: false
    code-fold: show
---

# Introduction

지금까지 우리는 Conformal Prediction(CP)을 통해 $1-\alpha$의 커버리지를 보장하는 예측 집합을 만드는 법을 배웠습니다.
하지만 **"평균적으로 90% 정답을 포함한다(Marginal Coverage)"**는 사실만으로는 충분하지 않습니다.

예를 들어, 어떤 의사가 쉬운 환자에게는 100% 정확한 진단을 내리지만, 어려운 희귀병 환자에게는 0%의 정확도를 보인다면 어떨까요?
전체 평균으로는 90% 정확도일지 몰라도, 이는 좋은 시스템이라 할 수 없습니다.

좋은 CP 알고리즘은 **쉬운 입력에는 작은 집합(Small Sets)**을, **어려운 입력에는 큰 집합(Large Sets)**을 출력해야 합니다. 이를 **Adaptivity(적응성)**라고 합니다.
이번 포스트에서는 내 모델이 얼마나 "적응적(Adaptive)"인지 평가하는 구체적인 지표들을 알아봅니다.

# Metric 1: Set Size Distribution

가장 먼저 확인해야 할 지표는 예측 집합 크기(Set Size)의 분포입니다.
단순히 평균 크기만 보는 것이 아니라, **히스토그램**을 그려봐야 합니다.

![Placeholder: Histogram of Set Sizes. X축은 집합의 크기, Y축은 빈도수를 나타낸다. 분포가 넓게 퍼져 있을수록 다양한 난이도의 입력을 잘 구별하고 있다는 의미이다.](images/set_size_histogram.png)

1.  **Average Set Size**: 작을수록 좋습니다. (단, $1-\alpha$ 커버리지를 만족하는 전제하에)
    * 평균이 너무 크다면? $\rightarrow$ 모델 성능이 나쁘거나, Score Function이 효율적이지 않음을 의미합니다.
2.  **Spread of Set Sizes**: 분포가 넓을수록(Dynamic Range가 클수록) 좋습니다.
    * 분포가 넓다는 것은 모델이 "확실한 것(크기 1)"과 "불확실한 것(크기 5 이상)"을 잘 구분하고 있다는 뜻입니다.

# Metric 2: Conditional Coverage

우리가 궁극적으로 원하는 것은 모든 개별 입력 $X$에 대해 커버리지를 보장하는 **Conditional Coverage**입니다.

$$
\mathbb{P}[Y_{test} \in \mathcal{C}(X_{test}) | X_{test}] \ge 1 - \alpha
$$

하지만 현실적으로 모든 $X$값 하나하나에 대해 이를 검증하는 것은 불가능합니다(Impossible in general case).
대신 우리는 데이터를 의미 있는 **그룹(Group)**으로 나누어, 각 그룹별로 커버리지가 잘 지켜지는지 확인해야 합니다.

![Figure 10: Marginal Coverage vs Conditional Coverage 비교. (좌측) 커버리지가 지켜지지 않음. (중앙) Marginal Coverage는 만족하지만 특정 그룹(Group 2)에서 에러가 집중됨. (우측) Conditional Coverage는 모든 그룹에서 고르게 에러가 분산됨.](images/figure10_conditional_coverage.png)

위 그림의 중앙(Marginal) 케이스를 봅시다.
* **Group 1 (Easy)**: 100% 커버리지 (과잉)
* **Group 2 (Hard)**: 80% 커버리지 (부족)
* **전체 평균**: 90% 커버리지 (만족)

이런 경우 "Marginal Coverage는 만족하지만, Conditional Coverage는 실패했다"고 합니다.
이를 잡아내기 위해 다음 두 가지 지표를 사용합니다.

## 2.1 Feature-stratified Coverage (FSC)

데이터의 특성(Feature)에 따라 그룹을 나누어 커버리지를 측정하는 방법입니다.
예를 들어 인종(Race), 나이대(Age), 혹은 이미지의 밝기 등으로 데이터를 그룹화($g=1, \dots, G$)합니다.

각 그룹 $\mathcal{I}_g$에 속한 데이터들의 커버리지를 계산하고, **가장 성능이 안 좋은 그룹(Minimum Coverage)**을 찾습니다.

$$
\text{FSC metric} = \min_{g \in \{1, \dots, G\}} \frac{1}{|\mathcal{I}_g|} \sum_{i \in \mathcal{I}_g} \mathbb{I}\{ Y_i \in \mathcal{C}(X_i) \}
$$

* 만약 완벽한 Conditional Coverage라면, 이 값은 $1-\alpha$에 근접해야 합니다.
* 이 값이 $1-\alpha$보다 현저히 낮다면, 특정 그룹(예: 야간 주행 이미지)에서 모델이 실패하고 있음을 의미합니다.

## 2.2 Size-stratified Coverage (SSC)

하지만 어떤 Feature가 중요한지 모를 때는 어떻게 할까요?
이때 사용할 수 있는 아주 일반적이고 강력한 방법이 **"예측 집합의 크기"**로 그룹을 나누는 것입니다.

* 그룹 1: 집합 크기가 1인 데이터들 ($\mathcal{C}(x)| = 1$)
* 그룹 2: 집합 크기가 2인 데이터들
* ...

모델이 "이건 정말 어려워서 후보를 10개나 뽑았어"라고 말한 경우(집합 크기 10), 실제로도 그 안에 정답이 90% 확률로 들어 있어야 합니다.

$$
\text{SSC metric} = \min_{g \in \{ \text{set sizes} \}} \frac{1}{|\mathcal{I}_g|} \sum_{i \in \mathcal{I}_g} \mathbb{I}\{ Y_i \in \mathcal{C}(X_i) \}
$$

이 지표는 사용자가 사전에 그룹을 정의할 필요가 없으므로(Feature-agnostic), 어떤 상황에서도 바로 적용해볼 수 있는 훌륭한 진단 도구입니다.

# Summary

Conformal Prediction을 평가할 때는 단순히 전체 커버리지만 보지 말고, 다음을 꼭 확인하세요:

1.  **Histogram of Set Sizes**: 쉬운 건 작게, 어려운 건 크게 잘 구분하고 있는가?
2.  **Stratified Coverage (FSC / SSC)**: 특정 그룹(인종, 나이, 혹은 모델이 불확실해하는 그룹)에서만 커버리지가 깨지고 있지는 않은가?

이러한 **Adaptivity** 체크는 실험실 환경을 넘어 실제 서비스(Production)에 CP를 적용할 때 "Non-negotiable(타협할 수 없는)" 필수 검증 단계입니다.

---
**Next Step**: 다음 포스트에서는 이러한 평가 지표들을 실제로 계산할 때 필요한 데이터셋 크기(Calibration Set Size)와 통계적 검증 방법에 대해 알아보겠습니다.