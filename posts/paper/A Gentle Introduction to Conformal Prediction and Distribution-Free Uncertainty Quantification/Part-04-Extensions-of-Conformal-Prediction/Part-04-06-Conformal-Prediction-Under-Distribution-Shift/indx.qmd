---
title: "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.6)"
subtitle: "4.6. Conformal Prediction Under Distribution Shift"
author: "유성현"
date: "2026-01-16"
categories: [Paper Review]
format:
  html:
    toc: true
    number-sections: false
    code-fold: show
---

# Introduction: When Data Changes Over Time

이전 포스트(4.5절)에서는 입력 분포가 변하는 Covariate Shift를 다루었습니다. 하지만 그보다 더 다루기 까다로운 것은 **Distribution Drift(분포 표류)**입니다.

Distribution Drift는 데이터의 분포가 **시간이 지남에 따라 서서히(Slowly varying), 혹은 알 수 없는 방식으로 변하는 현상**을 말합니다.
* **주식 시장**: 10년 전의 시장 상황과 오늘의 시장 상황은 전혀 다릅니다.
* **센서 데이터**: 기계가 노후화되면서 센서의 측정값 분포가 서서히 달라집니다.

이런 시계열(Time-series) 문제에서는 "과거의 모든 데이터가 미래를 예측하는 데 동등하게 중요하다"는 i.i.d. 가정이 성립하지 않습니다. 1년 전 데이터보다 어제의 데이터가 훨씬 중요하기 때문입니다.

이번 포스트에서는 **최신 데이터에 더 큰 가중치**를 부여하는 Weighted Conformal Prediction을 통해 이 문제를 해결하는 방법을 알아봅니다.

# The Method: Weighted Conformal Prediction (Again)

기본적인 아이디어는 4.5절의 Covariate Shift와 동일하게 **Weighted Quantile**을 사용하는 것입니다.
하지만 가중치 $w_i$를 결정하는 방식이 다릅니다. Likelihood Ratio를 계산하는 대신, **시간적 근접성(Recency)**을 기준으로 가중치를 설정합니다.

## Step 1: Define Weight Schedule
사용자는 도메인 지식에 기반하여 "오래된 데이터를 얼마나 잊을 것인가"를 결정하는 가중치 스케줄을 정의합니다.

가장 널리 쓰이는 두 가지 방법은 다음과 같습니다:

1.  **Rolling Window (Sliding Window)**:
    최근 $K$개의 데이터만 사용하고, 나머지는 버립니다.
    $$ w_i^{\text{fixed}} = \mathbb{I}\{i \ge n - K\} $$

2.  **Exponential Decay (Smooth Decay)**:
    과거 데이터의 영향력을 지수적으로 감소시킵니다.
    $$ w_i^{\text{decay}} = \gamma^{n-i+1} \quad (0 < \gamma < 1) $$
    (예: $\gamma = 0.99$라면 바로 직전 데이터는 1, 그 전은 0.99, 그 전은 $0.98 \dots$ 가중치를 가짐)

## Step 2: Normalize Weights
정의된 가중치 $w_i$를 전체 합이 1이 되도록 정규화(Normalize)합니다. 이때 테스트 포인트 $X_{test}$의 가중치 $w_{test}$도 포함하여 계산합니다. (보통 $w_{test}=1$로 둠)

$$
\tilde{w}_i = \frac{w_i}{\sum_{j=1}^{n} w_j + 1}
$$

## Step 3: Weighted Quantile
정규화된 가중치를 사용하여 보정된 분위수(Quantile) $\hat{q}$를 계산합니다.
Calibration Score $s_i$들을 오름차순 정렬했을 때, 누적 가중치 합이 $1-\alpha$를 넘는 지점을 찾습니다.

$$
\hat{q} = \inf \left\{ q : \sum_{i=1}^{n} \tilde{w}_i \mathbb{I}\{s_i \le q\} \ge 1-\alpha \right\}
$$

# Theoretical Analysis

이 방식이 왜 작동하는지 수학적으로 살펴보겠습니다.
Barber et al. (2022)의 연구에 따르면, Weighted CP는 분포 간의 거리(Total Variation Distance)에 비례하는 오차 범위 내에서 커버리지를 보장합니다.

> **Theorem 4 (Conformal prediction under distribution drift)**
>
> $i$번째 Calibration 데이터와 테스트 데이터 사이의 TV 거리(Total Variation Distance)를 $\epsilon_i$라고 하자.
> $$ \epsilon_i = d_{TV}((X_i, Y_i), (X_{test}, Y_{test})) $$
>
> 이때 위에서 정의한 Weighted CP 절차는 다음을 만족한다:
> $$ \mathbb{P}(Y_{test} \in \mathcal{C}(X_{test})) \ge 1 - \alpha - 2 \sum_{i=1}^{n} \tilde{w}_i \epsilon_i $$

### Interpretation
이 부등식의 우변에 있는 **페널티 항 ($2 \sum \tilde{w}_i \epsilon_i$)**을 줄이는 것이 핵심입니다.

* **$\epsilon_i$ (Drift)**: 데이터 $i$가 현재 시점($test$)과 얼마나 다른 분포를 가지는지를 나타냅니다. 오래된 데이터일수록 $\epsilon_i$가 클(1에 가까울) 것입니다.
* **$\tilde{w}_i$ (Weight)**: 우리가 부여한 가중치입니다.

우리의 목표는 **$\epsilon_i$가 큰(오래되어 분포가 달라진) 데이터에 작은 가중치 $\tilde{w}_i$를 부여**하여, 곱 $\tilde{w}_i \epsilon_i$를 0에 가깝게 만드는 것입니다.
즉, **"분포가 많이 변한 데이터는 무시하겠다"**는 전략을 통해 커버리지 손실을 막을 수 있습니다.

![Figure: Distribution Drift 상황에서의 가중치 전략. (위) 데이터 분포가 시간에 따라 파란색에서 초록색으로 변함. (아래) 오래된 데이터(파란색 영역)에 낮은 가중치를 부여함으로써, 현재 시점의 분포(초록색)에 맞는 Quantile을 추정한다.](images/distribution_drift_weights.png)

# Practical Consideration: Effective Sample Size

가중치를 사용하면 분포 변화에는 대응할 수 있지만, 대가가 따릅니다. 바로 **유효 샘플 수(Effective Sample Size)**가 줄어든다는 점입니다.

$$
n^{\text{eff}} = \frac{(\sum w_i)^2}{\sum w_i^2}
$$

* **Uniform Weights ($w_i=1$)**: $n^{\text{eff}} = n$. 모든 데이터를 다 쓰므로 샘플 수가 많아 분산이 작습니다 (안정적).
* **Concentrated Weights**: 최근 데이터에만 큰 가중치를 주면 $n^{\text{eff}}$가 급격히 작아집니다.
    * $n^{\text{eff}}$가 작아지면 $\rightarrow$ 커버리지의 분산(Variance)이 커집니다. (즉, 예측 집합의 크기가 들쑥날쑥해짐)

따라서 **Trade-off**가 존재합니다:
* 가중치를 너무 급격하게 줄이면(빠른 적응) $\rightarrow$ 분포 변화에는 강하지만, 예측이 불안정해짐.
* 가중치를 너무 천천히 줄이면(느린 적응) $\rightarrow$ 예측은 안정적이지만, 이미 변해버린 과거 분포의 영향을 받음.

# Conclusion

Conformal Prediction Under Distribution Drift는 시계열 데이터와 같이 **Non-stationary** 환경에서 신뢰할 수 있는 예측 구간을 만드는 방법입니다.

* 핵심은 **Weighted Quantile**을 사용하는 것입니다.
* 가중치 스케줄(Sliding Window, Decay)을 통해 과거 데이터를 적절히 "망각(Forget)"해야 합니다.
* 이론적으로, 분포 변화가 큰 데이터에 가중치를 적게 줌으로써 커버리지 하락을 방어합니다.
* 하지만 너무 과도한 가중치 조절은 유효 샘플 수를 줄여 불안정성을 초래할 수 있으므로 주의가 필요합니다.

---
**Next Step**: 이제 이론적인 확장은 모두 다루었습니다. 다음 포스트부터는 **Section 5. Worked Examples**로 넘어가서, 실제 데이터셋(Multilabel Classification, Tumor Segmentation 등)에 이 기법들을 어떻게 적용하는지 예제 코드를 통해 구체적으로 살펴보겠습니다.
