---
title: "[Causal Inference] 16. Causal Discovery (Part 6)"
description: "Time Series Causal Discovery"
author: "유성현"
date: "2026-01-22"
categories: [Causal Inference]
format:
  html:
    toc: true
    number-sections: false
    code-fold: show
    math: true
---

# 1. Introduction

시계열 데이터(Time Series Data)는 유전체학(mRNA 발현), 신경심리학(fMRI 신호), 금융, 기상학 등 수많은 과학적 탐구의 기반이 됩니다. 인과추론(Causal Inference) 관점에서 시계열 데이터는 **"원인은 결과보다 시간적으로 선행한다(Temporal Precedence)"**는 강력한 단서를 제공하기 때문에 매력적입니다.

하지만 동시에 시계열 데이터는 다음과 같은 고유한 난제들을 안고 있습니다.

1.  **Time lags & Spurious Associations:** 시차(Time lag)가 존재하며, 과거의 변수들이 현재의 여러 변수에 동시에 영향을 미칠 때 허위 상관(Spurious Association)이 발생하기 쉽습니다.
2.  **Sampling Rate:** 실제 인과 과정보다 데이터 수집 속도가 느릴 경우, 인과 관계가 왜곡되어 보일 수 있습니다.
3.  **Non-stationarity:** 데이터의 분포나 인과 구조 자체가 시간에 따라 변할 수 있습니다.

![Figure: 시계열 인과 구조와 허위 상관의 예시. X1, X2, X3, X4가 시간 축을 따라 전개될 때, 검은색 화살표(Causal links)는 실제 인과관계를 나타내며, 회색 점선 화살표(Spurious associations)는 공통된 과거 원인 등에 의해 관측되는 허위 상관을 의미한다.](./images/time_series_causal_structure.png)

이번 포스트에서는 시계열 데이터에서 인과 구조를 발견(Causal Discovery)하는 대표적인 방법론인 **Granger Causality**, **PCMCI**, **TiMINo**, 그리고 **DYNOTEARS**에 대해 심도 있게 다룹니다.

---

# 2. Granger Causality

## 2.1. Definition and Intuition
Granger Causality는 시계열 인과추론의 가장 고전적이고 기초적인 개념입니다. Clive Granger는 인과성을 **예측 가능성(Predictability)**의 관점에서 정의했습니다.

> **Granger Causality의 정의**:
> 어떤 시계열 $X$의 과거 정보를 포함했을 때, $Y$의 미래를 예측하는 오차(Prediction Error)가 줄어든다면, $X$는 $Y$를 "Granger-cause" 한다고 말합니다.

수식으로 엄밀하게 정의하면 다음과 같습니다. 전체 정보 집합 $V = \{X, Y\} \cup Z$에 대하여, 다음 조건이 성립하면 $X$는 $Y$에 대해 **Granger Non-causal**입니다.

$$
Y_{t+1} \perp\!\!\perp X^t \mid Y^t, Z^t
$$

여기서 $X^t = \{X_1, X_2, \dots, X_t\}$는 시점 $t$까지의 $X$의 모든 과거 정보를 의미합니다. 즉, $Y$와 $Z$의 과거를 모두 알고 있는 상태에서 $X$의 과거 정보가 $Y_{t+1}$의 확률 분포에 아무런 추가 정보를 주지 못한다면 인과관계가 없다고 봅니다. 반대의 경우, $X$는 $Y$를 **Granger-cause** 한다고 합니다.

![Figure: Granger Causality의 기본 아이디어 도식. N과 B라는 두 시계열이 있을 때, 과거의 N값들이 현재의 B값을 설명하는 데 유의미한지, 혹은 그 반대인지를 시차(Lag)를 고려하여 파악하는 구조를 보여준다.](./images/granger_causality_dag.png)

## 2.2. Vector Autoregressive (VAR) Models
실증 분석에서 Granger Causality는 주로 **벡터 자기회귀(Vector Autoregressive, VAR)** 모형을 통해 검정됩니다.

$p$차 VAR 모형, 즉 $VAR(p)$는 다음과 같이 표현됩니다.

### Univariate Case
$$
X_t = c + A_1 X_{t-1} + A_2 X_{t-2} + \dots + A_p X_{t-p} + e_t
$$

### Multivariate Case
다변량 시계열 $X_t = (X_{1,t}, \dots, X_{d,t})^T$에 대하여:
$$
X_{j,t} = \sum_{u=1}^{\infty} \sum_{k=1}^{d} A_{jk,u} X_{k, t-u} + \epsilon_{j,t}
$$

여기서 $A_{jk,u}$는 시차 $u$에서 변수 $k$가 변수 $j$에 미치는 영향력을 나타내는 계수입니다.

### Testing for Causality
VAR 모형에서 $X_i$가 $X_j$를 Granger-cause 하지 않는다는 조건은 다음과 동치입니다.

$$
A_{ji, u} = 0 \quad \text{for all lags } u > 0
$$

즉, $X_j$를 예측하는 식에서 $X_i$의 모든 과거 항들의 계수가 0이어야 합니다. 이는 $X_j$를 전체 과거 정보 $X^{t-1}$로 예측했을 때의 평균제곱오차(MSE)와, $X_i$를 제외한 정보 $X_{-i}^{t-1}$로 예측했을 때의 MSE를 비교하여 검정할 수 있습니다.

$$
\text{var}(X_{j,t} \mid X^{t-1}) = \text{var}(X_{j,t} \mid X_{-i}^{t-1})
$$

::: {.callout-warning}
**주의사항**: Granger Causality는 "인과(Causality)"라는 용어를 사용하지만, 근본적으로는 **예측 유용성**에 관한 개념입니다. 잠재 변수(Confounder)가 존재하거나 시간 집계(Temporal aggregation)가 발생할 경우, 실제 인과관계와 다른 결론을 낼 수 있습니다.
:::

---

# 3. Constraint-Based Approach: PCMCI

## 3.1. Motivation
전통적인 PC 알고리즘(Peter-Clark algorithm)을 시계열에 그대로 적용하면 문제가 발생합니다. 변수의 수가 많고 시차(Lag)가 길어지면 조건부 독립성 검정(Conditional Independence Test)을 수행해야 할 조건부 집합(Conditioning set)의 크기가 지나치게 커집니다. 이는 통계적 검정력(Power)을 떨어뜨리고, 결과적으로 **False Positives(잘못된 인과 발견)**를 양산합니다.

**PCMCI** 알고리즘은 조건부 집합을 최적화하여 검정력은 높이고 False Discovery Rate는 제어하기 위해 고안되었습니다.

## 3.2. Two-Stage Algorithm
PCMCI는 이름에서 알 수 있듯이 **PC 단계($PC_1$)**와 **MCI(Momentary Conditional Independence) 단계**로 구성됩니다.

### Step 1: $PC_1$ Stage (Parent Selection)
이 단계의 목표는 각 변수 $X_t^j$에 대한 **잠정적인 부모 집합(Superset of parents)** $\hat{Pa}(X_t^j)$를 찾는 것입니다.
* 표준 PC 알고리즘의 변형을 사용하되, 인접성(Adjacency)만 파악합니다.
* 모든 가능한 분리 집합(Separator)을 테스트하는 대신, 상관관계가 가장 강한 $k$개의 변수만을 조건부로 사용하여 효율성을 높입니다.
* 이 단계가 끝나면 실제 부모 변수들이 포함되지만, 일부 허위 부모(Spurious parents)들도 포함되어 있을 수 있습니다.

### Step 2: MCI Stage (Refining)
$PC_1$ 단계에서 구한 잠정적 부모 집합을 이용하여, 정밀한 조건부 독립성 검정을 수행합니다. 이를 **Momentary Conditional Independence (MCI)** 검정이라고 합니다.

검정하고자 하는 관계가 $X_{t-\tau}^i \to X_t^j$일 때, 다음의 조건부 독립성을 검정합니다.

$$
X_t^j \perp\!\!\perp X_{t-\tau}^i \mid \hat{Pa}(X_t^j) \setminus \{X_{t-\tau}^i\}, \hat{Pa}(X_{t-\tau}^i)
$$

여기서 주목할 점은 조건부 집합에 $X_t^j$의 부모뿐만 아니라 **원인 변수인 $X_{t-\tau}^i$의 부모 $\hat{Pa}(X_{t-\tau}^i)$**까지 포함시킨다는 것입니다. 이는 시계열 데이터의 자기상관(Autocorrelation)으로 인한 허위 발견을 억제하는 데 핵심적인 역할을 합니다.

![Figure: PCMCI의 MCI 단계 도식. X^j_t에 대한 인과를 검정할 때, 단순히 X^j의 과거값만 조건부로 거는 것이 아니라, 잠재적 원인 변수인 X^i_{t-tau}의 부모 변수들까지 함께 조건부로 걸어줌으로써(Double Conditioning) 시계열의 자기상관 효과를 제거하는 과정을 보여준다.](./images/pcmci_conditioning.png)

## 3.3. Algorithm Detail
MCI 단계에서의 구체적인 절차는 다음과 같습니다.

1.  모든 $i, j$와 시차 $\tau$에 대해 $X_{t-\tau}^i$가 $X_t^j$의 잠정 부모 집합 $\hat{Pa}(X_t^j)$에 속하는지 확인합니다.
2.  속한다면, MCI 조건부 독립성 검정을 수행하여 p-value를 계산합니다.
3.  p-value가 유의수준 $\alpha$보다 크다면(독립이라면), $X_{t-\tau}^i$를 부모 집합에서 제거합니다.

---

# 4. Parametric Approach: TiMINo

## 4.1. Key Idea: Independent Noise
Granger Causality나 PC 알고리즘은 주로 조건부 독립성에 의존하지만, **TiMINo (Time Series Models with Independent Noise)**는 함수적 인과 모형(Functional Causal Model)의 구조적 가정을 활용합니다.

핵심 아이디어는 **"올바른 인과 방향으로 모델을 적합(Fit)했을 때만 잔차(Residual)가 독립적인 노이즈가 된다"**는 것입니다.

## 4.2. TiMINo Definition
시계열 $X_t = (X_t^i)_{i \in V}$가 TiMINo를 만족한다는 것은, 각 변수 $X_t^i$가 자신의 부모 변수들의 함수와 독립적인 노이즈의 결합으로 표현됨을 의미합니다.

$$
X_t^i = f_i \left( (Pa_p^i)_{t-p}, \dots, (Pa_0^i)_t, N_t^i \right)
$$

여기서 $N_t^i$는 모든 $i$와 $t$에 대해 결합 독립(Jointly Independent)입니다.

## 4.3. TiMINo Theorem & Algorithm
Peters et al.은 함수 $f_i$가 식별 가능한 함수 클래스(예: Linear non-Gaussian, Additive Noise Model 등)에 속하거나 시간 구조가 명확할 때, **전체 인과 그래프(Full Time Graph)를 복원할 수 있음**을 증명했습니다.

알고리즘은 다음과 같은 절차를 따릅니다 (Counter-intuitive하지만 효과적임):

1.  모든 변수 $k \in S$에 대해, 나머지 변수들을 입력으로 하는 TiMINo 모델을 적합합니다.
2.  잔차(Residual)가 입력 변수들과 독립인지 검정합니다.
3.  **가장 의존성이 약한(Weakest dependence)**, 즉 독립성 가정에 가장 가까운 변수 $k^*$를 선택합니다. (이 변수는 인과 구조상 가장 하위(Sink)에 위치할 가능성이 높습니다.)
4.  $k^*$를 집합 $S$에서 제거하고, 순서(Order)를 기록합니다.
5.  이 과정을 반복하여 변수들의 위상학적 순서(Topological Order)를 찾고, 불필요한 부모를 제거(Pruning)합니다.

논문에서는 모델 $f_i$로 GAM(Generalized Additive Model), Gaussian Processes 등을 고려했으며, 선형 모형을 사용할 경우 TS-LiNGAM은 TiMINo의 특수한 형태가 됩니다.

---

# 5. Score-Based Approach: DYNOTEARS

## 5.1. From NOTEARS to DYNOTEARS
NOTEARS는 인과 구조 학습(Structure Learning)을 조합 최적화 문제(Combinatorial Optimization)가 아닌 **연속 최적화 문제(Continuous Optimization)**로 변환하여 획기적인 발전을 이룬 알고리즘입니다. **DYNOTEARS**는 이를 시계열 데이터로 확장한 버전입니다.

## 5.2. Structural Vector Autoregression (SVAR)
DYNOTEARS는 구조적 벡터 자기회귀(SVAR) 모형을 가정합니다. $Y_1, \dots, Y_p$를 $X$의 시차(Lagged) 버전 데이터라고 할 때:

$$
X = XW + Y_1 A_1 + \dots + Y_p A_p + Z
$$

* $W$: 동시간대(Contemporaneous) 인과 관계 행렬 (Acyclicity 제약 필요)
* $A_k$: 시차 $k$에서의 인과 관계 행렬 (제약 없음)
* $Z$: 노이즈

## 5.3. Optimization Problem
목표는 데이터 적합도(Data fidelity)를 높이면서, $W$가 DAG(Directed Acyclic Graph) 조건을 만족하도록 하는 $W$와 $A$를 찾는 것입니다.

$$
\min_{W, A} F(W, A) = \underbrace{l(W, A)}_{\text{Loss}} + \underbrace{\lambda_W \|W\|_1 + \lambda_A \|A\|_1}_{\text{Sparsity}} + \underbrace{\frac{\rho}{2} h(W)^2 + \alpha h(W)}_{\text{Augmented Lagrangian for Acyclicity}}
$$

1.  **Loss Function**: $l(W, A) = \frac{1}{2n} \| X - XW - YA \|_F^2$ (Frobenius norm, 최소제곱오차)
2.  **Acyclicity Constraint**: $h(W) = \text{tr}(e^{W \circ W}) - d = 0$. 이 값이 0이면 $W$는 사이클이 없는 그래프입니다.

DYNOTEARS는 시차 데이터($Y$)를 포함하되, 비순환성(Acyclicity) 제약은 동시간대 행렬 $W$에만 적용한다는 점이 핵심입니다 (과거가 현재에 영향을 주는 것은 사이클이 아니기 때문입니다).

---

# 6. Summary

이번 포스트에서는 시계열 인과 구조 발견을 위한 세 가지 주요 접근 방식을 살펴보았습니다.

| 접근 방식 | 대표 알고리즘 | 특징 | 장점 | 단점 |
| :--- | :--- | :--- | :--- | :--- |
| **Constraint-Based** | **PCMCI** | 조건부 독립성 검정 활용 ($PC_1$ + MCI) | 비선형성 처리가능, 직관적 | 샘플이 적을 때 검정력 저하 가능성 |
| **Parametric / Noise** | **TiMINo** | 독립 노이즈 구조 모형 적합 (SEM) | 식별 가능성 보장 (Identifiability) | 함수 형태에 대한 가정 필요 (Model misspecification 위험) |
| **Score-Based** | **DYNOTEARS** | 연속 최적화 문제로 변환 (SVAR + Acyclicity) | 계산 효율성, 대규모 데이터 적용 가능 | 선형성 가정 (기본형), 변수 스케일에 민감할 수 있음 |

각 방법론은 데이터의 특성(샘플 수, 선형성 여부, 노이즈 분포 등)에 따라 장단점이 뚜렷하므로, 실제 연구에서는 데이터에 적합한 가정을 가진 알고리즘을 선택하는 것이 중요합니다.

---

### **Appendix: Checklist for Verification**

1.  **내용 처리 원칙 준수 여부:**
    * [x] Granger Causality의 정의와 한계 (O)
    * [x] PCMCI의 동기(Conditioning set size)와 2단계 절차 (O)
    * [x] TiMINo의 핵심 아이디어(Independent Noise)와 알고리즘 (O)
    * [x] DYNOTEARS의 최적화 식과 Acyclicity 제약 조건 (O)
    * [x] 모든 알고리즘을 서술형으로 풀어서 설명 (O)

2.  **수식 처리 원칙 준수 여부:**
    * [x] Granger Causality ($Y_{t+1} \perp X^t \dots$), VAR 수식 포함 (O)
    * [x] PCMCI 조건부 독립성 검정 수식 포함 (O)
    * [x] TiMINo 함수적 인과 모형 수식 ($X_t^i = f_i(\dots)$) 포함 (O)
    * [x] DYNOTEARS의 목적 함수 $F(W,A)$ 및 제약 조건 $h(W)$ 포함 (O)

3.  **이미지 및 도표 처리 준수 여부:**
    * [x] 시계열 인과 구조 도식 (`time_series_causal_structure.png`) (O)
    * [x] Granger Causality 도식 (`granger_causality_dag.png`) (O)
    * [x] PCMCI Conditioning 도식 (`pcmci_conditioning.png`) (O)
    * [x] 캡션에 그림의 의미와 해석 상세 기술 (O)

4.  **누락된 내용:**
    * 강의 자료 마지막 슬라이드의 "Cyclic Graph" 및 "Combining multiple data sources"는 강의 자료에서도 "Didn't cover"로 명시되어 있어 본문에서 제외함.