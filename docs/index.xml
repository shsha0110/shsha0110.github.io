<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>shsha0110.github.io</title>
<link>https://shsha0110.github.io/</link>
<atom:link href="https://shsha0110.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog built with Quarto</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Wed, 04 Feb 2026 15:00:00 GMT</lastBuildDate>
<item>
  <title>[What If] Chapter 19. Time-Varying Treatments</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/</link>
  <description><![CDATA[ 





<section id="the-causal-effect-of-time-varying-treatments" class="level1">
<h1>19.1. The causal effect of time-varying treatments</h1>
<section id="introduction-고정-처치에서-시변-처치로" class="level2">
<h2 class="anchored" data-anchor-id="introduction-고정-처치에서-시변-처치로">1. Introduction: 고정 처치에서 시변 처치로</h2>
<ul>
<li>지금까지의 인과추론 논의는 주로 <strong>고정 처치(Fixed Treatment)</strong>에 집중해 왔습니다. 즉, 연구 시작 시점(baseline, <img src="https://latex.codecogs.com/png.latex?t=0">)에 처치 여부가 결정되고, 이후에는 변하지 않는 상황을 가정했습니다</li>
<li>하지만 현실 세계의 많은 인과적 질문들은 시간이 지남에 따라 처치 상태가 변하는 <strong>시변 처치(Time-Varying Treatments)</strong>를 포함합니다.</li>
<li>예를 들어 다음과 같은 상황을 생각해 볼 수 있습니다:
<ul>
<li><strong>의학:</strong> 환자가 매달 약을 복용할 수도 있고, 중단할 수도 있음.</li>
<li><strong>생활 습관:</strong> 흡연, 운동 여부는 시간에 따라 달라짐.</li>
<li><strong>사회과학:</strong> 고용 상태, 결혼 여부 등은 생애 주기에 따라 변화함.</li>
</ul></li>
<li>Hernán &amp; Robins의 Chapter 19는 이러한 현실적인 질문을 다루기 위해 기존의 인과추론 프레임워크를 시변 처치로 확장합니다.</li>
<li>본 포스트에서는 그 첫 번째 단계로 시변 처치의 인과 효과를 어떻게 <strong>정의(Definition)</strong>하고 <strong>표기(Notation)</strong>하는지 다룹니다. &gt; <strong>Note:</strong> 저자들은 이 챕터가 책에서 가장 기술적인(technical) 부분 중 하나라고 언급하며, 엄밀함을 잃지 않기 위해 일정 수준의 복잡함이 불가피함을 강조합니다.</li>
</ul>
</section>
<section id="notation-and-concepts" class="level2">
<h2 class="anchored" data-anchor-id="notation-and-concepts">2. Notation and Concepts</h2>
<ul>
<li>시변 처치를 다루기 위해서는 시간을 명시적으로 포함하는 새로운 표기법이 필요합니다.</li>
</ul>
<section id="time-indexing" class="level3">
<h3 class="anchored" data-anchor-id="time-indexing">2.1. Time Indexing</h3>
<ul>
<li>많은 출판된 논문들과의 호환성을 위해 <strong>0-based indexing</strong>을 사용합니다. <img src="https://latex.codecogs.com/png.latex?%0Ak%20=%200,%201,%202,%20...,%20K%0A">
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?k=0">은 추적 관찰(follow-up)이 시작되는 시점이자 첫 번째 처치가 가능한 시점을 의미합니다.</li>
</ul></li>
</ul>
</section>
<section id="time-varying-treatment-a_k" class="level3">
<h3 class="anchored" data-anchor-id="time-varying-treatment-a_k">2.2. Time-Varying Treatment (<img src="https://latex.codecogs.com/png.latex?A_k">)</h3>
<ul>
<li>각 시점 <img src="https://latex.codecogs.com/png.latex?k">에서의 처치 변수를 <img src="https://latex.codecogs.com/png.latex?A_k">로 정의합니다. <img src="https://latex.codecogs.com/png.latex?%0AA_k%20%5Cin%20%5C%7B0,%201%5C%7D%0A">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A_k%20=%201">: 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 처치를 받음 (Treated)</li>
<li><img src="https://latex.codecogs.com/png.latex?A_k%20=%200">: 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 처치를 받지 않음 (Untreated)</li>
</ul></li>
<li>예를 들어, HIV 환자의 5년(<img src="https://latex.codecogs.com/png.latex?K=59">개월) 추적 연구에서 <img src="https://latex.codecogs.com/png.latex?A_k">는 <img src="https://latex.codecogs.com/png.latex?k">번째 달에 항바이러스제 치료를 받았는지 여부를 나타냅니다. 연구 시작 전(<img src="https://latex.codecogs.com/png.latex?k%20%3C%200">)에는 아무도 처치를 받지 않았다고 가정합니다(<img src="https://latex.codecogs.com/png.latex?A_%7B-1%7D=0">).</li>
</ul>
</section>
<section id="treatment-history-overlinea_k" class="level3">
<h3 class="anchored" data-anchor-id="treatment-history-overlinea_k">2.3. Treatment History (<img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D_k">)</h3>
<ul>
<li>시변 처치 인과추론의 핵심은 단일 시점의 처치가 아닌 <strong>처치 이력(History)</strong>을 다룬다는 점입니다.</li>
<li>시점 <img src="https://latex.codecogs.com/png.latex?0">부터 <img src="https://latex.codecogs.com/png.latex?k">까지의 처치 이력을 overbar를 사용하여 다음과 같이 표기합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BA%7D_k%20=%20(A_0,%20A_1,%20...,%20A_k)%0A"></p>
<ul>
<li>전체 추적 기간(<img src="https://latex.codecogs.com/png.latex?K">) 동안의 전체 처치 이력은 아래 첨자를 생략하고 단순히 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D">로 표기하기도 합니다.</li>
</ul>
<section id="예시-처치-전략의-표현" class="level4">
<h4 class="anchored" data-anchor-id="예시-처치-전략의-표현">예시: 처치 전략의 표현</h4>
<ul>
<li><p>전체 기간 동안의 처치 이력 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D">는 개인마다 다르게 나타날 수 있습니다.</p></li>
<li><ol type="1">
<li><strong>Always Treated (<img src="https://latex.codecogs.com/png.latex?%5Coverline%7B1%7D">)</strong>: 연구 기간 내내 처치를 받은 경우 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D%20=%20(1,%201,%20...,%201)%20=%20%5Coverline%7B1%7D"></li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Never Treated (<img src="https://latex.codecogs.com/png.latex?%5Coverline%7B0%7D">)</strong>: 연구 기간 내내 처치를 받지 않은 경우 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D%20=%20(0,%200,%20...,%200)%20=%20%5Coverline%7B0%7D"></li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Intermediate Histories</strong>: 일부 기간만 처치를 받은 경우 (예: <img src="https://latex.codecogs.com/png.latex?1,%200,%201,%200,%20...">)</li>
</ol></li>
</ul>
</section>
</section>
</section>
<section id="defining-the-causal-effect" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-causal-effect">3. Defining the Causal Effect</h2>
<section id="the-limitation-of-fixed-point-contrast" class="level3">
<h3 class="anchored" data-anchor-id="the-limitation-of-fixed-point-contrast">3.1. The Limitation of Fixed-Point Contrast</h3>
<ul>
<li><p>고정 처치 상황에서는 인과 효과를 “처치군(<img src="https://latex.codecogs.com/png.latex?a=1">)의 평균 결과와 대조군(<img src="https://latex.codecogs.com/png.latex?a=0">)의 평균 결과의 차이”로 단순하게 정의할 수 있었습니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BFixed%20Effect%7D%20=%20E%5BY%5E%7Ba=1%7D%5D%20-%20E%5BY%5E%7Ba=0%7D%5D%0A"></p></li>
<li><p>여기서 처치 시점은 <img src="https://latex.codecogs.com/png.latex?k=0"> 하나뿐이므로, 시간에 대한 언급이 필요 없었습니다.</p></li>
<li><p>하지만 시변 처치 상황에서는 단일 시점 <img src="https://latex.codecogs.com/png.latex?k">에서의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">만으로 인과 효과를 정의할 수 없습니다. <img src="https://latex.codecogs.com/png.latex?%0AE%5BY%5E%7Ba_k=1%7D%5D%20-%20E%5BY%5E%7Ba_k=0%7D%5D%20%5Cquad%20(%5Ctext%7BInsufficient%7D)%0A"></p></li>
<li><p>위 식은 나머지 기간(<img src="https://latex.codecogs.com/png.latex?k"> 이외의 시점)의 처치가 어떻게 되는지에 대한 정보를 담고 있지 않기 때문에, 전체 기간에 걸친 시변 처치 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BA%7D">의 효과를 대변하지 못합니다.</p></li>
</ul>
</section>
<section id="contrast-of-counterfactual-outcomes-under-treatment-strategies" class="level3">
<h3 class="anchored" data-anchor-id="contrast-of-counterfactual-outcomes-under-treatment-strategies">3.2. Contrast of Counterfactual Outcomes under Treatment Strategies</h3>
<ul>
<li>따라서 시변 처치의 평균 인과 효과(Average Causal Effect)는 <strong>전체 처치 이력(Treatment History)에 따른 반사실적(Counterfactual) 평균 결과의 차이</strong>로 정의해야 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCausal%20Effect%7D%20=%20E%5BY%5E%7B%5Coverline%7Ba%7D%7D%5D%20-%20E%5BY%5E%7B%5Coverline%7Ba%7D'%7D%5D%0A"></p>
<ul>
<li>여기서:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D">: 특정 처치 전략 (예: 항상 치료 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B1%7D">)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D'">: 비교 대상이 되는 다른 처치 전략 (예: 절대 치료 안 함 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7B0%7D">)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Coverline%7Ba%7D%7D">: 개인이 전략 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D">를 따랐을 때 관측되었을 반사실적 결과(Outcome)</li>
</ul></li>
</ul>
<section id="결과-변수-y" class="level4">
<h4 class="anchored" data-anchor-id="결과-변수-y">결과 변수 (<img src="https://latex.codecogs.com/png.latex?Y">)</h4>
<ul>
<li>편의상 결과 변수 <img src="https://latex.codecogs.com/png.latex?Y">는 추적 관찰이 끝난 시점(<img src="https://latex.codecogs.com/png.latex?K%20+%201%20=%2060">)에 측정된 건강 상태(값이 클수록 좋음)라고 가정합니다. 물론 이 개념은 시변 결과(Time-varying outcome)나 생존 분석(Failure time outcome)에도 적용 가능합니다.</li>
</ul>
</section>
</section>
<section id="non-uniqueness-of-the-effect" class="level3">
<h3 class="anchored" data-anchor-id="non-uniqueness-of-the-effect">3.3. Non-Uniqueness of the Effect</h3>
<ul>
<li><p>중요한 점은 시변 처치에서 “인과 효과”는 유일하게 정의되지 않는다는 것입니다. <img src="https://latex.codecogs.com/png.latex?K=59">일 때, 이분형(dichotomous) 처치만 고려하더라도 가능한 처치 전략의 수는 최소 <img src="https://latex.codecogs.com/png.latex?2%5EK">개에 달합니다. 따라서 우리는 어떤 전략끼리 비교할 것인지 명확히 해야 합니다.</p></li>
<li><p>가장 단순한 비교의 예시는 다음과 같습니다: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BEffect%7D%20=%20E%5BY%5E%7B%5Coverline%7Ba%7D=%5Coverline%7B1%7D%7D%5D%20-%20E%5BY%5E%7B%5Coverline%7Ba%7D=%5Coverline%7B0%7D%7D%5D%0A"></p></li>
<li><p>이는 “연구 기간 내내 치료받는 것”과 “전혀 치료받지 않는 것” 사이의 인과 효과를 의미합니다.</p></li>
<li><p>하지만 다음과 같은 복잡한 비교도 가능합니다:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D%20=%20(1,%200,%201,%200,%20...)">: 격월로 치료</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D'%20=%20(0,%201,%201,%201,%20...)">: 첫 달만 빼고 매달 치료</li>
</ul></li>
</ul>
</section>
</section>
<section id="static-vs.-dynamic-treatment-strategies" class="level2">
<h2 class="anchored" data-anchor-id="static-vs.-dynamic-treatment-strategies">4. Static vs.&nbsp;Dynamic Treatment Strategies</h2>
<ul>
<li>논문의 19.1절 후반부와 19.2절 초반부에서는 처치 전략을 더 확장하여 설명합니다.</li>
</ul>
<section id="static-treatment-strategy" class="level3">
<h3 class="anchored" data-anchor-id="static-treatment-strategy">4.1. Static Treatment Strategy</h3>
<ul>
<li>위에서 언급한 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D=(1,1,...1)"> 처럼, 처치 규칙이 사전에 고정된 값들의 시퀀스로 정해지는 것을 의미합니다. 모든 개인이 동일한 시점에 동일한 규칙을 적용받습니다.</li>
</ul>
</section>
<section id="dynamic-treatment-strategy" class="level3">
<h3 class="anchored" data-anchor-id="dynamic-treatment-strategy">4.2. Dynamic Treatment Strategy</h3>
<ul>
<li>하지만 현실적인 임상 전략은 환자의 상태에 따라 달라질 수 있습니다. 이를 <strong>동적 처치 전략(Dynamic Treatment Strategy)</strong>이라고 합니다.</li>
<li>예를 들어, CD4 세포 수(<img src="https://latex.codecogs.com/png.latex?L_k">)라는 시변 공변량(Time-varying covariate)이 있다고 가정해 봅시다.
<ul>
<li>전략: “CD4 수치가 높으면(<img src="https://latex.codecogs.com/png.latex?L_k=0">) 치료하지 않고, 수치가 낮아지면(<img src="https://latex.codecogs.com/png.latex?L_k=1">) 그때부터 치료를 시작하여 계속 유지한다”.</li>
</ul></li>
<li>이 경우, <img src="https://latex.codecogs.com/png.latex?k"> 시점의 처치 <img src="https://latex.codecogs.com/png.latex?a_k">는 고정된 값이 아니라, 개인의 이전 상태 <img src="https://latex.codecogs.com/png.latex?L_k">의 진화(evolution)에 따라 달라집니다. 이러한 동적 전략까지 포함하면 정의할 수 있는 인과 효과의 종류는 무한히 많아집니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="treatment-strategies" class="level1">
<h1>19.2 Treatment strategies</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<ul>
<li>이전 포스트(Chapter 19.1)에서 우리는 시변 처치(Time-Varying Treatment) 환경에서는 단일 시점의 비교가 불가능하며, <strong>처치 이력(Treatment History)</strong> 전체를 고려해야 함을 배웠습니다.</li>
<li>하지만 단순히 “치료를 받았다/안 받았다”의 이력만으로는 현실의 복잡한 의사결정 과정을 모두 담아낼 수 없습니다. 임상 현장이나 정책 결정에서는 환자의 상태 변화에 따라 유연하게 대처하는 ’규칙’이 필요하기 때문입니다.</li>
<li>이번 포스트에서는 이러한 규칙을 <strong>처치 전략(Treatment Strategies)</strong>이라고 정의하고, 이를 <strong>정적(Static)</strong> 전략과 <strong>동적(Dynamic)</strong> 전략으로 분류하는 방법, 그리고 이를 통해 인과 효과를 어떻게 일반화하여 정의하는지 다룹니다.</li>
</ul>
</section>
<section id="treatment-strategies-definition" class="level2">
<h2 class="anchored" data-anchor-id="treatment-strategies-definition">2. Treatment Strategies: Definition</h2>
<ul>
<li><p><strong>처치 전략(Treatment Strategy)</strong>이란 연구 기간 동안 각 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 처치를 어떻게 할당할지 결정하는 <strong>규칙(Rule)</strong>을 의미합니다. 문헌에 따라 Plan, Policy, Protocol, Regime 등으로 불리기도 합니다.</p></li>
<li><p>일반적으로 전략 <img src="https://latex.codecogs.com/png.latex?g">는 시점 <img src="https://latex.codecogs.com/png.latex?k">에서의 처치 할당 함수들의 집합으로 정의될 수 있습니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ag%20=%20%5Bg_0,%20g_1,%20...,%20g_K%5D%0A"></p>
<ul>
<li>여기서 각 <img src="https://latex.codecogs.com/png.latex?g_k">는 과거의 정보(이력)를 바탕으로 현재의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">를 결정합니다. 이 규칙이 고정되어 있는지, 아니면 변화하는 공변량(Covariate)에 반응하는지에 따라 전략은 크게 두 가지로 나뉩니다.</li>
</ul>
</section>
<section id="static-treatment-strategies-정적-처치-전략" class="level2">
<h2 class="anchored" data-anchor-id="static-treatment-strategies-정적-처치-전략">3. Static Treatment Strategies (정적 처치 전략)</h2>
<section id="정의" class="level3">
<h3 class="anchored" data-anchor-id="정의">3.1. 정의</h3>
<ul>
<li><p><strong>정적 처치 전략(Static Treatment Strategy)</strong>은 처치 할당 규칙이 시간에 따라 변하는 피험자의 상태(Time-varying covariates)에 의존하지 않는 전략입니다. 즉, 연구 시작 시점에 “언제 처치를 하고 언제 안 할지”가 미리 결정되어 있는 시나리오입니다.</p></li>
<li><p>수식으로 표현하면, 시점 <img src="https://latex.codecogs.com/png.latex?k">에서의 처치 결정 규칙 <img src="https://latex.codecogs.com/png.latex?g_k">는 오직 과거의 처치 이력 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D_%7Bk-1%7D">에만 의존하거나, 아예 사전에 고정된 값입니다. <img src="https://latex.codecogs.com/png.latex?%0Ag_k(%5Cbar%7Ba%7D_%7Bk-1%7D)%20%5Crightarrow%20%5C%7B0,%201%5C%7D%0A"></p>
<ul>
<li>Note: <img src="https://latex.codecogs.com/png.latex?L_k"> 등의 공변량이 함수 인자에 포함되지 않음</li>
</ul></li>
</ul>
</section>
<section id="예시" class="level3">
<h3 class="anchored" data-anchor-id="예시">3.2. 예시</h3>
<ul>
<li>가장 대표적인 정적 전략은 다음과 같습니다.
<ul>
<li><strong>“Always treat”</strong>: 연구 기간 내내 치료함. <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D%20=%20(1,%201,%20...,%201)%20=%20%5Coverline%7B1%7D"></li>
<li><strong>“Never treat”</strong>: 연구 기간 내내 치료하지 않음. <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D%20=%20(0,%200,%20...,%200)%20=%20%5Coverline%7B0%7D"></li>
<li><strong>“Alternating”</strong>: 격월로 치료함. <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D%20=%20(1,%200,%201,%200,%20...)"></li>
</ul></li>
</ul>
</section>
<section id="경우의-수" class="level3">
<h3 class="anchored" data-anchor-id="경우의-수">3.3. 경우의 수</h3>
<ul>
<li>이분형(Dichotomous) 처치인 경우에도, 가능한 정적 전략의 수는 <img src="https://latex.codecogs.com/png.latex?2%5EK">개 이상입니다 (각 시점마다 0 또는 1).</li>
<li>하지만 이 <img src="https://latex.codecogs.com/png.latex?2%5EK">개의 전략만으로는 현실의 모든 처치 프로토콜을 설명할 수 없습니다.</li>
</ul>
</section>
</section>
<section id="dynamic-treatment-strategies-동적-처치-전략" class="level2">
<h2 class="anchored" data-anchor-id="dynamic-treatment-strategies-동적-처치-전략">4. Dynamic Treatment Strategies (동적 처치 전략)</h2>
<section id="motivation" class="level3">
<h3 class="anchored" data-anchor-id="motivation">4.1. Motivation</h3>
<ul>
<li>현실, 특히 의학 분야에서는 환자의 상태 변화에 따라 처치를 변경하는 것이 일반적입니다.</li>
<li>예를 들어:
<ul>
<li>환자의 CD4 수치가 일정 수준 이하로 떨어지면 치료를 시작한다.</li>
<li>“약물 독성 반응이 나타나면 치료를 중단한다.”<br>
</li>
</ul></li>
<li>이러한 전략은 사전에 고정된 처치 시퀀스 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D%20=%20(a_0,%20...,%20a_K)">로 표현할 수 없습니다. 왜냐하면 <strong>누가 언제 치료를 시작할지 미리 알 수 없고, 개인의 생체 지표 변화에 따라 결정되기 때문</strong>입니다.</li>
</ul>
</section>
<section id="정의-1" class="level3">
<h3 class="anchored" data-anchor-id="정의-1">4.2. 정의</h3>
<ul>
<li><strong>동적 처치 전략(Dynamic Treatment Strategy)</strong>은 시점 <img src="https://latex.codecogs.com/png.latex?k">에서의 처치 결정이 시간 가변 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">의 진화(evolution)에 의존하는 전략입니다. <img src="https://latex.codecogs.com/png.latex?%0Ag_k(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%5Crightarrow%20%5C%7B0,%201%5C%7D%0A">
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bl%7D_k">는 시점 <img src="https://latex.codecogs.com/png.latex?k">까지의 공변량 이력을 의미합니다.</li>
</ul></li>
</ul>
</section>
<section id="예시-hiv-연구" class="level3">
<h3 class="anchored" data-anchor-id="예시-hiv-연구">4.3. 예시 (HIV 연구)</h3>
<ul>
<li><p><strong>상황</strong>: <img src="https://latex.codecogs.com/png.latex?L_k">는 CD4 세포 수치 (1: 낮음/나쁨, 0: 높음/좋음). 초기에는 모두 <img src="https://latex.codecogs.com/png.latex?L_0=0">.</p></li>
<li><p><strong>전략 <img src="https://latex.codecogs.com/png.latex?g"></strong>: “CD4 수치가 높을 땐(<img src="https://latex.codecogs.com/png.latex?L_k=0">) 치료하지 않다가, 수치가 떨어지면(<img src="https://latex.codecogs.com/png.latex?L_k=1">) 치료를 시작하고 그 이후 계속 치료한다.”</p></li>
<li><p>이 전략 하에서는 모든 피험자가 같은 규칙 <img src="https://latex.codecogs.com/png.latex?g">를 따르지만, 실제 받게 되는 처치 이력 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7Ba%7D">는 개인의 <img src="https://latex.codecogs.com/png.latex?L_k"> 변화에 따라 달라집니다.</p></li>
</ul>
</section>
</section>
<section id="deterministic-vs.-random-strategies-fine-point-19.1" class="level2">
<h2 class="anchored" data-anchor-id="deterministic-vs.-random-strategies-fine-point-19.1">5. Deterministic vs.&nbsp;Random Strategies (Fine Point 19.1)</h2>
<ul>
<li>논문의 <strong>Fine Point 19.1</strong>에서는 전략을 더욱 세밀하게 분류합니다.</li>
<li>우리가 지금까지 논의한 것은 특정 조건에서 처치 여부(0 or 1)가 확실히 정해지는 <strong>결정론적(Deterministic)</strong> 전략이었습니다. 하지만 확률적 요소가 개입된 전략도 존재합니다.</li>
</ul>
<section id="classification-table" class="level3">
<h3 class="anchored" data-anchor-id="classification-table">5.1. Classification Table</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">구분</th>
<th style="text-align: left;">Static (공변량 <img src="https://latex.codecogs.com/png.latex?L"> 무관)</th>
<th style="text-align: left;">Dynamic (공변량 <img src="https://latex.codecogs.com/png.latex?L"> 의존)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Deterministic</strong><br>(결과값: 0 or 1)</td>
<td style="text-align: left;">“항상 치료한다” (<img src="https://latex.codecogs.com/png.latex?%5Coverline%7B1%7D">)<br>“절대 치료 안 한다” (<img src="https://latex.codecogs.com/png.latex?%5Coverline%7B0%7D">)</td>
<td style="text-align: left;">“CD4 수치가 낮으면 치료한다”</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Random</strong><br>(결과값: 확률 <img src="https://latex.codecogs.com/png.latex?p">)</td>
<td style="text-align: left;">“매달 30% 확률로 치료한다”</td>
<td style="text-align: left;">“CD4 수치가 낮으면 30% 확률로, 높으면 0% 확률로 치료한다”</td>
</tr>
</tbody>
</table>
</section>
<section id="random-strategy의-의의" class="level3">
<h3 class="anchored" data-anchor-id="random-strategy의-의의">5.2. Random Strategy의 의의</h3>
<ul>
<li><strong>개념</strong>: <img src="https://latex.codecogs.com/png.latex?g_k(%5Ccdot)">가 0 또는 1의 값이 아니라, 처치를 받을 확률(예: 0.3)을 반환합니다.</li>
<li><strong>용도</strong>: 현실적으로 “최적의 전략(Optimal Strategy)”은 거의 항상 결정론적 동적 전략(예: 독성 생기면 중단)입니다. 하지만, <strong>무작위 배정 임상시험(RCT)</strong> 자체가 하나의 Random Strategy입니다. 어떤 전략이 최적일지 모르는 상태에서 이를 탐색하기 위해 과학적으로 필수적인 전략입니다.</li>
</ul>
</section>
</section>
<section id="defining-causal-effects-with-strategies" class="level2">
<h2 class="anchored" data-anchor-id="defining-causal-effects-with-strategies">6. Defining Causal Effects with Strategies</h2>
<ul>
<li>이제 인과 효과(Causal Effect)를 더 일반화하여 정의할 수 있습니다. 인과 효과는 <strong>두 가지 처치 전략 간의 반사실적 결과(Counterfactual Outcome)의 비교</strong>입니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BCausal%20Effect%7D%20=%20E%5BY%5Eg%5D%20-%20E%5BY%5E%7Bg'%7D%5D%0A">
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?Y%5Eg">는 모든 피험자가 전략 <img src="https://latex.codecogs.com/png.latex?g">를 따랐을 때 관측되었을 잠재적 결과입니다.</li>
</ul></li>
</ul>
<section id="비교의-유연성" class="level3">
<h3 class="anchored" data-anchor-id="비교의-유연성">6.1. 비교의 유연성</h3>
<ul>
<li><ol type="1">
<li><strong>Static vs Static</strong>: “항상 치료” vs “절대 치료 안 함” (<img src="https://latex.codecogs.com/png.latex?E%5BY%5E%7B%5Coverline%7B1%7D%7D%5D%20-%20E%5BY%5E%7B%5Coverline%7B0%7D%7D%5D">)</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Static vs Dynamic</strong>: “항상 치료” vs “CD4 떨어지면 치료”</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Dynamic vs Dynamic</strong>: “CD4 &lt; 500일 때 치료” vs “CD4 &lt; 200일 때 치료”</li>
</ol></li>
</ul>
</section>
<section id="표기법-notation" class="level3">
<h3 class="anchored" data-anchor-id="표기법-notation">6.2. 표기법 (Notation)</h3>
<ul>
<li>저자들은 <img src="https://latex.codecogs.com/png.latex?g">를 일반적인 전략(정적 또는 동적)을 나타내는 기호로 사용합니다.
<ul>
<li>정적 전략을 강조할 때는 <img src="https://latex.codecogs.com/png.latex?Y%5E%7Bg=%5Coverline%7Ba%7D%7D"> 또는 <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Coverline%7Ba%7D%7D">로 표기하기도 합니다.</li>
<li>동적 전략을 포함한 일반적인 경우 <img src="https://latex.codecogs.com/png.latex?Y%5Eg">를 사용합니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="sequentially-randomized-experiments" class="level1">
<h1>19.3 Sequentially randomized experiments</h1>
<section id="introduction-1" class="level2">
<h2 class="anchored" data-anchor-id="introduction-1">1. Introduction</h2>
<ul>
<li><p>이전 섹션(19.2)에서 우리는 시변 처치(Time-Varying Treatment)의 인과 효과를 정의하기 위해 다양한 <strong>처치 전략(Treatment Strategies)</strong>을 고려해야 함을 배웠습니다. 그렇다면 이러한 전략들의 효과를 데이터로부터 어떻게 추정할 수 있을까요?</p></li>
<li><p>고정 처치(Fixed Treatment) 상황에서 무작위 배정(Randomization)이 교란(Confounding)을 제거하는 “Gold Standard”였던 것처럼, 시변 처치 상황에서도 이에 대응하는 실험 설계가 존재합니다. 바로 <strong>순차적 무작위 실험(Sequentially Randomized Experiment)</strong>입니다.</p></li>
<li><p>이번 포스트에서는 세 가지 인과 다이어그램(Causal Diagrams)을 통해 시변 처치 데이터의 생성 과정을 모델링하고, 어떤 조건 하에서 인과 효과가 식별 가능한지(Identifiable) 살펴봅니다.</p></li>
</ul>
</section>
<section id="causal-structures-in-time-varying-settings" class="level2">
<h2 class="anchored" data-anchor-id="causal-structures-in-time-varying-settings">2. Causal Structures in Time-Varying Settings</h2>
<ul>
<li>시변 처치 <img src="https://latex.codecogs.com/png.latex?A_k">, 측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">, 결과 변수 <img src="https://latex.codecogs.com/png.latex?Y">, 그리고 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U_k"> 간의 관계를 나타내는 세 가지 대표적인 시나리오를 살펴보겠습니다. 편의를 위해 시간은 <img src="https://latex.codecogs.com/png.latex?k=0,%201"> 두 시점만 표시합니다.</li>
</ul>
<section id="scenario-1-marginal-randomization-figure-19.1" class="level3">
<h3 class="anchored" data-anchor-id="scenario-1-marginal-randomization-figure-19.1">2.1. Scenario 1: Marginal Randomization (Figure 19.1)</h3>
<ul>
<li><p>첫 번째 시나리오는 가장 단순한 형태의 실험입니다.</p></li>
<li><p><strong>구조적 특징</strong>:</p>
<ul>
<li>측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L">이나 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U">에서 처치 <img src="https://latex.codecogs.com/png.latex?A">로 향하는 화살표가 없습니다.</li>
<li>처치 <img src="https://latex.codecogs.com/png.latex?A_k">는 오직 이전 처치 이력 <img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D">에만 의존하거나, 완전히 무작위로 결정됩니다.</li>
</ul></li>
<li><p><strong>예시 (HIV 연구)</strong>:</p>
<ul>
<li>연구자가 동전 던지기로 처치를 배정합니다.</li>
<li>단, 이전 달에 치료받은 사람(<img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D=1">)은 계속 치료하고(확률 1), 받지 않은 사람(<img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D=0">)은 50% 확률로 치료를 시작합니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_1.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.1: 측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L">과 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U">가 처치 <img src="https://latex.codecogs.com/png.latex?A">에 아무런 영향을 주지 않는 상황. 이는 이상적인 무작위 실험을 나타내며, 측정되거나 측정되지 않은 교란 요인이 모두 존재하지 않는다.</figcaption>
</figure>
</div>
<ul>
<li>이 경우, 정적 처치 전략(Static Strategy) <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">에 대한 반사실적 평균 <img src="https://latex.codecogs.com/png.latex?E%5BY%5E%7B%5Cbar%7Ba%7D%7D%5D">는 단순히 해당 전략을 실제로 따른 사람들의 평균 <img src="https://latex.codecogs.com/png.latex?E%5BY%7CA=%5Cbar%7Ba%7D%5D">와 같습니다.</li>
<li>즉, 별도의 조정(Adjustment) 없이도 인과 효과를 구할 수 있습니다.</li>
</ul>
</section>
<section id="scenario-2-conditional-randomization-figure-19.2" class="level3">
<h3 class="anchored" data-anchor-id="scenario-2-conditional-randomization-figure-19.2">2.2. Scenario 2: Conditional Randomization (Figure 19.2)</h3>
<ul>
<li><p>두 번째 시나리오는 처치 배정이 <strong>측정된 과거 정보</strong>에 의존하는 경우입니다.</p></li>
<li><p><strong>구조적 특징</strong>:</p>
<ul>
<li>측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">에서 처치 <img src="https://latex.codecogs.com/png.latex?A_k">로 향하는 화살표가 <strong>존재합니다</strong>.</li>
<li>하지만 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U_k">에서 처치 <img src="https://latex.codecogs.com/png.latex?A_k">로 향하는 화살표는 <strong>없습니다</strong>.</li>
</ul></li>
<li><p><strong>예시</strong>:</p>
<ul>
<li>의사(또는 연구자)가 환자의 CD4 수치(<img src="https://latex.codecogs.com/png.latex?L_k">)를 보고 처치 여부를 결정합니다.</li>
<li>“CD4 수치가 낮으면(<img src="https://latex.codecogs.com/png.latex?L_k=1">) 80% 확률로 치료하고, 높으면(<img src="https://latex.codecogs.com/png.latex?L_k=0">) 40% 확률로 치료한다.”</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_2.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.2: 측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">가 처치 <img src="https://latex.codecogs.com/png.latex?A_k">에 영향을 주지만, 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U_k">는 처치에 영향을 주지 않는 상황. 이를 ’측정된 변수에 의한 교란(Confounding by measured variables)’만 존재하는 상태라고 한다.</figcaption>
</figure>
</div>
<ul>
<li>이 시나리오에서는 측정된 변수 <img src="https://latex.codecogs.com/png.latex?L">에 의한 교란(Confounding)이 존재합니다.</li>
<li>따라서 단순 평균 비교는 편향되지만, <img src="https://latex.codecogs.com/png.latex?L">에 대한 적절한 조정(G-methods 등)을 통해 인과 효과를 추정할 수 있습니다.</li>
<li>이것이 바로 <strong>순차적 무작위 실험</strong>의 전형적인 모델입니다.</li>
</ul>
</section>
<section id="scenario-3-unmeasured-confounding-figure-19.3" class="level3">
<h3 class="anchored" data-anchor-id="scenario-3-unmeasured-confounding-figure-19.3">2.3. Scenario 3: Unmeasured Confounding (Figure 19.3)</h3>
<ul>
<li><p>세 번째 시나리오는 가장 현실적이면서도 까다로운 상황입니다.</p></li>
<li><p><strong>구조적 특징</strong>:</p>
<ul>
<li>측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">뿐만 아니라, <strong>측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U_k">에서도</strong> 처치 <img src="https://latex.codecogs.com/png.latex?A_k">로 향하는 화살표가 존재합니다.</li>
</ul></li>
<li><p><strong>예시</strong>:</p>
<ul>
<li>환자의 면역 시스템 손상 정도(<img src="https://latex.codecogs.com/png.latex?U_k">)는 CD4 수치(<img src="https://latex.codecogs.com/png.latex?L_k">)와 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 모두 영향을 줍니다.</li>
<li>의사가 CD4 수치 외에도 환자의 안색이나 기력 등 데이터에 기록되지 않는 정보(<img src="https://latex.codecogs.com/png.latex?U_k">)를 보고 처치를 결정합니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_3.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.3: 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?U_k">가 처치 <img src="https://latex.codecogs.com/png.latex?A_k">와 결과 <img src="https://latex.codecogs.com/png.latex?Y">(또는 <img src="https://latex.codecogs.com/png.latex?L">)에 동시에 영향을 주는 상황. 이는 ’측정되지 않은 교란(Unmeasured Confounding)’이 존재하는 상태로, 일반적인 방법으로는 인과 효과를 추정할 수 없다.</figcaption>
</figure>
</div>
<ul>
<li>이 경우, <img src="https://latex.codecogs.com/png.latex?U_k">를 보정할 방법이 없으므로 인과 효과를 편향 없이 추정하는 것은 불가능합니다. <strong>Figure 19.3은 무작위 실험이 될 수 없습니다.</strong></li>
</ul>
</section>
</section>
<section id="sequentially-randomized-experiments-1" class="level2">
<h2 class="anchored" data-anchor-id="sequentially-randomized-experiments-1">3. Sequentially Randomized Experiments</h2>
<section id="정의-definition" class="level3">
<h3 class="anchored" data-anchor-id="정의-definition">3.1. 정의 (Definition)</h3>
<ul>
<li><p><strong>순차적 무작위 실험(Sequentially Randomized Experiment)</strong>이란, 모든 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 처치 <img src="https://latex.codecogs.com/png.latex?A_k">가 연구자(investigator)에 의해 무작위로 할당되되, 그 할당 확률이 <strong>이전의 처치 및 측정된 공변량 이력 <img src="https://latex.codecogs.com/png.latex?(%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k)"></strong>에 의존할 수 있는 실험을 말합니다.</p></li>
<li><p>이를 수식보다는 인과 그래프의 조건으로 표현하면 다음과 같습니다:</p>
<ul>
<li>어떤 시점 <img src="https://latex.codecogs.com/png.latex?k">에서도 측정되지 않은 예후 인자(Prognostic factor) <img src="https://latex.codecogs.com/png.latex?U">로부터 처치 <img src="https://latex.codecogs.com/png.latex?A_k">로 향하는 <img src="https://latex.codecogs.com/png.latex?U%20%5Crightarrow%20A_k"> 직접적인 화살표가 없어야 한다.”</li>
</ul></li>
<li><p>따라서 앞서 살펴본 <strong>Figure 19.1</strong>과 <strong>Figure 19.2</strong>는 순차적 무작위 실험을 나타낼 수 있습니다.</p></li>
</ul>
</section>
<section id="관찰-연구와의-연결-observational-studies" class="level3">
<h3 class="anchored" data-anchor-id="관찰-연구와의-연결-observational-studies">3.2. 관찰 연구와의 연결 (Observational Studies)</h3>
<ul>
<li><p>실제 현실 데이터(Observational Data)는 대부분 Figure 19.2와 Figure 19.3 중 하나에 해당합니다.</p>
<ul>
<li>의사는 환자의 상태(<img src="https://latex.codecogs.com/png.latex?L_k">)를 보고 처방하므로 <img src="https://latex.codecogs.com/png.latex?L_k%20%5Cto%20A_k"> 화살표는 존재합니다.</li>
<li>문제는 <img src="https://latex.codecogs.com/png.latex?U_k%20%5Cto%20A_k"> 화살표의 존재 여부입니다.</li>
</ul></li>
<li><p>관찰 연구에서 인과 추론을 수행한다는 것은, <strong>“우리의 연구가 Figure 19.3이 아니라 Figure 19.2를 따르고 있다”</strong>고 가정하는 것과 같습니다. 즉, 처치 결정에 영향을 미치는 모든 변수를 측정하였다고 가정하는 것입니다.</p></li>
<li><p>하지만 불행히도, 데이터만 가지고는 이 연구가 Figure 19.2인지 19.3인지 경험적으로 검증할(Empirically test) 방법은 없습니다. 이는 연구자의 도메인 지식에 기반한 가정(Assumption)의 영역입니다.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="sequential-exchangeability" class="level1">
<h1>19.4 Sequential exchangeability</h1>
<section id="introduction-2" class="level2">
<h2 class="anchored" data-anchor-id="introduction-2">1. Introduction</h2>
<ul>
<li>이전 포스트(Chapter 19.3)에서 우리는 <strong>순차적 무작위 실험(Sequentially Randomized Experiment)</strong>을 통해 시변 처치의 인과 효과를 편향 없이 추정할 수 있음을 배웠습니다. 하지만 현실의 데이터는 대부분 관찰 연구(Observational Study)에서 나옵니다.</li>
<li>고정 처치(Time-fixed treatment) 문제에서 관찰 데이터를 마치 무작위 실험처럼 다루기 위해 <strong>조건부 교환 가능성(Conditional Exchangeability)</strong> 가정을 도입했던 것처럼, 시변 처치 문제에서도 이에 상응하는 가정이 필요합니다.</li>
<li>이번 포스트에서는 시변 처치 인과추론의 가장 중요한 식별 가정인 <strong>순차적 교환 가능성(Sequential Exchangeability)</strong>을 정의하고, 복잡한 시변 구조에서 이 가정이 성립하는지 판별하기 위해 <strong>SWIG(Single World Intervention Graphs)</strong>를 어떻게 활용하는지 살펴봅니다.</li>
</ul>
</section>
<section id="from-conditional-to-sequential-exchangeability" class="level2">
<h2 class="anchored" data-anchor-id="from-conditional-to-sequential-exchangeability">2. From Conditional to Sequential Exchangeability</h2>
<section id="recap-fixed-treatment" class="level3">
<h3 class="anchored" data-anchor-id="recap-fixed-treatment">2.1. Recap: Fixed Treatment</h3>
<ul>
<li>고정 처치 <img src="https://latex.codecogs.com/png.latex?A">에 대한 인과 효과를 식별하기 위해서는, 측정된 공변량 <img src="https://latex.codecogs.com/png.latex?L">이 주어졌을 때 처치가 배정되는 메커니즘이 잠재적 결과(Counterfactual Outcome) <img src="https://latex.codecogs.com/png.latex?Y%5Ea">와 독립이어야 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%5Ea%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A%20%5Cmid%20L%20%5Cquad%20%5Ctext%7Bfor%20all%20%7D%20a%0A"></p>
<ul>
<li>이 조건이 성립하면 우리는 <img src="https://latex.codecogs.com/png.latex?L">을 통제(Adjustment)함으로써 인과 효과를 계산할 수 있습니다.</li>
</ul>
</section>
<section id="the-need-for-sequential-exchangeability" class="level3">
<h3 class="anchored" data-anchor-id="the-need-for-sequential-exchangeability">2.2. The Need for Sequential Exchangeability</h3>
<ul>
<li><p>시변 처치 상황에서는 처치가 한 번(<img src="https://latex.codecogs.com/png.latex?k=0">)이 아니라 여러 시점(<img src="https://latex.codecogs.com/png.latex?k=0,%201,%20...,%20K">)에 걸쳐 발생합니다. 따라서 “교환 가능성” 조건도 모든 시점에서 성립해야 합니다.</p></li>
<li><p>순차적 무작위 실험에서는 연구자가 <img src="https://latex.codecogs.com/png.latex?k"> 시점의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">를 과거 이력(History)에 기반하여 무작위로 배정하므로, 다음의 조건이 자연스럽게 성립합니다.</p></li>
</ul>
<blockquote class="blockquote">
<p>“어떤 시점 <img src="https://latex.codecogs.com/png.latex?k">에서도, 과거의 처치 및 공변량 이력이 주어졌을 때, 미래의 잠재적 결과는 현재의 처치 할당과 독립적이다.”</p>
</blockquote>
<ul>
<li>이를 수식화한 것이 바로 <strong>순차적 교환 가능성</strong>입니다.</li>
</ul>
</section>
</section>
<section id="defining-sequential-exchangeability" class="level2">
<h2 class="anchored" data-anchor-id="defining-sequential-exchangeability">3. Defining Sequential Exchangeability</h2>
<section id="for-static-strategies" class="level3">
<h3 class="anchored" data-anchor-id="for-static-strategies">3.1. For Static Strategies</h3>
<ul>
<li>정적 처치 전략(Static Strategy) <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D%20=%20(a_0,%20a_1,%20...,%20a_K)">에 대한 인과 효과를 추정하기 위해서는 다음 조건이 모든 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 성립해야 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%5E%7B%5Cbar%7Ba%7D%7D%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k%20%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D%20=%20%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%20=%20%5Cbar%7Bl%7D_k%0A"></p>
<section id="수식의-의미-해석" class="level4">
<h4 class="anchored" data-anchor-id="수식의-의미-해석">수식의 의미 해석</h4>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D"></strong>: 전체 처치 계획 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">를 끝까지 따랐을 때의 잠재적 결과.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D%20=%20%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%20=%20%5Cbar%7Bl%7D_k"></strong>: 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 처치 <img src="https://latex.codecogs.com/png.latex?A_k">를 결정하기 직전까지 우리가 알고 있는 모든 정보(과거의 처치 이력과 공변량 이력)를 조건부로 합니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k"></strong>: 그 조건 하에서, <img src="https://latex.codecogs.com/png.latex?k"> 시점의 실제 처치 <img src="https://latex.codecogs.com/png.latex?A_k">는 잠재적 결과와 무관하게(마치 동전 던지기처럼) 결정되어야 합니다. 즉, <strong>“측정되지 않은 교란 요인(Unmeasured Confounding)이 <img src="https://latex.codecogs.com/png.latex?A_k">에 영향을 주지 않는다”</strong>는 뜻입니다.</li>
</ul>
</section>
</section>
<section id="for-dynamic-strategies" class="level3">
<h3 class="anchored" data-anchor-id="for-dynamic-strategies">3.2. For Dynamic Strategies</h3>
<ul>
<li>동적 전략 <img src="https://latex.codecogs.com/png.latex?g">에 대해서도 유사한 형태의 가정이 필요합니다.</li>
<li>전략 <img src="https://latex.codecogs.com/png.latex?g">가 주어졌을 때의 잠재적 결과를 <img src="https://latex.codecogs.com/png.latex?Y%5Eg">라고 할 때:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%5Eg%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k%20%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D%20=%20g(%5Cbar%7BA%7D_%7Bk-2%7D,%20%5Cbar%7BL%7D_%7Bk-1%7D),%20%5Cbar%7BL%7D_k%0A"></p>
<ul>
<li>이 조건은 정적 전략의 경우와 구조적으로 동일하지만, 조건부로 하는 과거 처치 이력 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BA%7D_%7Bk-1%7D">이 전략 <img src="https://latex.codecogs.com/png.latex?g">에 의해 결정된 경로를 따르고 있다는 점이 다릅니다.</li>
</ul>
</section>
</section>
<section id="observational-studies-and-figure-19.4" class="level2">
<h2 class="anchored" data-anchor-id="observational-studies-and-figure-19.4">4. Observational Studies and Figure 19.4</h2>
<ul>
<li>순차적 교환 가능성은 가정일 뿐, 데이터 자체에서 증명할 수 없습니다. 연구자는 자신의 연구 설계와 도메인 지식을 바탕으로 이 가정이 성립하는지 판단해야 합니다. 이를 위해 인과 그래프(DAG)를 활용합니다.</li>
</ul>
<section id="the-ideal-case-figure-19.2-revisited" class="level3">
<h3 class="anchored" data-anchor-id="the-ideal-case-figure-19.2-revisited">4.1. The Ideal Case (Figure 19.2 Revisited)</h3>
<ul>
<li>앞서 본 Figure 19.2는 <strong>순차적 무작위 실험</strong> 혹은 이상적인 관찰 연구를 나타냅니다.</li>
<li>처치 <img src="https://latex.codecogs.com/png.latex?A_k">는 과거의 이력(<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k">)에 영향을 받습니다.</li>
<li>하지만 <strong>측정되지 않은 변수 <img src="https://latex.codecogs.com/png.latex?U">가 <img src="https://latex.codecogs.com/png.latex?A_k">에 직접적인 영향을 주지 않습니다.</strong></li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BL%7D_k">를 통제하면 <img src="https://latex.codecogs.com/png.latex?A_k">는 <img src="https://latex.codecogs.com/png.latex?Y">와 조건부 독립이 되며, 순차적 교환 가능성이 성립합니다.</li>
</ul>
</section>
<section id="the-realistic-case-figure-19.4" class="level3">
<h3 class="anchored" data-anchor-id="the-realistic-case-figure-19.4">4.2. The Realistic Case (Figure 19.4)</h3>
<ul>
<li>하지만 많은 관찰 연구는 Figure 19.4와 같이 더 복잡한 구조를 가집니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_4.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.4: 측정되지 않은 변수 <img src="https://latex.codecogs.com/png.latex?W_k">가 처치 <img src="https://latex.codecogs.com/png.latex?A_k">와 미래의 공변량 <img src="https://latex.codecogs.com/png.latex?L_%7Bt%3Ek%7D">에 동시에 영향을 미치는 구조. 이는 Figure 19.2와 달리 순차적 교환 가능성 판단을 어렵게 만든다.</figcaption>
</figure>
</div>
<ul>
<li>Figure 19.4의 핵심 특징은 다음과 같습니다:
<ul>
<li><ol type="1">
<li><strong>공통 원인 존재</strong>: 측정되지 않은 변수(<img src="https://latex.codecogs.com/png.latex?W_k"> 혹은 <img src="https://latex.codecogs.com/png.latex?U">)가 <strong>현재의 처치 <img src="https://latex.codecogs.com/png.latex?A_k"></strong>와 <strong>미래의 공변량 <img src="https://latex.codecogs.com/png.latex?L_%7Bk+1%7D"></strong>의 공통 원인으로 작용합니다. *2. <strong>구조적 차이</strong>:</li>
</ol>
<ul>
<li>Figure 19.2에서는 <img src="https://latex.codecogs.com/png.latex?L_k">만 잘 측정하면 되었지만,</li>
<li>Figure 19.4에서는 <img src="https://latex.codecogs.com/png.latex?A_k">와 <img src="https://latex.codecogs.com/png.latex?L_%7Bk+1%7D">을 연결하는 뒷문 경로(Backdoor path)가 측정되지 않은 변수를 통해 형성됩니다.</li>
</ul></li>
</ul></li>
<li>이러한 구조에서는 <strong>어떤 처치 전략을 평가하느냐</strong>에 따라 식별 가능성(Identifiability)이 달라질 수 있습니다.</li>
<li><strong>정적 전략(<img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">)</strong>의 경우, 여전히 순차적 교환 가능성이 성립할 수도 있습니다.</li>
<li>하지만 <strong>동적 전략(<img src="https://latex.codecogs.com/png.latex?g">)</strong>의 경우, 미래 공변량 <img src="https://latex.codecogs.com/png.latex?L_%7Bk+1%7D">의 분포를 알아야 하는데, 이것이 측정되지 않은 변수에 의해 교란되어 있다면 식별이 불가능할 수 있습니다.</li>
</ul>
</section>
</section>
<section id="other-key-assumptions-technical-point-19.2" class="level2">
<h2 class="anchored" data-anchor-id="other-key-assumptions-technical-point-19.2">5. Other Key Assumptions (Technical Point 19.2)</h2>
<ul>
<li>시변 처치 인과추론이 성립하기 위해서는 순차적 교환 가능성 외에도 <strong>일관성(Consistency)</strong>과 <strong>양수성(Positivity)</strong> 가정이 시변(Time-varying) 문맥에 맞게 엄밀하게 확장되어야 합니다.</li>
</ul>
<section id="sequential-consistency-순차적-일관성" class="level3">
<h3 class="anchored" data-anchor-id="sequential-consistency-순차적-일관성">5.1. Sequential Consistency (순차적 일관성)</h3>
<ul>
<li>고정 처치에서의 일관성 가정을 확장하여, 시변 처치에서는 처치 전략(Strategy)과 관찰된 이력(History) 사이의 관계를 다음과 같이 정의합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AY%5E%7B%5Cbar%7Ba%7D%7D%20&amp;=%20Y%5E%7B%5Cbar%7Ba%7D%5E*%7D%20%5Cquad%20&amp;%5Ctext%7B%20if%20%7D%20%5Cbar%7Ba%7D%5E*%20=%20%5Cbar%7Ba%7D%20%5C%5C%0AY%5E%7B%5Cbar%7Ba%7D%7D%20&amp;=%20Y%20%5Cquad%20&amp;%5Ctext%7B%20if%20%7D%20%5Cbar%7BA%7D%20=%20%5Cbar%7Ba%7D%20%5C%5C%0A%5Cbar%7BL%7D_%7Bk%7D%5E%7B%5Cbar%7Ba%7D%7D%20&amp;=%20%5Cbar%7BL%7D_%7Bk%7D%20%5Cquad%20&amp;%5Ctext%7B%20if%20%7D%20%5Cbar%7BA%7D_%7Bk-1%7D%20=%20%5Cbar%7Ba%7D_%7Bk-1%7D%0A%5Cend%7Baligned%7D%0A"></p>
<section id="수식-설명" class="level4">
<h4 class="anchored" data-anchor-id="수식-설명"><strong>수식 설명:</strong></h4>
<ul>
<li><ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D%20=%20Y%5E%7B%5Cbar%7Ba%7D%5E*%7D%20%5Ctext%7B%20if%20%7D%20%5Cbar%7Ba%7D%5E*%20=%20%5Cbar%7Ba%7D"></strong>: 우리가 정의한 처치 전략 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">가 동일하다면, 그에 따른 잠재적 결과도 유일하게 정의된다는 뜻입니다. (Well-defined counterfactuals)</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D%20=%20Y%20%5Ctext%7B%20if%20%7D%20%5Cbar%7BA%7D%20=%20%5Cbar%7Ba%7D"></strong>: 만약 현실에서 관찰된 처치 이력 전체(<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BA%7D">)가 특정 전략 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">와 일치한다면, 그 개인의 관찰된 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 해당 전략 하에서의 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D">와 같아야 합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cbar%7BL%7D_%7Bk%7D%5E%7B%5Cbar%7Ba%7D%7D%20=%20%5Cbar%7BL%7D_%7Bk%7D%20%5Ctext%7B%20if%20%7D%20%5Cbar%7BA%7D_%7Bk-1%7D%20=%20%5Cbar%7Ba%7D_%7Bk-1%7D"></strong>: 만약 시점 <img src="https://latex.codecogs.com/png.latex?k">까지의 처치 이력이 같다면, 그 시점까지 관찰된 공변량도 잠재적 공변량과 일치해야 합니다.</li>
</ol></li>
</ul>
</section>
</section>
<section id="sequential-positivity-순차적-양수성" class="level3">
<h3 class="anchored" data-anchor-id="sequential-positivity-순차적-양수성">5.2. Sequential Positivity (순차적 양수성)</h3>
<ul>
<li>양수성 가정은 고정 처치에서의 조건(<img src="https://latex.codecogs.com/png.latex?f(a%7Cl)%20%3E%200">)을 시변 공변량과 처치 이력을 포함한 조건부 확률밀도 함수로 일반화하여 다음과 같이 표현합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BIf%20%7D%20f_%7B%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%7D(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%5Cneq%200,%20%5Ctext%7B%20then%20%7D%20f_%7BA_k%20%7C%20%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%7D(a_k%20%7C%20%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%3E%200%0A"></p>
<section id="수식-설명-1" class="level4">
<h4 class="anchored" data-anchor-id="수식-설명-1"><strong>수식 설명:</strong></h4>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?f_%7B%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%7D(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%5Cneq%200"></strong>: 특정 시점 <img src="https://latex.codecogs.com/png.latex?k">까지의 처치 및 공변량 이력 <img src="https://latex.codecogs.com/png.latex?(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)">이 모집단 내에서 실제로 발생했다면(밀도가 0이 아니라면),</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?f_%7BA_k%20%7C%20%5Cdots%7D(a_k%20%7C%20%5Cdots)%20%3E%200"></strong>: 그 이력을 가진 사람들은 다음 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 가능한 모든 처치 옵션 <img src="https://latex.codecogs.com/png.latex?a_k">를 선택할 확률(밀도)을 0보다 크게 가져야 합니다.</li>
<li><strong>의미</strong>: 과거에 어떤 경로를 밟아왔든 간에, 현재 시점에서 특정 처치를 받을 확률이 아예 없어서는 안 된다는 뜻입니다. 만약 특정 상태에서 치료가 불가능하다면(구조적 0), 해당 데이터로는 인과 효과를 비교할 수 없습니다.</li>
</ul>
<hr>
</section>
</section>
</section>
</section>
<section id="identifiability-under-some-but-not-all-treatment-strategies" class="level1">
<h1>19.5. Identifiability under some but not all treatment strategies</h1>
<section id="introduction-3" class="level2">
<h2 class="anchored" data-anchor-id="introduction-3">1. Introduction</h2>
<ul>
<li><p>이전 섹션들에서 우리는 시변 처치(Time-Varying Treatment)의 인과 효과를 추정하기 위해 <strong>순차적 교환 가능성(Sequential Exchangeability)</strong> 가정이 필요함을 배웠습니다.</p></li>
<li><p>하지만 인과 구조(Causal Structure)에 따라 이 가정이 “모든 전략”에 대해 성립할 수도 있고, “일부 전략”에 대해서만 성립할 수도 있다는 점은 매우 중요합니다. 특히, <strong>정적 전략(Static Strategy, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">)</strong>에 대한 효과는 계산할 수 있지만, <strong>동적 전략(Dynamic Strategy, <img src="https://latex.codecogs.com/png.latex?g">)</strong>에 대한 효과는 계산할 수 없는 상황이 존재합니다.</p></li>
<li><p>이번 포스트에서는 이러한 비대칭적인 식별 가능성(Identifiability)이 발생하는 원인을 인과 그래프(DAG)와 SWIG를 통해 분석하고, 이를 뒷받침하는 추가 가정인 <strong>일관성(Consistency)</strong>과 <strong>양수성(Positivity)</strong>을 시변 처치 맥락으로 확장해 봅니다.</p></li>
</ul>
</section>
<section id="key-assumptions-consistency-and-positivity" class="level2">
<h2 class="anchored" data-anchor-id="key-assumptions-consistency-and-positivity">2. Key Assumptions: Consistency and Positivity</h2>
<ul>
<li>식별 가능성 논의에 앞서, 인과 추론의 기본 가정이 시변 처치 환경에서 어떻게 확장되는지 정의해야 합니다. (Technical Point 19.2)</li>
</ul>
<section id="consistency-일관성" class="level3">
<h3 class="anchored" data-anchor-id="consistency-일관성">2.1. Consistency (일관성)</h3>
<ul>
<li>고정 처치에서의 일관성 가정(“처치 <img src="https://latex.codecogs.com/png.latex?A=a">를 받으면 관측된 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y%5Ea">와 같다”)은 시변 처치에서 전체 이력(History)에 대한 조건으로 확장됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BIf%20%7D%20%5Cbar%7BA%7D%20=%20%5Cbar%7Ba%7D,%20%5Ctext%7B%20then%20%7D%20Y%5E%7B%5Cbar%7Ba%7D%7D%20=%20Y%0A"></p>
<ul>
<li>즉, 개인의 실제 처치 이력 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BA%7D">가 특정 전략 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">와 일치한다면, 그 개인의 관측된 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 해당 전략 하의 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D">와 동일해야 합니다.</li>
<li>동적 전략 <img src="https://latex.codecogs.com/png.latex?g">에 대해서도, 실제 이력이 <img src="https://latex.codecogs.com/png.latex?g">가 지시하는 바와 일치한다면 <img src="https://latex.codecogs.com/png.latex?Y%5Eg%20=%20Y">가 성립합니다.</li>
</ul>
</section>
<section id="positivity-양수성" class="level3">
<h3 class="anchored" data-anchor-id="positivity-양수성">2.2. Positivity (양수성)</h3>
<ul>
<li>양수성 가정은 데이터 내에서 모든 가능한 처치 경로를 관측할 수 있어야 함을 의미합니다.</li>
<li>시변 처치에서는 “과거의 어떤 이력 <img src="https://latex.codecogs.com/png.latex?(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)">이 주어지더라도, 현재 시점 <img src="https://latex.codecogs.com/png.latex?k">에서 가능한 모든 처치 <img src="https://latex.codecogs.com/png.latex?a_k">를 받을 확률이 0보다 커야 한다”로 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BIf%20%7D%20f_%7B%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%7D(%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%5Cneq%200,%20%5Ctext%7B%20then%20%7D%20f_%7BA_k%20%7C%20%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%7D(a_k%20%7C%20%5Cbar%7Ba%7D_%7Bk-1%7D,%20%5Cbar%7Bl%7D_k)%20%3E%200%0A"></p>
<ul>
<li>만약 특정 전략 <img src="https://latex.codecogs.com/png.latex?g">에만 관심이 있다면, 모든 경로가 아닌 <img src="https://latex.codecogs.com/png.latex?g">와 호환되는 경로(compatible history)에 대해서만 이 조건이 성립하면 됩니다.</li>
</ul>
</section>
</section>
<section id="scenario-analysis-when-can-we-identify-effects" class="level2">
<h2 class="anchored" data-anchor-id="scenario-analysis-when-can-we-identify-effects">3. Scenario Analysis: When Can We Identify Effects?</h2>
<ul>
<li>이제 세 가지 대표적인 인과 구조(DAG)를 통해 식별 가능성의 차이를 살펴보겠습니다.</li>
</ul>
<section id="scenario-1-full-identifiability-figure-19.5" class="level3">
<h3 class="anchored" data-anchor-id="scenario-1-full-identifiability-figure-19.5">3.1. Scenario 1: Full Identifiability (Figure 19.5)</h3>
<ul>
<li>첫 번째 시나리오는 측정되지 않은 공변량 <img src="https://latex.codecogs.com/png.latex?W_k">가 존재하지만, 이것이 처치 <img src="https://latex.codecogs.com/png.latex?A_k">와 <em>미래의</em> 공변량 <img src="https://latex.codecogs.com/png.latex?L_%7Bt%3Ek%7D">에만 영향을 주는 경우입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_5.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.5: 측정되지 않은 변수 <img src="https://latex.codecogs.com/png.latex?W_0">가 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?L_1">에 영향을 주지만, 순차적 교환 가능성을 해치지 않는 구조.</figcaption>
</figure>
</div>
<ul>
<li><strong>해석</strong>: 이 구조에서는 <img src="https://latex.codecogs.com/png.latex?W_k">가 “과거 처치와 미래 공변량의 공통 원인” 역할을 합니다. 하지만 우리가 관심을 갖는 <img src="https://latex.codecogs.com/png.latex?k"> 시점의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">에 대한 교란 요인으로는 작용하지 않거나(적절히 통제됨), 순차적 교환 가능성 조건을 만족시킵니다.</li>
<li><strong>결론</strong>: 이 경우, 동적 전략 <img src="https://latex.codecogs.com/png.latex?g">에 대한 순차적 교환 가능성(Sequential Exchangeability for <img src="https://latex.codecogs.com/png.latex?Y%5Eg">)이 성립합니다. <img src="https://latex.codecogs.com/png.latex?%0A%20%20Y%5Eg%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k%20%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%0A%20%20"> 따라서 <strong>정적 전략(Static)과 동적 전략(Dynamic) 모두에 대한 인과 효과를 식별할 수 있습니다.</strong></li>
</ul>
</section>
<section id="scenario-2-partial-identifiability-figure-19.6" class="level3">
<h3 class="anchored" data-anchor-id="scenario-2-partial-identifiability-figure-19.6">3.2. Scenario 2: Partial Identifiability (Figure 19.6)</h3>
<ul>
<li>두 번째 시나리오는 매우 흥미롭고 주의가 필요한 경우입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_6.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.6: <img src="https://latex.codecogs.com/png.latex?W_0">가 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?L_1">의 공통 원인인 구조. 정적 전략은 식별 가능하나, 동적 전략은 식별 불가능하다.</figcaption>
</figure>
</div>
<ul>
<li><strong>구조적 특징</strong>: 측정되지 않은 변수 <img src="https://latex.codecogs.com/png.latex?W_0">가 <img src="https://latex.codecogs.com/png.latex?A_0">(과거 처치)와 <img src="https://latex.codecogs.com/png.latex?L_1">(미래 공변량)에 동시에 영향을 줍니다. (<img src="https://latex.codecogs.com/png.latex?A_0%20%5Cleftarrow%20W_0%20%5Crightarrow%20L_1">)</li>
<li><strong>정적 전략 (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D">)</strong>: <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D">에 대해서는 순차적 교환 가능성이 성립합니다. <img src="https://latex.codecogs.com/png.latex?L_1">을 조건부로 통제하면 <img src="https://latex.codecogs.com/png.latex?A_1">의 배정은 무작위라고 볼 수 있기 때문입니다.</li>
<li><strong>동적 전략 (<img src="https://latex.codecogs.com/png.latex?g">)</strong>: 문제가 발생합니다. 동적 전략 <img src="https://latex.codecogs.com/png.latex?g">는 <img src="https://latex.codecogs.com/png.latex?L_1">의 값에 따라 <img src="https://latex.codecogs.com/png.latex?A_1">을 결정합니다 (예: <img src="https://latex.codecogs.com/png.latex?L_1=1">이면 치료). 따라서 <img src="https://latex.codecogs.com/png.latex?Y%5Eg">의 분포를 알기 위해서는 <img src="https://latex.codecogs.com/png.latex?L_1%5Eg">(전략 <img src="https://latex.codecogs.com/png.latex?g"> 하에서의 <img src="https://latex.codecogs.com/png.latex?L_1"> 분포)를 알아야 합니다.
<ul>
<li>하지만 <img src="https://latex.codecogs.com/png.latex?L_1">은 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?W_0">의 영향을 받습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?L_1"> 사이에는 <img src="https://latex.codecogs.com/png.latex?A_0%20%5Cleftarrow%20W_0%20%5Crightarrow%20L_1">이라는 뒷문 경로(Backdoor path)가 열려 있습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?W_0">를 측정하지 못했으므로, <img src="https://latex.codecogs.com/png.latex?A_0">가 <img src="https://latex.codecogs.com/png.latex?L_1">에 미치는 인과 효과를 분리해낼 수 없고, 결과적으로 <strong><img src="https://latex.codecogs.com/png.latex?L_1%5Eg">의 분포를 식별할 수 없습니다.</strong></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/swig_figure_19_10.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.10: Figure 19.6에 대응하는 SWIG. <img src="https://latex.codecogs.com/png.latex?L_1"> 노드(정확히는 <img src="https://latex.codecogs.com/png.latex?L_1%5Eg">)로 들어오는 화살표 <img src="https://latex.codecogs.com/png.latex?W_0%20%5Crightarrow%20L_1">과 <img src="https://latex.codecogs.com/png.latex?A_0%20%5Cleftarrow%20W_0"> 경로 때문에 <img src="https://latex.codecogs.com/png.latex?L_1%5Eg">의 분포가 식별되지 않음을 보여준다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결론</strong>: Figure 19.6과 같은 관찰 연구에서는 <strong>정적 전략의 효과만 추정 가능하며, 동적 전략의 효과는 추정할 수 없습니다.</strong></li>
</ul>
</section>
<section id="scenario-3-non-identifiability-figure-19.11" class="level3">
<h3 class="anchored" data-anchor-id="scenario-3-non-identifiability-figure-19.11">3.3. Scenario 3: Non-Identifiability (Figure 19.11)</h3>
<ul>
<li>마지막은 가장 나쁜 상황입니다. Figure 19.6의 구조에서 <img src="https://latex.codecogs.com/png.latex?L_1">이 결과 <img src="https://latex.codecogs.com/png.latex?Y">에 직접적인 영향을 주는 화살표(<img src="https://latex.codecogs.com/png.latex?L_1%20%5Crightarrow%20Y">)가 추가된 경우입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_11.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.11: Figure 19.6 구조에 <img src="https://latex.codecogs.com/png.latex?L_1%20%5Crightarrow%20Y"> 화살표가 추가된 상황. 모든 종류의 전략에 대해 식별 불가능하다.</figcaption>
</figure>
</div>
<ul>
<li><strong>해석</strong>: 이제 <img src="https://latex.codecogs.com/png.latex?L_1">은 단순한 중간 단계가 아니라 결과 <img src="https://latex.codecogs.com/png.latex?Y">의 직접적인 원인인 동시에 교란 요인입니다.</li>
<li><strong>분석 (with SWIG Fig 19.12)</strong>: D-separation을 적용해보면, <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D">에 대한 정적 순차적 교환 가능성조차 성립하지 않음을 알 수 있습니다.</li>
<li><strong>결론</strong>: 이 구조에서는 <strong>어떠한 전략(정적 또는 동적)에 대한 인과 효과도 데이터로부터 유효하게 추정할 수 없습니다.</strong></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="time-varying-confounding-and-time-varying-confounders" class="level1">
<h1>19.6 Time-varying confounding and time-varying confounders</h1>
<section id="introduction-4" class="level2">
<h2 class="anchored" data-anchor-id="introduction-4">1. Introduction</h2>
<ul>
<li>이전 챕터들에서 우리는 시변 처치(Time-Varying Treatment)의 인과 효과를 식별하기 위한 조건인 <strong>순차적 교환 가능성(Sequential Exchangeability)</strong>을 배웠습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%5E%7B%5Cbar%7Ba%7D%7D%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k%20%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D,%20%5Cbar%7BL%7D_k%0A"></p>
<ul>
<li><p>연구자가 이 조건을 만족시키기 위해 필요한 모든 공변량 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BL%7D_k">를 측정했다고 가정해 봅시다(즉, Figure 19.2와 같은 상황). 그렇다면 이제 남은 문제는 “어떻게 효과를 추정할 것인가?”입니다.</p></li>
<li><p>놀랍게도, 우리가 고정 처치(Fixed Treatment) 문제에서 흔히 사용했던 <strong>표준적인 조정 방법들(층화, 매칭, 일반적인 다중 회귀분석)</strong>은 시변 처치 상황, 특히 <strong>시변 교란(Time-varying Confounding)</strong>이 존재할 때 <strong>편향된 결과를 낳습니다.</strong></p></li>
<li><p>이번 포스트에서는 ’시변 교란’이 정확히 무엇인지 정의하고, 왜 고전적인 방법론이 이 문제 앞에서 무력해지는지 인과 그래프(DAG)를 통해 분석합니다.</p></li>
</ul>
</section>
<section id="defining-time-varying-confounding" class="level2">
<h2 class="anchored" data-anchor-id="defining-time-varying-confounding">2. Defining Time-Varying Confounding</h2>
<section id="confounders-vs.-confounding" class="level3">
<h3 class="anchored" data-anchor-id="confounders-vs.-confounding">2.1. Confounders vs.&nbsp;Confounding</h3>
<ul>
<li><strong>시변 교란 요인 (Time-varying Confounders)</strong>: 시간에 따라 변하는 공변량 <img src="https://latex.codecogs.com/png.latex?L_k"> 자체를 의미합니다. (예: 매달 측정되는 CD4 수치)</li>
<li><strong>시변 교란 (Time-varying Confounding)</strong>: 이러한 요인들로 인해 발생하는 <strong>편향(Bias)의 상태</strong>를 의미합니다.</li>
</ul>
</section>
<section id="formal-definition" class="level3">
<h3 class="anchored" data-anchor-id="formal-definition">2.2. Formal Definition</h3>
<ul>
<li>식별 가능성 조건(순차적 교환 가능성)이 성립한다고 가정할 때, <strong>시변 교란</strong>은 다음과 같이 정의됩니다.</li>
</ul>
<blockquote class="blockquote">
<p>베이스라인 공변량 <img src="https://latex.codecogs.com/png.latex?L_0">만으로 조정했을 때, 처치 이력에 따른 잠재적 결과의 평균이 관찰된 결과의 조건부 평균과 다르다면, 시변 교란이 존재한다고 말한다.</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5BY%5E%7B%5Cbar%7Ba%7D%7D%20%7C%20L_0%5D%20%5Cneq%20E%5BY%20%7C%20A=%5Cbar%7Ba%7D,%20L_0%5D%0A"></p>
<ul>
<li>역으로 말하면, <strong>시변 교란이 없다(No time-varying confounding)</strong>는 것은 베이스라인 정보만으로도 충분히 인과 효과를 추정할 수 있다는 뜻입니다. 이에 대한 충분 조건은 <strong>무조건적 순차적 교환 가능성(Unconditional Sequential Exchangeability)</strong>입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY%5E%7B%5Cbar%7Ba%7D%7D%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20A_k%20%5Cmid%20%5Cbar%7BA%7D_%7Bk-1%7D%0A"></p>
<ul>
<li><p>이는 과거의 처치 이력만 주어지면, 현재의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">가 잠재적 결과와 독립이라는 뜻입니다. 순차적 무작위 실험(Figure 19.1)이 바로 이 경우에 해당하며, 이때는 복잡한 시변 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">를 조정할 필요가 없습니다.</p></li>
<li><p>하지만 관찰 연구에서는 대부분 <img src="https://latex.codecogs.com/png.latex?L_k">가 처치 <img src="https://latex.codecogs.com/png.latex?A_k">에 영향을 주기 때문에 시변 교란이 발생합니다.</p></li>
</ul>
</section>
</section>
<section id="the-problem-treatment-confounder-feedback" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-treatment-confounder-feedback">3. The Problem: Treatment-Confounder Feedback</h2>
<ul>
<li>시변 교란이 발생했을 때 가장 심각한 문제는 <strong>처치-교란 피드백(Treatment-Confounder Feedback)</strong> 구조가 형성될 때입니다.</li>
</ul>
<section id="feedback-loop-structure" class="level3">
<h3 class="anchored" data-anchor-id="feedback-loop-structure">3.1. Feedback Loop Structure</h3>
<ul>
<li>피드백 루프는 다음 두 가지 화살표가 동시에 존재할 때 발생합니다.
<ul>
<li><ol type="1">
<li><strong>Confounder <img src="https://latex.codecogs.com/png.latex?%5Cto"> Treatment</strong>: 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">가 미래의 처치 <img src="https://latex.codecogs.com/png.latex?A_k">에 영향을 줌 (의사가 환자 상태를 보고 처방).</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Treatment <img src="https://latex.codecogs.com/png.latex?%5Cto"> Confounder</strong>: 과거의 처치 <img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D">이 미래의 공변량 <img src="https://latex.codecogs.com/png.latex?L_k">에 영향을 줌 (약물이 환자 상태를 변화시킴).</li>
</ol></li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AA_%7Bk-1%7D%20%5Clongrightarrow%20L_k%20%5Clongrightarrow%20A_k%0A"></p>
<ul>
<li>이 구조는 표준적인 조정 방법(Standard Adjustment Methods)을 사용할 수 없게 만드는 “딜레마”를 형성합니다.</li>
</ul>
</section>
<section id="the-dilemma-of-adjustment-figure-19.2-analysis" class="level3">
<h3 class="anchored" data-anchor-id="the-dilemma-of-adjustment-figure-19.2-analysis">3.2. The Dilemma of Adjustment (Figure 19.2 Analysis)</h3>
<ul>
<li>Figure 19.2는 전형적인 시변 교란과 피드백이 있는 상황을 묘사합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/images/dag_figure_19_2_feedback.png" class="img-fluid figure-img"></p>
<figcaption>Figure 19.2: 시변 처치 <img src="https://latex.codecogs.com/png.latex?A_0,%20A_1">과 시변 공변량 <img src="https://latex.codecogs.com/png.latex?L_1">이 존재하는 관찰 연구. <img src="https://latex.codecogs.com/png.latex?L_1">은 <img src="https://latex.codecogs.com/png.latex?A_1">의 교란 요인이면서 동시에 <img src="https://latex.codecogs.com/png.latex?A_0">의 결과이다. 또한 <img src="https://latex.codecogs.com/png.latex?U">는 <img src="https://latex.codecogs.com/png.latex?L_1">과 <img src="https://latex.codecogs.com/png.latex?Y">의 공통 원인이다.</figcaption>
</figure>
</div>
<ul>
<li><p>우리가 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?A_1">의 합동 효과(Joint Effect), 즉 <img src="https://latex.codecogs.com/png.latex?Y%5E%7B%5Cbar%7Ba%7D%7D">를 추정하려 한다고 가정해 봅시다.</p></li>
<li><ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?A_1">에 대한 교란 통제 필요성</strong>:</li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?L_1">은 <img src="https://latex.codecogs.com/png.latex?A_1">의 원인이자 <img src="https://latex.codecogs.com/png.latex?Y">의 원인(<img src="https://latex.codecogs.com/png.latex?L_1%20%5Cto%20Y"> 또는 <img src="https://latex.codecogs.com/png.latex?L_1%20%5Cleftarrow%20U%20%5Cto%20Y">)이므로, <img src="https://latex.codecogs.com/png.latex?A_1">의 효과를 추정하려면 <strong><img src="https://latex.codecogs.com/png.latex?L_1">을 통제(Conditioning/Adjustment)해야 합니다.</strong></li>
</ul></li>
<li><ol start="2" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?A_0">에 대한 편향 발생 위험</strong>:</li>
</ol>
<ul>
<li>하지만 <img src="https://latex.codecogs.com/png.latex?L_1">은 <img src="https://latex.codecogs.com/png.latex?A_0">의 결과(<img src="https://latex.codecogs.com/png.latex?A_0%20%5Cto%20L_1">)입니다.</li>
<li><strong>Case A (Mediation)</strong>: <img src="https://latex.codecogs.com/png.latex?L_1">이 <img src="https://latex.codecogs.com/png.latex?A_0">의 효과를 매개하는 경로(<img src="https://latex.codecogs.com/png.latex?A_0%20%5Cto%20L_1%20%5Cto%20Y">)에 있다면, <img src="https://latex.codecogs.com/png.latex?L_1">을 통제하는 순간 <img src="https://latex.codecogs.com/png.latex?A_0">의 효과 일부를 제거해 버리는 <strong>과소 추정(Over-adjustment)</strong> 문제가 발생합니다.</li>
<li><strong>Case B (Collider Bias)</strong>: Figure 19.2처럼 <img src="https://latex.codecogs.com/png.latex?U%20%5Cto%20L_1">과 <img src="https://latex.codecogs.com/png.latex?U%20%5Cto%20Y">가 존재하는 경우, <img src="https://latex.codecogs.com/png.latex?L_1">은 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?U"> 사이의 충돌자(Collider)가 됩니다 (<img src="https://latex.codecogs.com/png.latex?A_0%20%5Cto%20L_1%20%5Cleftarrow%20U">).
<ul>
<li>이때 <img src="https://latex.codecogs.com/png.latex?L_1">을 통제하면 <img src="https://latex.codecogs.com/png.latex?A_0">와 <img src="https://latex.codecogs.com/png.latex?U"> 사이에 가상의 상관관계가 생기고(<img src="https://latex.codecogs.com/png.latex?A_0%20%5Cleftrightarrow%20U">), 결과적으로 <img src="https://latex.codecogs.com/png.latex?A_0%20%5Cto%20L_1%20%5Cleftarrow%20U%20%5Cto%20Y">라는 <strong>뒷문 경로(Backdoor Path)가 열리게 됩니다.</strong> 이를 <strong>선택 편향(Selection Bias)</strong> 또는 <strong>충돌자 편향</strong>이라고 합니다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="결론-진퇴양난-catch-22" class="level3">
<h3 class="anchored" data-anchor-id="결론-진퇴양난-catch-22">3.3. 결론: 진퇴양난 (Catch-22)</h3>
<ul>
<li><p><img src="https://latex.codecogs.com/png.latex?L_1">을 조정하지 않으면? <img src="https://latex.codecogs.com/png.latex?%5Cto"> <img src="https://latex.codecogs.com/png.latex?A_1">의 효과가 교란됨 (Confounding Bias).</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?L_1">을 조정하면? <img src="https://latex.codecogs.com/png.latex?%5Cto"> <img src="https://latex.codecogs.com/png.latex?A_0">의 효과가 편향됨 (Selection Bias or Over-adjustment).</p></li>
<li><p>따라서 일반적인 회귀분석( <img src="https://latex.codecogs.com/png.latex?Y%20%5Csim%20A_0%20+%20A_1%20+%20L_1"> )이나 층화 분석은 이 딜레마를 해결할 수 없으며, 시변 처치 효과 추정에 <strong>부적합</strong>합니다.</p></li>
</ul>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">4. Summary</h2>
<p><strong>Chapter 19.6</strong>은 시변 인과추론에서 “왜 특별한 방법론이 필요한가?”에 대한 답을 줍니다.</p>
<ul>
<li><ol type="1">
<li><strong>시변 교란(Time-varying Confounding)</strong>: 베이스라인 정보만으로는 통제되지 않는 교란이 존재하며, 이는 시간이 지남에 따라 변하는 공변량(<img src="https://latex.codecogs.com/png.latex?L_k">)에 의해 발생합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>표준 방법의 실패</strong>: <img src="https://latex.codecogs.com/png.latex?L_k">가 과거 처치의 영향을 받고(<img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D%20%5Cto%20L_k">), 동시에 미래 처치에 영향을 줄 때(<img src="https://latex.codecogs.com/png.latex?L_k%20%5Cto%20A_k">), 이를 <strong>처치-교란 피드백</strong>이라고 합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>인과적 딜레마</strong>: 피드백이 있는 변수 <img src="https://latex.codecogs.com/png.latex?L_k">를 통제하면 과거 처치(<img src="https://latex.codecogs.com/png.latex?A_%7Bk-1%7D">)에 대한 충돌자 편향이 발생하고, 통제하지 않으면 미래 처치(<img src="https://latex.codecogs.com/png.latex?A_k">)에 대한 교란 편향이 남습니다.</li>
</ol></li>
<li>이러한 한계를 극복하기 위해서는 피드백 구조를 적절히 다룰 수 있는 <strong>G-methods</strong> (G-formula, IP Weighting, G-estimation)가 필수적입니다. 다음 챕터부터는 이러한 방법론들을 구체적으로 다루게 됩니다.</li>
</ul>



</section>
</section>

 ]]></description>
  <category>Paper Review</category>
  <category>What If</category>
  <guid>https://shsha0110.github.io/posts/book/What If/19.Time-Varying Treatments/</guid>
  <pubDate>Wed, 04 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 11A. Generalized Propensity Score</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L11A/</link>
  <description><![CDATA[ 





<section id="introduction-beyond-binary-treatments" class="level1">
<h1>1. Introduction: Beyond Binary Treatments</h1>
<p>[cite_start]기존의 인과추론 방법론, 특히 <strong>Propensity Score (PS)</strong> 매칭이나 가중치(Weighting) 기법은 주로 <strong>이분형 처치(Binary Treatment)</strong> 상황을 가정합니다[cite: 9]. 예를 들어, 약을 복용했는지(<img src="https://latex.codecogs.com/png.latex?D=1">) 안 했는지(<img src="https://latex.codecogs.com/png.latex?D=0">)와 같은 상황입니다.</p>
<p>[cite_start]하지만 현실 세계의 많은 정책과 처치는 <strong>연속형(Continuous)</strong>이거나 <strong>다중 값(Multi-valued)</strong>을 가집니다[cite: 9]. * 직업 훈련 프로그램의 <strong>교육 시간</strong> (Continuous) * 복용하는 약의 <strong>투여량(Dose)</strong> (Continuous) * 다양한 수준의 장학금 지원 액수 (Multi-valued/Ordinal)</p>
<p>이러한 상황에서 단순히 처치 여부만을 따지는 것은 정보의 손실을 야기하며, “얼마나 처치했을 때 효과가 극대화되는가?”라는 <strong>Dose-Response Function</strong>을 추정하기 어렵게 만듭니다. [cite_start]이를 해결하기 위해 Hirano &amp; Imbens (2004)는 기존의 Propensity Score를 확장한 <strong>Generalized Propensity Score (GPS)</strong>를 제안했습니다[cite: 10].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L11A/images/binary_vs_continuous_treatment_concept.png" class="img-fluid figure-img"></p>
<figcaption>Figure: 이분형 처치와 연속형 처치의 개념적 차이. 왼쪽은 처치군/대조군이 명확히 나뉘는 반면, 오른쪽은 처치 수준(Dose)이 연속적인 스펙트럼으로 존재하여 각 수준별로 잠재적 결과가 달라짐을 시사한다.</figcaption>
</figure>
</div>
<hr>
</section>
<section id="generalized-propensity-score-gps의-정의" class="level1">
<h1>2. Generalized Propensity Score (GPS)의 정의</h1>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">2.1. Notation</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">: 처치(Treatment). 여기서는 연속형 변수일 수 있습니다 (기존의 <img src="https://latex.codecogs.com/png.latex?D">나 <img src="https://latex.codecogs.com/png.latex?T">에 해당).</li>
<li><img src="https://latex.codecogs.com/png.latex?Y">: 결과 변수(Outcome).</li>
<li><img src="https://latex.codecogs.com/png.latex?Z">: 공변량 벡터(Covariates). (기존의 <img src="https://latex.codecogs.com/png.latex?X">에 해당하나, 혼동을 피하기 위해 강의 자료의 표기를 따름).</li>
<li><img src="https://latex.codecogs.com/png.latex?Y(x)">: 처치 수준이 <img src="https://latex.codecogs.com/png.latex?x">일 때의 잠재적 결과(Potential Outcome).</li>
</ul>
</section>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">2.2. Definition</h2>
<p>[cite_start]<strong>Generalized Propensity Score (GPS)</strong>는 관측된 공변량 <img src="https://latex.codecogs.com/png.latex?Z">가 주어졌을 때, 특정 처치 수준 <img src="https://latex.codecogs.com/png.latex?x">를 받을 조건부 확률 밀도(Conditional Probability Density)로 정의됩니다[cite: 11].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ar(x,%20z)%20=%20f_%7BX%7CZ%7D(x%7Cz)%0A%5Bcite_start%5D"> [cite: 12]</p>
<p>이분형 처치에서의 PS가 <img src="https://latex.codecogs.com/png.latex?P(D=1%7CX)">라는 확률(Probability)이었던 것과 달리, 연속형 처치에서는 <strong>확률 밀도 함수(PDF)</strong>의 값을 갖는다는 점이 핵심입니다.</p>
<hr>
</section>
</section>
<section id="핵심-가정-및-성질-key-assumptions-properties" class="level1">
<h1>3. 핵심 가정 및 성질 (Key Assumptions &amp; Properties)</h1>
<p>GPS를 사용하여 인과 효과를 식별(Identification)하기 위해서는 기존 Rosenbaum &amp; Rubin (1983)의 가정을 연속형 처치에 맞게 확장해야 합니다.</p>
<section id="weak-unconfoundedness-약한-교란해소-가정" class="level2">
<h2 class="anchored" data-anchor-id="weak-unconfoundedness-약한-교란해소-가정">3.1. Weak Unconfoundedness (약한 교란해소 가정)</h2>
<p>[cite_start]이분형 처치에서의 ‘Strong Unconfoundedness’ (<img src="https://latex.codecogs.com/png.latex?Y(0),%20Y(1)%20%5Cperp%20D%20%7C%20X">)와 달리, GPS에서는 <strong>Weak Unconfoundedness</strong>를 가정합니다[cite: 17, 18].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY(x)%20%5Cperp%20X%20%5Cmid%20Z%20%5Cquad%20%5Cforall%20x%20%5Cin%20%5Cmathcal%7BX%7D%0A%5Bcite_start%5D"> [cite: 19, 20]</p>
<ul>
<li><strong>의미:</strong> 공변량 <img src="https://latex.codecogs.com/png.latex?Z">를 통제했을 때, 처치 수준 <img src="https://latex.codecogs.com/png.latex?X">와 해당 처치 수준에서의 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y(x)">는 독립입니다.</li>
<li><strong>Why “Weak”?</strong> 모든 처치 수준에 대한 잠재적 결과들의 결합 분포 <img src="https://latex.codecogs.com/png.latex?%5C%7BY(x)%5C%7D_%7Bx%20%5Cin%20%5Cmathcal%7BX%7D%7D">가 <img src="https://latex.codecogs.com/png.latex?X">와 독립일 필요는 없습니다. [cite_start]우리는 각 처치 수준 <img src="https://latex.codecogs.com/png.latex?x"> 하나하나에 대해서만 조건부 독립이 성립하면 충분하기 때문에 “Weak”라는 용어를 사용합니다[cite: 21, 22].</li>
</ul>
</section>
<section id="balancing-property-균형-성질" class="level2">
<h2 class="anchored" data-anchor-id="balancing-property-균형-성질">3.2. Balancing Property (균형 성질)</h2>
<p>[cite_start]표준 PS와 마찬가지로 GPS도 공변량의 균형을 맞추는 성질을 가집니다[cite: 27].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ%20%5Cperp%20%5Cmathbb%7B1%7D%5C%7BX=x%5C%7D%20%5Cmid%20r(x,%20Z)%0A%5Bcite_start%5D"> [cite: 31]</p>
<ul>
<li>[cite_start]<strong>해석:</strong> GPS 값 <img src="https://latex.codecogs.com/png.latex?r(x,Z)">가 동일한 하위 그룹(strata) 내에서는, 처치 수준 <img src="https://latex.codecogs.com/png.latex?X=x">를 받을 확률이 공변량 <img src="https://latex.codecogs.com/png.latex?Z">에 의존하지 않습니다[cite: 30].</li>
<li>[cite_start]이 성질은 GPS가 고차원의 공변량 <img src="https://latex.codecogs.com/png.latex?Z"> 정보를 <img src="https://latex.codecogs.com/png.latex?r(x,Z)">라는 1차원 스칼라 값으로 압축(Dimensionality Reduction)하면서도, 교란 요인을 통제할 수 있음을 시사합니다[cite: 114].</li>
</ul>
<hr>
</section>
</section>
<section id="gps를-이용한-편향-제거-bias-removal" class="level1">
<h1>4. GPS를 이용한 편향 제거 (Bias Removal)</h1>
<p>Hirano &amp; Imbens (2004)는 GPS를 이용해 편향을 제거하고 Average Potential Outcome을 추정할 수 있음을 증명했습니다.</p>
<section id="theorem-bias-removal" class="level2">
<h2 class="anchored" data-anchor-id="theorem-bias-removal">4.1. Theorem: Bias Removal</h2>
<p>[cite_start]처치 할당이 공변량 <img src="https://latex.codecogs.com/png.latex?Z">에 대해 Weakly Unconfounded 하다면, 다음이 성립합니다[cite: 46, 47].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY(x)%5D%20=%20%5Cmathbb%7BE%7D%5B%5Cbeta(x,%20r(x,%20Z))%5D%0A"></p>
<p>[cite_start]여기서 <img src="https://latex.codecogs.com/png.latex?%5Cbeta(x,%20r)">은 다음과 같이 정의된 조건부 기대값입니다[cite: 49].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbeta(x,%20r)%20%5Ctriangleq%20%5Cmathbb%7BE%7D%5BY(x)%20%5Cmid%20r(x,%20Z)%20=%20r%5D%0A"></p>
</section>
<section id="proof-상세-유도-과정" class="level2">
<h2 class="anchored" data-anchor-id="proof-상세-유도-과정">4.2. Proof (상세 유도 과정)</h2>
<p>[cite_start]이 정리는 <strong>반복 기대의 법칙(Law of Iterated Expectations, LIE)</strong>과 <strong>Weak Unconfoundedness</strong> 가정을 사용하여 유도할 수 있습니다. [cite: 54]</p>
<p><strong>Step 1: 관측 데이터와의 연결</strong> Unconfoundedness 가정에 의해, <img src="https://latex.codecogs.com/png.latex?r(x,Z)">를 조건부로 주었을 때도 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y(x)">는 독립입니다. [cite_start]따라서 다음이 성립합니다[cite: 39, 57]. <img src="https://latex.codecogs.com/png.latex?%0Af_%7BY(x)%7CX,%20r(x,Z)%7D(y%7Cx,%20r)%20=%20f_%7BY(x)%7Cr(x,Z)%7D(y%7Cr)%0A"> [cite_start]즉, 실제 처치 <img src="https://latex.codecogs.com/png.latex?X=x">를 받은 집단에서의 <img src="https://latex.codecogs.com/png.latex?Y">의 기대값은 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y(x)">의 기대값과 같습니다[cite: 58]. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY%20%5Cmid%20X=x,%20r(x,Z)=r%5D%20=%20%5Cmathbb%7BE%7D%5BY(x)%20%5Cmid%20r(x,Z)=r%5D%20=%20%5Cbeta(x,%20r)%0A"> (참고: <img src="https://latex.codecogs.com/png.latex?Y%20=%20Y(X)"> 이므로, <img src="https://latex.codecogs.com/png.latex?X=x">일 때 <img src="https://latex.codecogs.com/png.latex?Y=Y(x)">)</p>
<p><strong>Step 2: 전체 기대값 도출</strong> 우리가 구하고자 하는 것은 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY(x)%5D">입니다. [cite_start]이는 <img src="https://latex.codecogs.com/png.latex?%5Cbeta(x,%20r(x,Z))">의 기대값을 취함으로써 얻을 수 있습니다[cite: 63].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D%5B%5Cbeta(x,%20r(x,%20Z))%5D%20&amp;=%20%5Cmathbb%7BE%7D%5Cleft%5B%20%5Cmathbb%7BE%7D%5BY(x)%20%5Cmid%20r(x,%20Z)%5D%20%5Cright%5D%20%5Cquad%20(%5Cbecause%20%5Ctext%7BDefinition%20of%20%7D%20%5Cbeta)%20%5C%5C%0A&amp;=%20%5Cmathbb%7BE%7D%5BY(x)%5D%20%5Cquad%20(%5Cbecause%20%5Ctext%7BLaw%20of%20Iterated%20Expectations%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<p>따라서, 관측된 <img src="https://latex.codecogs.com/png.latex?Y,%20X,%20Z">를 이용해 <img src="https://latex.codecogs.com/png.latex?%5Cbeta(x,r)">을 추정하고, 이를 <img src="https://latex.codecogs.com/png.latex?r(x,Z)">의 분포에 대해 평균을 내면 인과 효과 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY(x)%5D">를 얻을 수 있습니다.</p>
<hr>
</section>
</section>
<section id="estimation-procedure-3-step-method" class="level1">
<h1>5. Estimation Procedure (3-Step Method)</h1>
<p>[cite_start]실제 데이터 분석에서 GPS를 활용하는 방법은 다음 3단계로 요약됩니다[cite: 68].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L11A/images/gps_estimation_procedure_flowchart.png" class="img-fluid figure-img"></p>
<figcaption>Figure: GPS를 이용한 3단계 인과효과 추정 알고리즘 흐름도. 1단계: GPS 추정, 2단계: 조건부 기대값 함수 추정, 3단계: 용량-반응 함수(DRF) 도출 과정을 시각적으로 표현함.</figcaption>
</figure>
</div>
<section id="step-1-gps-모델링-및-추정" class="level2">
<h2 class="anchored" data-anchor-id="step-1-gps-모델링-및-추정">Step 1: GPS 모델링 및 추정</h2>
<p>먼저 공변량 <img src="https://latex.codecogs.com/png.latex?Z">가 주어졌을 때 처치 <img src="https://latex.codecogs.com/png.latex?X">의 조건부 분포를 모델링합니다. [cite_start]보통 정규분포 등을 가정합니다[cite: 70]. <img src="https://latex.codecogs.com/png.latex?%0AX_i%20%5Cmid%20Z_i%20%5Csim%20N(%5Cbeta_0%20+%20%5Cbeta_1%5ET%20Z_i,%20%5Csigma%5E2)%0A%5Bcite_start%5D"> [cite: 91] [cite_start]최대우도법(MLE)이나 OLS로 파라미터를 추정한 후, 각 개체 <img src="https://latex.codecogs.com/png.latex?i">에 대해 GPS 값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D_i">를 계산합니다[cite: 93]. <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_i%20=%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Chat%7B%5Csigma%7D%5E2%7D%7D%20%5Cexp%20%5Cleft%5C%7B%20-%5Cfrac%7B1%7D%7B2%5Chat%7B%5Csigma%7D%5E2%7D%20(X_i%20-%20%5Chat%7B%5Cbeta%7D_0%20-%20%5Chat%7B%5Cbeta%7D_1%20Z_i)%5E2%20%5Cright%5C%7D%0A"></p>
</section>
<section id="step-2-조건부-기대값-betax-r-추정" class="level2">
<h2 class="anchored" data-anchor-id="step-2-조건부-기대값-betax-r-추정">Step 2: 조건부 기대값 <img src="https://latex.codecogs.com/png.latex?%5Cbeta(x,%20r)"> 추정</h2>
<p>[cite_start]관측된 결과 <img src="https://latex.codecogs.com/png.latex?Y_i">를 처치 <img src="https://latex.codecogs.com/png.latex?X_i">와 추정된 GPS <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D_i">에 대해 회귀분석합니다[cite: 71]. [cite_start]이 함수는 인과적 해석을 갖지 않는 단순한 연관성 모델입니다[cite: 83]. [cite_start]유연한 추정을 위해 고차항이나 교차항을 포함한 다항 회귀(Polynomial Regression)를 주로 사용합니다[cite: 95].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_i%20%7C%20X_i,%20R_i%5D%20=%20%5Calpha_0%20+%20%5Calpha_1%20X_i%20+%20%5Calpha_2%20X_i%5E2%20+%20%5Calpha_3%20R_i%20+%20%5Calpha_4%20R_i%5E2%20+%20%5Calpha_5%20X_i%20R_i%20+%20%5Cdots%0A"></p>
</section>
<section id="step-3-dose-response-function-drf-추정" class="level2">
<h2 class="anchored" data-anchor-id="step-3-dose-response-function-drf-추정">Step 3: Dose-Response Function (DRF) 추정</h2>
<p>특정 처치 수준 <img src="https://latex.codecogs.com/png.latex?x">에서의 평균 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)%20=%20%5Cmathbb%7BE%7D%5BY(x)%5D">를 추정합니다. [cite_start]이는 Step 2에서 구한 회귀식에 <strong>고정된 <img src="https://latex.codecogs.com/png.latex?x"></strong>와 <strong>관측된 <img src="https://latex.codecogs.com/png.latex?Z_i">로 계산한 <img src="https://latex.codecogs.com/png.latex?r(x,%20Z_i)"></strong> 값을 대입하여 모든 개체에 대해 평균을 냄으로써 구합니다[cite: 73, 74].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cmu%7D(x)%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Chat%7B%5Cbeta%7D(x,%20%5Chat%7Br%7D(x,%20Z_i))%0A%5Bcite_start%5D"> [cite: 104]</p>
<p>이 과정을 관심 있는 모든 <img src="https://latex.codecogs.com/png.latex?x"> 값에 대해 반복하면 전체 <strong>Dose-Response Function</strong>을 그릴 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L11A/images/dose_response_function_example.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Dose-Response Function 예시 그래프. X축은 처치 수준(Dose), Y축은 기대 결과(Outcome)를 나타내며, 점선은 Bootstrap을 통해 구한 신뢰구간을 의미한다.</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><strong>주의사항:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cbeta(x,%20r)"> 자체는 인과적 해석이 불가능합니다. [cite_start]<img src="https://latex.codecogs.com/png.latex?r">을 주변화(marginalize)하여 얻은 <img src="https://latex.codecogs.com/png.latex?%5Cmu(x)">만이 인과적 해석(Dose-Response)을 가집니다[cite: 83, 84].</p>
</blockquote>
<hr>
</section>
</section>
<section id="example-normal-distribution-case" class="level1">
<h1>6. Example: Normal Distribution Case</h1>
<p>[cite_start]강의 자료에 소개된 구체적인 구현 예시를 살펴보겠습니다[cite: 89].</p>
<ol type="1">
<li><strong>Treatment Model:</strong> <img src="https://latex.codecogs.com/png.latex?X%7CZ">를 정규분포로 가정하고 GPS <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D_i">를 도출합니다.</li>
<li>[cite_start]<strong>Outcome Model:</strong> <img src="https://latex.codecogs.com/png.latex?Y">를 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?R">의 3차 다항식 및 교차항으로 모델링합니다[cite: 95]. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D%5BY%7CX,R%5D%20&amp;=%20%5Calpha_0%20+%20%5Calpha_1%20X%20+%20%5Calpha_2%20X%5E2%20+%20%5Calpha_3%20X%5E3%20%5C%5C%0A&amp;+%20%5Calpha_4%20R%20+%20%5Calpha_5%20R%5E2%20+%20%5Calpha_6%20R%5E3%20%5C%5C%0A&amp;+%20%5Calpha_7%20XR%20+%20%5Calpha_8%20X%5E2%20R%20+%20%5Calpha_9%20X%20R%5E2%0A%5Cend%7Baligned%7D%0A"></li>
<li>[cite_start]<strong>Averaging:</strong> 특정 <img src="https://latex.codecogs.com/png.latex?x">에 대한 인과 효과는 다음과 같이 계산됩니다[cite: 104]. <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cmathbb%7BE%7D%7D%5BY(x)%5D%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi=1%7D%5E%7BN%7D%20%5Cleft(%20%5Chat%7B%5Calpha%7D_0%20+%20%5Chat%7B%5Calpha%7D_1%20x%20+%20%5Cdots%20+%20%5Chat%7B%5Calpha%7D_9%20x%20%5Chat%7Br%7D(x,%20Z_i)%5E2%20%5Cright)%0A"></li>
<li>[cite_start]<strong>Inference:</strong> 표준오차(Standard Errors)는 Bootstrap 방법을 사용하여 구합니다[cite: 106].</li>
</ol>
<hr>
</section>
<section id="summary-advantages" class="level1">
<h1>7. Summary &amp; Advantages</h1>
<p><strong>Generalized Propensity Score (GPS)</strong>는 복잡한 현실 데이터에서 인과관계를 추론하기 위한 강력한 도구입니다.</p>
<ul>
<li>[cite_start]<strong>확장성(Applicability):</strong> 연속형, 다항형, 순서형 등 다양한 형태의 처치에 적용 가능합니다[cite: 112].</li>
<li>[cite_start]<strong>차원 축소(Dimensionality Reduction):</strong> 고차원의 공변량 <img src="https://latex.codecogs.com/png.latex?Z">를 직접 통제하는 대신, 1차원의 점수 <img src="https://latex.codecogs.com/png.latex?R">을 통해 균형을 맞춤으로써 추정의 효율성을 높입니다[cite: 114].</li>
<li>[cite_start]<strong>유연성(Flexibility):</strong> 모수적 방법(예: 다항 회귀)뿐만 아니라 비모수적 방법(예: Spline, Kernel Smoothing)과 결합하여 사용할 수 있습니다[cite: 97].</li>
</ul>
<p>GPS를 통해 연구자는 단순한 “효과 있음/없음”을 넘어, “최적의 처치 수준은 무엇인가?”라는 더 깊은 정책적 질문에 답할 수 있게 됩니다.</p>
<hr>
<section id="checklist-강의-자료-반영-여부" class="level3">
<h3 class="anchored" data-anchor-id="checklist-강의-자료-반영-여부">Checklist: 강의 자료 반영 여부</h3>
<ul>
<li><strong>포함된 내용:</strong>
<ul>
<li>GPS의 정의 및 수식 (<img src="https://latex.codecogs.com/png.latex?r(x,z)">) [O]</li>
<li>Weak Unconfoundedness 가정의 정의와 의미 [O]</li>
<li>Balancing Property 설명 및 수식 [O]</li>
<li>Hirano &amp; Imbens (2004)의 Bias Removal 정리 및 증명 (LIE 활용) [O]</li>
<li>3단계 추정 알고리즘 (GPS 추정 -&gt; 조건부 기대값 -&gt; DRF) [O]</li>
<li>인과적 해석에 대한 주의사항 (<img src="https://latex.codecogs.com/png.latex?%5Cbeta"> vs <img src="https://latex.codecogs.com/png.latex?%5Cmu">) [O]</li>
<li>정규분포 가정 시의 구체적 다항회귀 예시 수식 [O]</li>
<li>Bootstrap을 이용한 추론 [O]</li>
<li>주요 참고문헌 언급 [O]</li>
</ul></li>
<li><strong>생략된 내용:</strong>
<ul>
<li>없음. (강의 자료 13페이지 분량의 모든 핵심 이론과 수식을 반영함)</li>
</ul></li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L11A/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] Appendix 01.Single World Intervention Graphs (SWIGs)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/LA01/</link>
  <description><![CDATA[ 





<section id="introduction-two-worlds-of-causality" class="level1">
<h1>Introduction: Two Worlds of Causality</h1>
<ul>
<li><p>인과추론(Causal Inference)에는 오랫동안 두 가지의 거대한 흐름이 존재해 왔습니다.</p>
<ul>
<li>Neyman-Rubin으로 대표되는 <strong>잠재적 결과(Potential Outcomes, Counterfactuals)</strong> 프레임워크
<ul>
<li><strong>Potential Outcomes:</strong> <img src="https://latex.codecogs.com/png.latex?Y(x)">와 같이 “만약 치료 <img src="https://latex.codecogs.com/png.latex?X">를 받았다면(또는 받지 않았다면) 있었을 결과”를 직접적으로 변수로 정의하여 엄밀한 수식적 전개를 가능하게 합니다.</li>
</ul></li>
<li>Judea Pearl로 대표되는 <strong>그래프 모형(Graphical Models, DAGs)</strong> 프레임워크입니다.
<ul>
<li><strong>Graphical Models:</strong> 변수 간의 인과 관계를 시각적인 화살표로 표현하고, <img src="https://latex.codecogs.com/png.latex?d">-separation과 같은 규칙을 통해 조건부 독립성을 직관적으로 파악하게 해줍니다.</li>
</ul></li>
</ul></li>
<li><p>하지만 이 두 접근법 사이에는 미묘한 단절이 있었습니다. DAG(Directed Acyclic Graph) 위에는 관측된 변수 <img src="https://latex.codecogs.com/png.latex?X,%20Y">만 존재할 뿐, 가상의 변수 <img src="https://latex.codecogs.com/png.latex?Y(x)">는 존재하지 않기 때문입니다.</p></li>
<li><p><strong>Single World Intervention Graphs (SWIGs)</strong>는 Thomas Richardson과 James Robins가 제안한 개념으로, 이 두 세계를 <strong>Node Splitting(노드 분리)</strong>이라는 기법을 통해 완벽하게 통합하려는 시도입니다.</p></li>
<li><p>이번 포스트에서는 SWIGs가 어떻게 그래프 위에 Counterfactual을 명시적으로 표현하고, 복잡한 인과 식별(Identification) 문제를 해결하는지 살펴보겠습니다.</p></li>
</ul>
</section>
<section id="the-elephant-in-the-room-the-disconnect" class="level1">
<h1>1. The Elephant in the Room: The Disconnect</h1>
<section id="potential-outcomes-setup" class="level2">
<h2 class="anchored" data-anchor-id="potential-outcomes-setup">1.1. Potential Outcomes Setup</h2>
<ul>
<li>이분 변수(binary treatment) <img src="https://latex.codecogs.com/png.latex?X">와 결과 <img src="https://latex.codecogs.com/png.latex?Y">가 있다고 합시다. 우리는 두 개의 잠재적 결과 변수를 정의합니다. <img src="https://latex.codecogs.com/png.latex?Y(x=0),%20%5Cquad%20Y(x=1)"></li>
<li>이는 각각 <img src="https://latex.codecogs.com/png.latex?X=0"> 또는 <img src="https://latex.codecogs.com/png.latex?X=1">로 할당되었을 때 관측될 <img src="https://latex.codecogs.com/png.latex?Y">의 값을 의미합니다. Pearl의 <img src="https://latex.codecogs.com/png.latex?do">-calculus 표기법으로는 다음과 같이 연결됩니다. <img src="https://latex.codecogs.com/png.latex?P(Y(x)=y)%20%5Cequiv%20P(Y=y%20%5Cmid%20do(X=x))"></li>
<li>하지만 Counterfactual 표기법은 더 일반적입니다.</li>
<li>예를 들어, <em>“실제로 치료를 받은 사람(<img src="https://latex.codecogs.com/png.latex?X=1">)이 만약 치료를 받지 않았더라면(<img src="https://latex.codecogs.com/png.latex?x=0">) 겪었을 결과”</em>인 <strong>ETT (Effect of Treatment on the Treated)</strong>는 <img src="https://latex.codecogs.com/png.latex?do"> 표기법만으로는 표현하기 어렵습니다. <img src="https://latex.codecogs.com/png.latex?P(Y(x=0)=y%20%5Cmid%20X=1)"></li>
</ul>
</section>
<section id="the-gap-in-standard-dags" class="level2">
<h2 class="anchored" data-anchor-id="the-gap-in-standard-dags">1.2. The Gap in Standard DAGs</h2>
<ul>
<li>표준적인 인과 그래프(Causal DAG)를 생각해 봅시다.</li>
<li>교란요인(Confounding)이 없는 경우, 우리는 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">라고 그립니다.</li>
<li>이 모델은 “ignorability” 가정, 즉 처치가 잠재적 결과와 독립임을 암묵적으로 가정합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y(x_0)%20%5Cquad%20%5C&amp;%20%5Cquad%20X%20%5Cperp%20Y(x_1)"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA01/images/standard_dag_vs_independence.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Standard DAG vs.&nbsp;Counterfactual Independence. 좌측은 단순한 X-&gt;Y 그래프, 우측은 이 그래프가 함의해야 하는 독립성 조건. 그러나 그래프 상에는 Y(x) 노드가 없어 이를 시각적으로 확인할 수 없음.</figcaption>
</figure>
</div>
<ul>
<li>여기서 <strong>“방 안의 코끼리(Elephant in the room)”</strong>가 등장합니다.</li>
<li>위 수식의 <img src="https://latex.codecogs.com/png.latex?Y(x_0),%20Y(x_1)"> 변수는 그래프 어디에도 그려져 있지 않습니다.</li>
<li>따라서 그래프의 강력한 도구인 <strong><img src="https://latex.codecogs.com/png.latex?d">-separation</strong>을 사용하여 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y(x)"> 사이의 독립성을 읽어낼 수가 없습니다.</li>
</ul>
</section>
</section>
<section id="swigs-construction-via-node-splitting" class="level1">
<h1>2. SWIGs: Construction via Node Splitting</h1>
<ul>
<li>SWIGs는 이 문제를 해결하기 위해 <strong>Node Splitting</strong>이라는 직관적이고 강력한 아이디어를 도입합니다.</li>
</ul>
<section id="the-intuition-the-emetic-example" class="level2">
<h2 class="anchored" data-anchor-id="the-intuition-the-emetic-example">2.1. The Intuition: The Emetic Example</h2>
<ul>
<li>Robins, VanderWeele, Richardson(2007)은 다음과 같은 사고 실험을 제안했습니다.</li>
</ul>
<blockquote class="blockquote">
<p>“누군가가 치료제 <img src="https://latex.codecogs.com/png.latex?X=1">을 선택(swallow)하는 것을 관찰하는 동시에, 그가 치료제를 먹지 않았을 때(<img src="https://latex.codecogs.com/png.latex?x=0">)의 결과를 어떻게 알 수 있을까?”</p>
</blockquote>
<ul>
<li><p>환자가 약을 삼키는 순간(<img src="https://latex.codecogs.com/png.latex?X=1">), 즉시 안전한 구토제(emetic)를 투여하여 약이 혈류로 들어가기 전에 토하게 만든다고 가정해 봅시다(<img src="https://latex.codecogs.com/png.latex?x=0">으로 강제).</p></li>
<li><p>환자의 <strong>선택</strong>은 <img src="https://latex.codecogs.com/png.latex?X=1"> (Random nature)로 남아 있습니다.</p></li>
<li><p>하지만 <strong>신체적 노출</strong>은 <img src="https://latex.codecogs.com/png.latex?x=0"> (Fixed nature)이 됩니다.</p></li>
<li><p>이때 관측된 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 <img src="https://latex.codecogs.com/png.latex?Y(x=0)">이 됩니다.</p></li>
<li><p>이 논리에 따라 변수 <img src="https://latex.codecogs.com/png.latex?X">를 두 부분으로 쪼개는 것이 SWIG의 핵심입니다.</p></li>
</ul>
</section>
<section id="construction-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="construction-algorithm">2.2. Construction Algorithm</h2>
<ul>
<li>그래프 <img src="https://latex.codecogs.com/png.latex?G">와 개입(Intervention) 대상 변수 집합 <img src="https://latex.codecogs.com/png.latex?A%20=%20%5C%7BA_1,%20%5Cdots,%20A_k%5C%7D">가 주어졌을 때, SWIG <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BG%7D(a)">는 다음 두 단계로 생성됩니다.</li>
</ul>
<section id="step-1-node-splitting" class="level3">
<h3 class="anchored" data-anchor-id="step-1-node-splitting">Step 1: Node Splitting</h3>
<ul>
<li>개입 변수 <img src="https://latex.codecogs.com/png.latex?A">를 <strong>랜덤 노드(Random node) <img src="https://latex.codecogs.com/png.latex?A"></strong>와 <strong>고정 노드(Fixed node) <img src="https://latex.codecogs.com/png.latex?a"></strong>로 분리합니다.</li>
<li><strong>Random Node <img src="https://latex.codecogs.com/png.latex?A">:</strong> 원래 그래프 <img src="https://latex.codecogs.com/png.latex?G">에서 들어오는 모든 화살표(입력)를 받습니다. (교란요인의 영향을 받는 자연스러운 상태)</li>
<li><strong>Fixed Node <img src="https://latex.codecogs.com/png.latex?a">:</strong> 원래 그래프 <img src="https://latex.codecogs.com/png.latex?G">에서 나가는 모든 화살표(출력)를 보냅니다. (하위 변수들에게 영향을 미치는 고정된 값)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA01/images/node_splitting_schematic.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Node Splitting Schematic. 원형 노드 A가 반원 형태의 상단(Random A)과 하단(Fixed a)으로 분리됨. 상단은 부모의 영향을 받고, 하단은 자식에게 영향을 줌.</figcaption>
</figure>
</div>
</section>
<section id="step-2-relabeling-descendants" class="level3">
<h3 class="anchored" data-anchor-id="step-2-relabeling-descendants">Step 2: Relabeling Descendants</h3>
<ul>
<li>고정 노드 <img src="https://latex.codecogs.com/png.latex?a">의 자손(Descendant) 변수들은 이제 <img src="https://latex.codecogs.com/png.latex?a">의 값에 영향을 받으므로, 변수명을 잠재적 결과(Counterfactual) 형태로 바꿉니다.
<ul>
<li>예: <img src="https://latex.codecogs.com/png.latex?Y">가 <img src="https://latex.codecogs.com/png.latex?A">의 자손이라면, <img src="https://latex.codecogs.com/png.latex?Y%20%5Cto%20Y(a)">로 변경.</li>
<li>중첩된 구조: <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?A">의 자손이고 <img src="https://latex.codecogs.com/png.latex?Y">가 <img src="https://latex.codecogs.com/png.latex?Z">의 자손이라면, <img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20Z(a)">, <img src="https://latex.codecogs.com/png.latex?Y%20%5Cto%20Y(a,%20%5Cdots)">.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA01/images/swig_construction_full.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Full SWIG Construction. 좌측의 원래 그래프(T-&gt;X-&gt;A, Y-&gt;C 등)에서 A와 a가 분리되고, 그 후손인 B, C, D, E, F가 모두 인자 a를 포함하는 변수(B(a), C(a)…)로 변환되는 과정.</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="mathematical-properties" class="level1">
<h1>3. Mathematical Properties</h1>
<ul>
<li>SWIG는 단순한 그림이 아닙니다. 여기에는 엄밀한 확률적 구조가 내재되어 있습니다.</li>
</ul>
<section id="factorization" class="level2">
<h2 class="anchored" data-anchor-id="factorization">3.1. Factorization</h2>
<ul>
<li>원래 그래프 <img src="https://latex.codecogs.com/png.latex?G">가 관측 데이터 <img src="https://latex.codecogs.com/png.latex?P(V)">를 분해(factorize)하듯이, SWIG <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BG%7D(%5Ctilde%7Ba%7D)">는 Counterfactual 분포 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbb%7BV%7D(%5Ctilde%7Ba%7D))">를 분해합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?P(%5Cmathbb%7BV%7D(%5Ctilde%7Ba%7D))%20=%20%5Cprod_%7BW%20%5Cin%20%5Cmathbb%7BV%7D(%5Ctilde%7Ba%7D)%7D%20P(W%20%5Cmid%20pa_%7B%5Cmathcal%7BG%7D(%5Ctilde%7Ba%7D)%7D(W))"></p>
</section>
<section id="modularity-the-linking-assumption" class="level2">
<h2 class="anchored" data-anchor-id="modularity-the-linking-assumption">3.2. Modularity (The Linking Assumption)</h2>
<ul>
<li>SWIG의 가장 강력한 점은 관측 세계와 반사실적 세계를 연결하는 <strong>Modularity</strong> 가정입니다.</li>
<li>SWIG 상의 변수 <img src="https://latex.codecogs.com/png.latex?Y(a)">의 조건부 확률은, 원래 그래프 <img src="https://latex.codecogs.com/png.latex?G">에서 부모 변수 <img src="https://latex.codecogs.com/png.latex?PA_Y"> 중 개입 변수 <img src="https://latex.codecogs.com/png.latex?A">를 고정값 <img src="https://latex.codecogs.com/png.latex?a">로 치환한 조건부 확률과 같습니다. <img src="https://latex.codecogs.com/png.latex?P(Y(a)%20=%20y%20%5Cmid%20Pa_%7BY(a)%7D%20=%20p)%20=%20P(Y%20=%20y%20%5Cmid%20Pa_Y%20=%20p')">
<ul>
<li>단, <img src="https://latex.codecogs.com/png.latex?p'">은 <img src="https://latex.codecogs.com/png.latex?p">에서 개입변수 <img src="https://latex.codecogs.com/png.latex?A">가 <img src="https://latex.codecogs.com/png.latex?a">로 대체된 값</li>
</ul></li>
<li>이 성질 덕분에, 우리는 SWIG 상에서 읽어낸 독립성을 통해 식별(Identification)을 수행할 수 있습니다.</li>
</ul>
</section>
<section id="d-separation-in-swigs" class="level2">
<h2 class="anchored" data-anchor-id="d-separation-in-swigs">3.3. d-separation in SWIGs</h2>
<ul>
<li>SWIG상에서의 <img src="https://latex.codecogs.com/png.latex?d">-separation은 Counterfactual 변수들 간의 조건부 독립성을 의미합니다.</li>
<li>만약 SWIG <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BG%7D(a)">에서 <img src="https://latex.codecogs.com/png.latex?Y(a)">와 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?d">-separated 되어 있다면, 다음이 성립합니다. <img src="https://latex.codecogs.com/png.latex?Y(a)%20%5Cperp%20X%20%5Cquad%20%5Ctext%7Bin%20%7D%20P(%5Cmathbb%7BV%7D(a))"></li>
</ul>
<p>아래 설명이 잘못된 것 같아. 첨부한 이미지를 확인하면 A는 H의 자식이 아니야. 위의 이미지와 아래 <qmd 코드="">를 검토하고, 오류가 없도록 <qmd 코드="">를 재작성해줘.</qmd></qmd></p>
</section>
</section>
<section id="application-1-complex-independence-the-kite-graph" class="level1">
<h1>4. Application 1: Complex Independence (The Kite Graph)</h1>
<ul>
<li>SWIG의 위력을 보여주는 유명한 예시인 “Kite Graph”를 살펴봅시다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA01/images/kite_graph_swig.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: The Kite Graph Problem. (좌상단) A -&gt; Z -&gt; Y 구조에 H가 Z와 B를 교란하는 복잡한 그래프. (우측) 이에 대응하는 SWIG. A와 a가 분리되고 Z는 Z(a), Y는 Y(a,b), B는 B(a)로 변환됨.</figcaption>
</figure>
</div>
<ul>
<li><strong>문제 상황:</strong>
<ul>
<li>원래 그래프에서 <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 매개변수이고, <img src="https://latex.codecogs.com/png.latex?H">는 <img src="https://latex.codecogs.com/png.latex?Z">와 <img src="https://latex.codecogs.com/png.latex?B">에 동시에 영향을 주는 숨겨진 교란요인입니다. (<img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?H">는 독립적입니다.)</li>
<li>여기서 다음의 조건부 독립성이 성립하는지 묻는다면 매우 헷갈립니다. <img src="https://latex.codecogs.com/png.latex?Y(a,%20b)%20%5Cperp%20B(a)%20%5Cmid%20Z(a),%20A"></li>
</ul></li>
<li><strong>SWIG를 통한 해결:</strong>
<ul>
<li>SWIG를 그려보면(위 그림 참조), <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?B">에 개입하여 <img src="https://latex.codecogs.com/png.latex?a,%20b">로 고정했을 때:
<ul>
<li><ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Y(a,%20b)">는 오직 <img src="https://latex.codecogs.com/png.latex?Z(a)">와 고정된 값 <img src="https://latex.codecogs.com/png.latex?b">에 의해서만 결정됩니다. (즉, <img src="https://latex.codecogs.com/png.latex?Y(a,b)">로 들어오는 화살표는 <img src="https://latex.codecogs.com/png.latex?Z(a)">에서 오는 것뿐입니다.)</li>
</ol></li>
<li><ol start="2" type="1">
<li><img src="https://latex.codecogs.com/png.latex?Y(a,%20b)">와 <img src="https://latex.codecogs.com/png.latex?B(a)">를 잇는 모든 경로는 반드시 <img src="https://latex.codecogs.com/png.latex?Z(a)">를 통과해야 합니다. (예: <img src="https://latex.codecogs.com/png.latex?Y(a,b)%20%5Cleftarrow%20Z(a)%20%5Cleftarrow%20H%20%5Crightarrow%20B(a)"> 등)</li>
</ol></li>
<li><ol start="3" type="1">
<li>이때 <img src="https://latex.codecogs.com/png.latex?Z(a)">가 조건부로 주어지면(conditioned), <img src="https://latex.codecogs.com/png.latex?Z(a)">를 통하는 이 경로들이 모두 차단(blocked)됩니다.</li>
</ol></li>
</ul></li>
<li>또한 원래 그래프에서 <img src="https://latex.codecogs.com/png.latex?A">는 <img src="https://latex.codecogs.com/png.latex?H">와 독립적이며, SWIG 상에서 <img src="https://latex.codecogs.com/png.latex?A">(랜덤 변수)는 <img src="https://latex.codecogs.com/png.latex?Z(a)">로 가는 링크가 끊겨 있어(<img src="https://latex.codecogs.com/png.latex?a">로 대체됨) 다른 변수들과 d-separated 되어 있습니다.</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?Z(a)">와 <img src="https://latex.codecogs.com/png.latex?A">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?Y(a,b)">와 <img src="https://latex.codecogs.com/png.latex?B(a)">는 독립이므로, 위 명제는 <strong>참(True)</strong>입니다.</li>
<li>이는 Pearl의 <img src="https://latex.codecogs.com/png.latex?do">-calculus만으로는 직관적으로 파악하기 어렵거나, 과거에 잘못 판단되었던 문제입니다.</li>
</ul></li>
</ul>
</section>
<section id="application-2-adjustment-formula-derivation" class="level1">
<h1>5. Application 2: Adjustment Formula Derivation</h1>
<ul>
<li>교란요인 <img src="https://latex.codecogs.com/png.latex?L">이 있을 때 <img src="https://latex.codecogs.com/png.latex?P(Y(x)=y)">를 구하는 표준적인 Backdoor Adjustment 공식을 SWIG를 이용해 유도해 보겠습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA01/images/adjustment_formula_swig.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: Adjustment for Confounding SWIG. L -&gt; X, L -&gt; Y, X -&gt; Y 구조의 그래프를 Node Splitting하여 SWIG로 변환. X와 x가 분리되고 Y는 Y(x)가 됨. L과 Y(x) 사이의 경로가 끊겨 있음이 보임.</figcaption>
</figure>
</div>
<ul>
<li><p><strong>목표:</strong> <img src="https://latex.codecogs.com/png.latex?P(Y(x)=y)"> 식별하기.</p></li>
<li><p><strong>Derivation:</strong></p></li>
<li><ol type="1">
<li><strong>SWIG에서 독립성 읽기:</strong></li>
</ol>
<ul>
<li>SWIG 그림을 보면 <img src="https://latex.codecogs.com/png.latex?X"> (random node)와 <img src="https://latex.codecogs.com/png.latex?Y(x)"> 사이에는 <img src="https://latex.codecogs.com/png.latex?L">을 통하는 뒷문 경로가 있습니다. 하지만 <img src="https://latex.codecogs.com/png.latex?X">로 들어오는 화살표가 없고 <img src="https://latex.codecogs.com/png.latex?x"> (fixed node)에서 나가는 화살표만 <img src="https://latex.codecogs.com/png.latex?Y(x)">로 가므로, <img src="https://latex.codecogs.com/png.latex?L">을 조건부로 주면 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y(x)">는 독립입니다. <img src="https://latex.codecogs.com/png.latex?Y(x)%20%5Cperp%20X%20%5Cmid%20L">
<ul>
<li>주의: SWIG 그림 상에서 <img src="https://latex.codecogs.com/png.latex?L%20%5Cto%20X">이고 <img src="https://latex.codecogs.com/png.latex?L%20%5Cto%20Y(x)">입니다. 따라서 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y(x)">는 <img src="https://latex.codecogs.com/png.latex?L">을 통해 d-connected 됩니다. 하지만 <img src="https://latex.codecogs.com/png.latex?L">을 알면(<img src="https://latex.codecogs.com/png.latex?L"> given), 이 경로가 차단되므로 <img src="https://latex.codecogs.com/png.latex?Y(x)%20%5Cperp%20X%20%5Cmid%20L">이 성립합니다.</li>
</ul></li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>전확률의 법칙:</strong> <img src="https://latex.codecogs.com/png.latex?P(Y(x)=y)%20=%20%5Csum_%7Bl%7D%20P(Y(x)=y%20%5Cmid%20L=l)%20P(L=l)"></li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>조건부 독립성 적용 (<img src="https://latex.codecogs.com/png.latex?Y(x)%20%5Cperp%20X%20%5Cmid%20L">):</strong> <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bl%7D%20P(Y(x)=y%20%5Cmid%20L=l,%20X=x)%20P(L=l)"></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X=x"> 조건을 추가해도 확률이 변하지 않음</li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>Consistency (Modularity):</strong> <img src="https://latex.codecogs.com/png.latex?X=x">일 때, <img src="https://latex.codecogs.com/png.latex?Y(x)">는 관측된 <img src="https://latex.codecogs.com/png.latex?Y">와 같습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bl%7D%20P(Y=y%20%5Cmid%20L=l,%20X=x)%20P(L=l)"></li>
</ol></li>
<li><p>이로써 우리가 잘 아는 Backdoor Adjustment 공식이 유도되었습니다.</p></li>
<li><p>SWIG는 이 과정에서 “왜 <img src="https://latex.codecogs.com/png.latex?X=x">를 조건부에 넣을 수 있는가?”에 대한 명확한 그래픽적 근거를 제공합니다.</p></li>
</ul>
</section>
<section id="conclusion" class="level1">
<h1>6. Conclusion</h1>
<ul>
<li>SWIGs(Single World Intervention Graphs)는 인과추론의 두 언어, 즉 <strong>Potential Outcomes</strong>와 <strong>Graphs</strong>를 통역하는 강력한 도구입니다.
<ul>
<li><ol type="1">
<li><strong>Unification:</strong> <img src="https://latex.codecogs.com/png.latex?Y(x)">와 같은 반사실적 변수를 그래프의 노드로 명시적으로 포함시킵니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Node Splitting:</strong> 처치 변수를 ’관찰된 선택(<img src="https://latex.codecogs.com/png.latex?X">)’과 ’물리적 개입(<img src="https://latex.codecogs.com/png.latex?x">)’으로 분리하여 인과적 상황을 모델링합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Clarity:</strong> 복잡한 조건부 독립성이나 식별 문제를 <img src="https://latex.codecogs.com/png.latex?d">-separation만으로 풀 수 있게 해줍니다.</li>
</ol></li>
</ul></li>
<li>이제 우리는 “가정을 그래프에 그리고, 그 그래프에서 반사실적 결론을 읽어내는” 완전한 파이프라인을 갖게 되었습니다.</li>
</ul>
<hr>
<p><strong>References</strong> [1] Richardson, T. S., &amp; Robins, J. M. (2013). Single World Intervention Graphs (SWIGs): Unifying the Counterfactual and Graphical Approaches to Causality. CSSS Technical Report. [2] Slide source: Richardson, T. (2014). Data, Society and Inference Seminar, Stanford.</p>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/LA01/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] Appendix 02. Direct and Indirect Effects (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/LA02/part-01/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li>인과추론(Causal Inference)의 핵심 목표 중 하나는 단순히 “X가 Y에 영향을 미치는가?”를 넘어, “X가 <strong>어떤 경로(Mechanism)</strong>를 통해 Y에 영향을 미치는가?”를 이해하는 것입니다.</li>
<li>이를 위해 우리는 총 효과(Total Effect)를 직접 효과(Direct Effect)와 간접 효과(Indirect Effect)로 분해(Decomposition)하고자 합니다.</li>
<li>본 포스트는 비선형 시스템(Nonlinear System)에서의 효과 분해와 <strong>Controlled Direct Effect (CDE)</strong> 및 <strong>Natural Direct Effect (NDE)</strong>의 개념적 차이를 정리합니다.</li>
</ul>
</section>
<section id="problem-setup-notation" class="level1">
<h1>1. Problem Setup &amp; Notation</h1>
<section id="causal-graph-mediation-triangle" class="level2">
<h2 class="anchored" data-anchor-id="causal-graph-mediation-triangle">1.1. Causal Graph (Mediation Triangle)</h2>
<ul>
<li>가장 기본적인 매개 모형(Mediation Model)은 다음과 같은 DAG(Directed Acyclic Graph)로 표현됩니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-01/images/mediation_triangle.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Mediation Triangle DAG. 변수 <img src="https://latex.codecogs.com/png.latex?X">는 원인 변수(Treatment), <img src="https://latex.codecogs.com/png.latex?Y">는 결과 변수(Outcome), <img src="https://latex.codecogs.com/png.latex?Z">는 매개 변수(Mediator)를 나타낸다. <img src="https://latex.codecogs.com/png.latex?X">에서 <img src="https://latex.codecogs.com/png.latex?Y">로 가는 직접 경로(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">)와 <img src="https://latex.codecogs.com/png.latex?Z">를 경유하는 간접 경로(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z%20%5Cto%20Y">)가 존재한다.</figcaption>
</figure>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">: 원인 변수 (Input / Treatment)</li>
<li><img src="https://latex.codecogs.com/png.latex?Z">: 매개 변수 (Intermediate variable / Mediator)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y">: 결과 변수 (Output / Outcome)</li>
</ul>
</section>
<section id="total-effects-te" class="level2">
<h2 class="anchored" data-anchor-id="total-effects-te">1.2. Total Effects (TE)</h2>
<ul>
<li>총 효과(Total Effect)는 외부 개입(External Intervention) <img src="https://latex.codecogs.com/png.latex?do(X=x)">가 가해졌을 때, <img src="https://latex.codecogs.com/png.latex?Y">가 <img src="https://latex.codecogs.com/png.latex?y">일 확률의 변화로 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y_x%20=%20y)%20=%20P(y%20%7C%20do(x))%0A"></p>
<ul>
<li>이는 <img src="https://latex.codecogs.com/png.latex?X">가 변할 때 <img src="https://latex.codecogs.com/png.latex?Z">를 통한 경로와 직접 경로를 모두 포함한 <img src="https://latex.codecogs.com/png.latex?Y">의 전체적인 변화 양상을 나타냅니다.</li>
</ul>
</section>
</section>
<section id="direct-effects-definition-challenge" class="level1">
<h1>2. Direct Effects: Definition &amp; Challenge</h1>
<section id="conceptual-definition" class="level2">
<h2 class="anchored" data-anchor-id="conceptual-definition">2.1. Conceptual Definition</h2>
<ul>
<li>직접 효과(Direct Effect)의 직관적인 정의는 “모델 내 다른 변수들에 의해 매개되지 않는 영향(influence not mediated by other variables)”입니다.</li>
<li>조금 더 형식적으로는, <strong>“모든 다른 요인들을 고정한 상태(holding fixed)에서 <img src="https://latex.codecogs.com/png.latex?X">의 변화에 대한 <img src="https://latex.codecogs.com/png.latex?Y">의 민감도”</strong>라고 정의할 수 있습니다.</li>
<li>그래프 관점에서 보면, 이는 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z%20%5Cto%20Y">와 같은 모든 간접 경로를 차단하고, 오직 직접 경로 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">만 남겨두었을 때의 효과를 의미합니다.</li>
</ul>
</section>
<section id="linear-vs.-nonlinear-systems" class="level2">
<h2 class="anchored" data-anchor-id="linear-vs.-nonlinear-systems">2.2. Linear vs.&nbsp;Nonlinear Systems</h2>
<ul>
<li>선형 시스템(Linear System)에서는 간접 효과를 단순히 “총 효과 - 직접 효과”로 정의할 수 있습니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BIndirect%20Effect%7D%20=%20%5Ctext%7BTotal%20Effect%7D%20-%20%5Ctext%7BDirect%20Effect%7D%20%5Cquad%20(%5Ctext%7Bin%20Linear%7D)%0A"></li>
<li>하지만 <strong>비선형 시스템(Nonlinear System)</strong>에서는 변수들을 단순히 상수로 고정하는 것만으로는 간접 효과를 정확히 측정할 수 없으며, 고정하는 값(reference value)에 따라 효과의 크기가 달라지는 문제가 발생합니다.</li>
<li>이로 인해 “간접 효과”의 정의가 불완전해질 수 있습니다.</li>
</ul>
</section>
</section>
<section id="motivation-why-distinguish-direct-effects" class="level1">
<h1>3. Motivation: Why Distinguish Direct Effects?</h1>
<ul>
<li>왜 우리는 굳이 총 효과를 쪼개서 직접 효과를 보려고 할까요? 아래는 세 가지 주요 예시를 통해 그 동기를 설명합니다.</li>
</ul>
<section id="transportability-birth-control-pill" class="level2">
<h2 class="anchored" data-anchor-id="transportability-birth-control-pill">3.1. Transportability (Birth Control Pill)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-01/images/pill_thrombosis_dag.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: 피임약(Birth-control pill)이 혈전증(Thrombosis)에 미치는 영향. 피임약은 혈전증 위험을 직접적으로 높일 수 있지만(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">), 동시에 임신 확률(Pregnancy rates)을 낮춤으로써 혈전증 위험을 간접적으로 낮추는 경로(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z%20%5Cto%20Y">)도 가진다.</figcaption>
</figure>
</div>
<ul>
<li>피임약 사례에서:
<ul>
<li><strong>Direct Path:</strong> 피임약 성분이 혈액에 직접 작용하여 혈전증 위험 증가.</li>
<li><strong>Indirect Path:</strong> 피임약 복용 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 임신 감소 <img src="https://latex.codecogs.com/png.latex?%5Cto"> (임신으로 인한) 혈전증 위험 감소.</li>
</ul></li>
<li>여기서 <strong>직접 효과</strong>는 생물학적 메커니즘에 기반하므로 사회적 요인(예: 결혼 여부, 문화적 배경 등)에 불변(Invariant)할 가능성이 높습니다.</li>
<li>반면 총 효과는 사회적 요인에 따라 달라지는 ’임신율’에 의존합니다.</li>
<li>따라서 <strong>직접 효과가 정책 분석이나 과학적 설명에 있어 더 이식성(Transportability)이 높고 유용</strong>합니다.</li>
</ul>
</section>
<section id="legal-liability-hiring-discrimination" class="level2">
<h2 class="anchored" data-anchor-id="legal-liability-hiring-discrimination">3.2. Legal Liability (Hiring Discrimination)</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-01/images/hiring_discrimination_dag.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: 채용 차별(Discrimination) 모델. 인종(Race)이 채용(Hiring)에 미치는 영향. 자격 요건(Qualification)을 매개변수로 한다. 법적으로는 자격 요건을 통제한 상태에서의 인종에 따른 직접적인 차별(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">)만이 제재 대상이 된다.</figcaption>
</figure>
</div>
<ul>
<li>채용 차별 소송(Title VII 등)에서는 “자격 요건(Qualification)”을 매개로 한 간접 효과(예: 교육 기회의 불평등으로 인한 자격 미달)보다는, <strong>채용 결정에 있어서의 직접적인 효과(Direct Effect)</strong>에 관심을 둡니다.</li>
<li>법적인 질문은 다음과 같습니다:
<ul>
<li>“다른 모든 조건이 동일할 때(all else being equal), 지원자의 인종이 달랐더라도 고용주가 같은 결정을 내렸을 것인가?”</li>
</ul></li>
</ul>
</section>
<section id="side-effects-aspirin-example" class="level2">
<h2 class="anchored" data-anchor-id="side-effects-aspirin-example">3.3. Side Effects (Aspirin Example)</h2>
<ul>
<li>약물(Drug)이 두통(Headache)이라는 부작용을 일으키고, 이로 인해 환자가 아스피린(Aspirin)을 복용하여 치료 결과(Outcome)에 영향을 주는 상황입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-01/images/drug_aspirin_dag.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: 약물(Drug)과 치료 결과(Outcome) 사이의 관계. 약물은 두통을 유발하고, 두통은 아스피린 복용을 유발하며, 아스피린은 결과에 영향을 미친다. 또한 약물 자체가 결과에 미치는 직접 경로도 존재한다.</figcaption>
</figure>
</div>
<ul>
<li>이때 제약회사는 아스피린 복용 여부와 무관한 약물의 <strong>순수한 효능(직접 효과)</strong>을 입증하고 싶을 수도 있고, 반대로 아스피린 복용까지 포함한 <strong>실제 임상 환경에서의 효과(총 효과)</strong>를 알고 싶을 수도 있습니다.</li>
</ul>
</section>
</section>
<section id="formalizing-direct-effects-prescriptive-vs.-descriptive" class="level1">
<h1>4. Formalizing Direct Effects: Prescriptive vs.&nbsp;Descriptive</h1>
<ul>
<li>비선형 시스템에서 직접 효과를 정의할 때, 매개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 <strong>어떤 값으로 고정하느냐</strong>에 따라 두 가지 중요한 개념으로 나뉩니다.</li>
</ul>
<section id="prescriptive-formulation-controlled-direct-effect-cde" class="level2">
<h2 class="anchored" data-anchor-id="prescriptive-formulation-controlled-direct-effect-cde">4.1. Prescriptive Formulation (Controlled Direct Effect, CDE)</h2>
<ul>
<li><strong>“처방적(Prescriptive)”</strong> 해석은 정책적으로 <img src="https://latex.codecogs.com/png.latex?Z">를 특정 값 <img src="https://latex.codecogs.com/png.latex?z">로 강제 고정(Controlled)했을 때의 효과를 의미합니다.</li>
<li><strong>정의:</strong> 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">를 특정 수준 <img src="https://latex.codecogs.com/png.latex?z">로 고정(Intervention)한 상태에서, <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x%5E*">에서 <img src="https://latex.codecogs.com/png.latex?x">로 바꿀 때 <img src="https://latex.codecogs.com/png.latex?Y">의 변화량.</li>
<li><strong>수식:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20CDE(z)%20=%20P(Y_%7Bx,%20z%7D%20=%20y)%20-%20P(Y_%7Bx%5E*,%20z%7D%20=%20y)%0A%20%20"></li>
<li><strong>의미:</strong> “모든 환자에게 아스피린 복용을 강제(<img src="https://latex.codecogs.com/png.latex?Z=z">)한다면, 치료(<img src="https://latex.codecogs.com/png.latex?X">)가 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 미치는 효과는 무엇인가?”. 실제 환자의 자율적인 아스피린 복용 성향은 무시됩니다.</li>
<li><strong>특징:</strong> 선형 시스템에서는 <img src="https://latex.codecogs.com/png.latex?z"> 값에 상관없이 일정하지만, 비선형 시스템에서는 <img src="https://latex.codecogs.com/png.latex?z">의 레벨에 따라 효과가 달라집니다.</li>
</ul>
</section>
<section id="descriptive-formulation-natural-direct-effect-nde" class="level2">
<h2 class="anchored" data-anchor-id="descriptive-formulation-natural-direct-effect-nde">4.2. Descriptive Formulation (Natural Direct Effect, NDE)</h2>
<ul>
<li><p><strong>“서술적(Descriptive)”</strong> 해석은 인위적인 개입 없이, 자연스러운 상태에서의 효과를 설명하려 합니다.</p></li>
<li><p><strong>정의:</strong> <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">에서 <img src="https://latex.codecogs.com/png.latex?x">로 변할 때, <img src="https://latex.codecogs.com/png.latex?Z">는 <strong>개입이 없을 때(<img src="https://latex.codecogs.com/png.latex?X=x%5E*">) 자연스럽게 가졌을 값</strong>으로 유지한 상태에서의 <img src="https://latex.codecogs.com/png.latex?Y">의 변화량.</p></li>
<li><p><strong>수식:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20NDE%20=%20P(Y_%7Bx,%20Z(x%5E*)%7D%20=%20y)%20-%20P(Y_%7Bx%5E*,%20Z(x%5E*)%7D%20=%20y)%0A%20%20"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?Z(x%5E*)">는 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때 개인이 자연적으로 갖게 되는 <img src="https://latex.codecogs.com/png.latex?Z">의 값을 의미합니다.</li>
</ul></li>
<li><p><strong>의미:</strong> “치료를 받지 않았을 때(<img src="https://latex.codecogs.com/png.latex?x%5E*">) 환자가 복용했을 아스피린 양(<img src="https://latex.codecogs.com/png.latex?Z(x%5E*)">)을 그대로 유지한다고 가정할 때, 치료(<img src="https://latex.codecogs.com/png.latex?x">) 자체가 결과에 미치는 영향은 무엇인가?”.</p></li>
<li><p><strong>필요성:</strong> 이 개념은 개인의 자연적인 행동(Natural behavior)에 대한 지식을 필요로 합니다.</p></li>
</ul>
</section>
<section id="comparison-why-nde-matters" class="level2">
<h2 class="anchored" data-anchor-id="comparison-why-nde-matters">4.3. Comparison: Why NDE matters?</h2>
<ul>
<li><p>어떤 환자는 치료를 받을 때만(<img src="https://latex.codecogs.com/png.latex?X=x">) 두통 때문에 아스피린을 먹고, 아스피린이 있어야만 치료 효과가 나타난다고 가정해 봅시다.</p></li>
<li><ol type="1">
<li><strong>NDE 관점:</strong> 치료를 받지 않은 상황(<img src="https://latex.codecogs.com/png.latex?x%5E*">)을 기준(baseline)으로 하면, 이 환자는 아스피린을 먹지 않았을 것입니다(<img src="https://latex.codecogs.com/png.latex?Z(x%5E*)=%5Ctext%7Bno%20aspirin%7D">). 이 상태로 고정하고 치료만(<img src="https://latex.codecogs.com/png.latex?x">) 하면, 아스피린이 없으므로 치료 효과가 없습니다. 즉, <strong>NDE = 0</strong>입니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>CDE 관점:</strong> 만약 우리가 강제로 아스피린을 먹인다면(<img src="https://latex.codecogs.com/png.latex?Z=%5Ctext%7Baspirin%7D">), 치료 효과가 나타납니다. 즉, <strong>CDE <img src="https://latex.codecogs.com/png.latex?%5Cneq"> 0</strong>입니다.</li>
</ol></li>
<li><p><strong>언제 무엇을 쓰는가?</strong></p>
<ul>
<li><strong>CDE (Prescriptive):</strong> 전체 인구에게 특정 수준의 매개변수(예: 아스피린 용량)를 <strong>정책적으로 규정</strong>할 때 유용합니다.</li>
<li><strong>NDE (Descriptive/Attributional):</strong> 관찰된 결과가 순수하게 치료 자체 때문인지, 아니면 치료로 인해 바뀐 매개변수(아스피린 복용) 때문인지를 <strong>귀인(Attribution)</strong>할 때 유용합니다. 제약회사가 부작용(두통)을 없앴을 때 여전히 약효가 있을지를 판단하려면, 사람마다 제각각인 아스피린 복용 성향을 반영한 <strong>평균적인 자연 직접 효과(Average NDE)</strong>를 봐야 합니다.</li>
</ul></li>
</ul>
</section>
</section>
<section id="indirect-effects" class="level1">
<h1>5. Indirect Effects</h1>
<ul>
<li>직접 효과의 정의가 명확해지면, 간접 효과(Indirect Effect) 역시 이에 상응하여 정의할 수 있습니다.</li>
</ul>
<section id="challenge-in-prescriptive-definition" class="level2">
<h2 class="anchored" data-anchor-id="challenge-in-prescriptive-definition">5.1. Challenge in Prescriptive Definition</h2>
<ul>
<li>간접 효과는 “처방적(Prescriptive)”으로 정의하기 어렵습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 영향을 <img src="https://latex.codecogs.com/png.latex?Z">를 통해서만 전달되도록 하려면, 직접 경로 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">를 차단해야 하는데, 이를 위해 변수를 고정(holding constant)하는 방식으로는 불가능하기 때문입니다.</li>
<li>직접 경로를 차단하려고 <img src="https://latex.codecogs.com/png.latex?X">를 고정하면 간접 경로의 시작점도 고정되어 버립니다.</li>
</ul>
</section>
<section id="descriptive-interpretation-natural-indirect-effect-nie" class="level2">
<h2 class="anchored" data-anchor-id="descriptive-interpretation-natural-indirect-effect-nie">5.2. Descriptive Interpretation (Natural Indirect Effect, NIE)</h2>
<ul>
<li><p>따라서 간접 효과는 주로 서술적(Descriptive) 관점에서 정의됩니다.</p></li>
<li><p><strong>아이디어:</strong> <img src="https://latex.codecogs.com/png.latex?X">는 <img src="https://latex.codecogs.com/png.latex?x%5E*"> (대조군 상태)로 묶어두어 직접 효과를 차단합니다. 그 상태에서 <img src="https://latex.codecogs.com/png.latex?Z">만 <strong>“<img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x"> (처치군)로 변했을 때 변했을 법한 값 <img src="https://latex.codecogs.com/png.latex?Z(x)">”</strong>로 변화시킵니다.</p></li>
<li><p><strong>수식:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20NIE%20=%20P(Y_%7Bx%5E*,%20Z(x)%7D%20=%20y)%20-%20P(Y_%7Bx%5E*,%20Z(x%5E*)%7D%20=%20y)%0A%20%20"></p></li>
<li><p><strong>해석:</strong></p>
<ul>
<li>채용 차별 예시로 들자면, “성별을 묻는 것을 금지하여(<img src="https://latex.codecogs.com/png.latex?x%5E*">) 성별에 따른 직접적 차별을 막더라도, 자격 요건(<img src="https://latex.codecogs.com/png.latex?Z">)이 성별에 따라 달라짐(<img src="https://latex.codecogs.com/png.latex?Z(x)">)으로 인해 발생하는 잔여 채용 격차는 얼마인가?”를 묻는 것과 같습니다.</li>
<li>이는 정책 도입 전 데이터로부터 <strong>평균 간접 효과(Average Indirect Effects)</strong>를 추정하는 문제가 됩니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="summary" class="level1">
<h1>6. Summary</h1>
<ul>
<li>본 포스트에서는 비선형 인과 모형에서 효과를 분해하는 두 가지 관점을 살펴보았습니다.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">구분</th>
<th style="text-align: left;">Prescriptive (Controlled)</th>
<th style="text-align: left;">Descriptive (Natural)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Direct Effect</strong></td>
<td style="text-align: left;"><strong>CDE</strong>: <img src="https://latex.codecogs.com/png.latex?Z">를 특정 상수 <img src="https://latex.codecogs.com/png.latex?z">로 강제 고정.<br><img src="https://latex.codecogs.com/png.latex?P(Y_%7Bx,z%7D)%20-%20P(Y_%7Bx%5E*,z%7D)"></td>
<td style="text-align: left;"><strong>NDE</strong>: <img src="https://latex.codecogs.com/png.latex?Z">를 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때의 자연적 수준 <img src="https://latex.codecogs.com/png.latex?Z(x%5E*)">로 유지.<br><img src="https://latex.codecogs.com/png.latex?P(Y_%7Bx,Z(x%5E*)%7D)%20-%20P(Y_%7Bx%5E*,Z(x%5E*)%7D)"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Indirect Effect</strong></td>
<td style="text-align: left;">정의하기 어려움 (불가능)</td>
<td style="text-align: left;"><strong>NIE</strong>: <img src="https://latex.codecogs.com/png.latex?X=x%5E*">로 고정하되, <img src="https://latex.codecogs.com/png.latex?Z">만 <img src="https://latex.codecogs.com/png.latex?Z(x)">로 변화시킴.<br><img src="https://latex.codecogs.com/png.latex?P(Y_%7Bx%5E*,Z(x)%7D)%20-%20P(Y_%7Bx%5E*,Z(x%5E*)%7D)"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>주요 활용</strong></td>
<td style="text-align: left;">정책적 개입(Intervention), 프로토콜 수립</td>
<td style="text-align: left;">현상의 설명(Explanation), 원인 귀속(Attribution), 차별 분석</td>
</tr>
</tbody>
</table>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/LA02/part-01/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] Appendix 02. Direct and Indirect Effects (Part 2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/LA02/part-02/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li>인과추론에서 총 효과(Total Effect)를 구하는 것만으로는 충분하지 않은 경우가 많습니다.</li>
<li>우리는 원인 변수 <img src="https://latex.codecogs.com/png.latex?X">가 결과 변수 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 미칠 때, 그 메커니즘이 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 <strong>직접(Directly)</strong> 영향을 미친 것인지, 아니면 중개 변수(Mediator, <img src="https://latex.codecogs.com/png.latex?Z">)를 거쳐 <strong>간접(Indirectly)</strong> 영향을 미친 것인지 구분하고자 합니다.</li>
<li>이번 포스트에서는 Pearl의 구조적 인과 모형(Structural Causal Model) 프레임워크를 기반으로 <strong>통제된 직접 효과(Controlled Direct Effect, CDE)</strong>와 <strong>자연 직접 효과(Natural Direct Effect, NDE)</strong>의 개념을 정의하고, 이를 실험적 데이터와 관찰 데이터에서 어떻게 식별(Identification)할 수 있는지 수식적으로 유도해 보겠습니다.</li>
</ul>
</section>
<section id="notation-framework" class="level1">
<h1>1. Notation &amp; Framework</h1>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">: 통제 변수 (Control variable, 원인)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y">: 반응 변수 (Response variable, 결과)</li>
<li><img src="https://latex.codecogs.com/png.latex?Z">: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 모든 중개 변수 집합 (Intermediate variables)</li>
<li><img src="https://latex.codecogs.com/png.latex?W">: 공변량 집합 (Covariates, <img src="https://latex.codecogs.com/png.latex?X">나 <img src="https://latex.codecogs.com/png.latex?Z">의 자손이 아님)</li>
</ul>
<section id="counterfactual-notation" class="level2">
<h2 class="anchored" data-anchor-id="counterfactual-notation">Counterfactual Notation</h2>
<ul>
<li>인과 효과를 정의하기 위해 잠재적 결과(Potential Outcome) 표기법을 사용합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?Y_x(u)"></p>
<ul>
<li>이는 단위(unit) <img src="https://latex.codecogs.com/png.latex?u">에서 <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x">로 설정(<img src="https://latex.codecogs.com/png.latex?do(X=x)">)했을 때 관측되는 <img src="https://latex.codecogs.com/png.latex?Y">의 값을 의미합니다.</li>
</ul>
</section>
</section>
<section id="controlled-direct-effects-cde" class="level1">
<h1>2. Controlled Direct Effects (CDE)</h1>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">2.1. Definition</h2>
<ul>
<li><p>가장 직관적인 직접 효과의 정의는 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 특정 값 <img src="https://latex.codecogs.com/png.latex?z">로 고정해버리는 것입니다. 이를 <strong>통제된 직접 효과(CDE)</strong>라고 합니다.</p></li>
<li><p>단위 <img src="https://latex.codecogs.com/png.latex?u">에 대하여, 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 <img src="https://latex.codecogs.com/png.latex?z">로 고정한 상태에서 <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x%5E*">에서 <img src="https://latex.codecogs.com/png.latex?x">로 바꿀 때의 효과는 다음과 같습니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?CDE_z(x,%20x%5E*;%20Y,%20u)%20=%20Y_%7Bxz%7D(u)%20-%20Y_%7Bx%5E*z%7D(u)"></p>
</section>
<section id="average-cde" class="level2">
<h2 class="anchored" data-anchor-id="average-cde">2.2. Average CDE</h2>
<ul>
<li>개별 단위가 아닌 모집단 전체에 대한 평균 효과는 기대값을 취하여 정의합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?CDE_z(x,%20x%5E*;%20Y)%20=%20%5Cmathbb%7BE%7D%5BY_%7Bxz%7D%20-%20Y_%7Bx%5E*z%7D%5D"></p>
<ul>
<li>이 척도는 정책적으로 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 물리적으로 통제(예: 처방, 강제 설정)할 수 있을 때 유용합니다. 하지만 <img src="https://latex.codecogs.com/png.latex?Z">를 인위적으로 고정하는 것이 불가능하거나 무의미한 상황에서는 한계가 있습니다.</li>
</ul>
</section>
</section>
<section id="natural-direct-effects-nde" class="level1">
<h1>3. Natural Direct Effects (NDE)</h1>
<section id="motivation-formulation" class="level2">
<h2 class="anchored" data-anchor-id="motivation-formulation">3.1. Motivation &amp; Formulation</h2>
<ul>
<li><p>우리가 자연스럽게 생각하는 “직접 효과”는, <img src="https://latex.codecogs.com/png.latex?X">가 변할 때 <img src="https://latex.codecogs.com/png.latex?Z">를 억지로 고정하는 것이 아니라, <strong><img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">의 기준 값(<img src="https://latex.codecogs.com/png.latex?x%5E*">) 하에서 가졌을 자연스러운 값</strong>을 유지하게 한 상태에서 <img src="https://latex.codecogs.com/png.latex?X">만 <img src="https://latex.codecogs.com/png.latex?x">로 바꾸는 효과입니다. 이를 <strong>자연 직접 효과(NDE)</strong>라고 합니다.</p></li>
<li><p>이를 표현하기 위해 <strong>중첩된 반사실(Nested Counterfactual)</strong> 개념이 필요합니다.</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D(u)">: <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">일 때 단위 <img src="https://latex.codecogs.com/png.latex?u">가 가질 <img src="https://latex.codecogs.com/png.latex?Z">의 값.</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bx%20Z_%7Bx%5E*%7D(u)%7D(u)">: <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x">로 설정하되, <img src="https://latex.codecogs.com/png.latex?Z">는 <strong>“<img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">였더라면 가졌을 값”</strong>으로 설정했을 때의 <img src="https://latex.codecogs.com/png.latex?Y">.</li>
</ul></li>
</ul>
</section>
<section id="definition-1" class="level2">
<h2 class="anchored" data-anchor-id="definition-1">3.2. Definition</h2>
<ul>
<li>단위 <img src="https://latex.codecogs.com/png.latex?u">에서의 자연 직접 효과는 다음과 같이 정의됩니다. <img src="https://latex.codecogs.com/png.latex?NDE(x,%20x%5E*;%20Y,%20u)%20=%20Y_%7Bx%20Z_%7Bx%5E*%7D(u)%7D(u)%20-%20Y_%7Bx%5E*%7D(u)">
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*%7D(u)">는 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때의 자연스러운 <img src="https://latex.codecogs.com/png.latex?Y">값이므로, <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*%20Z_%7Bx%5E*%7D(u)%7D(u)">와 동일합니다. 즉, <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때의 값으로 고정된 상태에서, <img src="https://latex.codecogs.com/png.latex?X">입력만 <img src="https://latex.codecogs.com/png.latex?x">로 바뀐 효과를 측정합니다.</li>
</ul></li>
<li>모집단 평균에 대한 NDE는 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?NDE(x,%20x%5E*;%20Y)%20=%20%5Cmathbb%7BE%7D%5BY_%7Bx%20Z_%7Bx%5E*%7D%7D%5D%20-%20%5Cmathbb%7BE%7D%5BY_%7Bx%5E*%7D%5D"></p>
</section>
</section>
<section id="experimental-identification-of-nde" class="level1">
<h1>4. Experimental Identification of NDE</h1>
<section id="the-identification-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-identification-problem">4.1. The Identification Problem</h2>
<ul>
<li>NDE 식별의 핵심 어려움은 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_%7Bx%20Z_%7Bx%5E*%7D%7D%5D"> 항에 있습니다. 이 항은 <img src="https://latex.codecogs.com/png.latex?X=x">일 때의 결과 <img src="https://latex.codecogs.com/png.latex?Y">와 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때의 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 동시에 요구하기 때문입니다.</li>
<li>현실에서는 하나의 단위에 대해 동시에 서로 다른 두 가지 <img src="https://latex.codecogs.com/png.latex?X"> 상태(<img src="https://latex.codecogs.com/png.latex?x">와 <img src="https://latex.codecogs.com/png.latex?x%5E*">)를 관측할 수 없습니다.</li>
<li>따라서 일반적인 <img src="https://latex.codecogs.com/png.latex?P(Y_x=y)">나 <img src="https://latex.codecogs.com/png.latex?P(Y_%7Bxz%7D=y)"> 형태의 표현식으로 환원되지 않습니다.</li>
</ul>
</section>
<section id="identification-conditions" class="level2">
<h2 class="anchored" data-anchor-id="identification-conditions">4.2. Identification Conditions</h2>
<ul>
<li>하지만 특정 조건 하에서는 실험적 데이터(experimental data)나 관찰 데이터로부터 NDE를 식별할 수 있습니다.</li>
<li><strong>조건:</strong> <img src="https://latex.codecogs.com/png.latex?X">나 <img src="https://latex.codecogs.com/png.latex?Z">의 자손이 아닌 공변량 집합 <img src="https://latex.codecogs.com/png.latex?W">가 존재하여 다음을 만족해야 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D%20%5Cperp%5C!%5C!%5Cperp%20Z_%7Bx%5E*%7D%20%5Cmid%20W"></p>
<ul>
<li>이 조건은 <img src="https://latex.codecogs.com/png.latex?W">를 통제했을 때, “<img src="https://latex.codecogs.com/png.latex?Z">를 <img src="https://latex.codecogs.com/png.latex?z">로 고정하고 <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x">로 설정했을 때의 잠재적 결과(<img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D">)”와 “<img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">일 때의 자연적 <img src="https://latex.codecogs.com/png.latex?Z">값(<img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D">)”이 독립이라는 의미입니다.</li>
</ul>
</section>
<section id="derivation" class="level2">
<h2 class="anchored" data-anchor-id="derivation">4.3. Derivation</h2>
<ul>
<li><p>위 조건이 만족될 때, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_%7Bx%20Z_%7Bx%5E*%7D%7D%5D">는 다음과 같이 유도됩니다.</p></li>
<li><ol type="1">
<li><strong>전체 확률의 법칙 (Conditioning on <img src="https://latex.codecogs.com/png.latex?W"> and <img src="https://latex.codecogs.com/png.latex?Z">):</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cmathbb%7BE%7D%5BY_%7Bx%20Z_%7Bx%5E*%7D%7D%5D%20=%20%5Csum_w%20%5Csum_z%20%5Cmathbb%7BE%7D%5BY_%7Bxz%7D%20%5Cmid%20Z_%7Bx%5E*%7D%20=%20z,%20W=w%5D%20P(Z_%7Bx%5E*%7D%20=%20z%20%5Cmid%20W=w)%20P(W=w)%0A%20%20"></li>
</ol>
<ul>
<li>여기서 내부의 <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%20Z_%7Bx%5E*%7D%7D">는 <img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D%20=%20z"> 조건 하에서 <img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D">가 됩니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>조건부 독립성 적용 (<img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D%20%5Cperp%20Z_%7Bx%5E*%7D%20%5Cmid%20W">):</strong></li>
</ol>
<ul>
<li>가정에 의해 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_%7Bxz%7D%20%5Cmid%20Z_%7Bx%5E*%7D%20=%20z,%20w%5D">는 <img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D">와 무관하므로 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_%7Bxz%7D%20%5Cmid%20w%5D">로 단순화됩니다. <img src="https://latex.codecogs.com/png.latex?%0A%20%20=%20%5Csum_w%20%5Csum_z%20%5Cmathbb%7BE%7D%5BY_%7Bxz%7D%20%5Cmid%20w%5D%20P(Z_%7Bx%5E*%7D%20=%20z%20%5Cmid%20w)%20P(w)%0A%20%20"></li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>NDE Formula:</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_%7Bx%5E*%7D%5D"> 부분 역시 Composition 법칙에 의해 유사하게 분해하면, 최종적으로 NDE 식별 공식은 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20NDE(x,%20x%5E*;%20Y)%20=%20%5Csum_%7Bw,z%7D%20%5B%5Cmathbb%7BE%7D(Y_%7Bxz%7D%7Cw)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E*z%7D%7Cw)%5D%20P(Z_%7Bx%5E*%7D=z%7Cw)%20P(w)%0A%20%20"></p></li>
<li><p>이 식의 의미는, <strong>“각 상황 <img src="https://latex.codecogs.com/png.latex?w">에서, 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">가 기준 상태(<img src="https://latex.codecogs.com/png.latex?x%5E*">)일 때 가질 확률(<img src="https://latex.codecogs.com/png.latex?P(Z_%7Bx%5E*%7D=z%7Cw)">)을 가중치로 하여 CDE를 평균 낸 것”</strong>입니다.</p></li>
</ul>
</section>
<section id="graphical-interpretation" class="level2">
<h2 class="anchored" data-anchor-id="graphical-interpretation">4.4. Graphical Interpretation</h2>
<ul>
<li>이러한 식별 조건은 인과 그래프(Causal Diagram)를 통해 시각적으로 확인할 수 있습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-02/images/causal_graph_nde.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: (a) NDE가 식별 가능한 인과 그래프, (b) 식별 조건을 확인하기 위한 부분 그래프</figcaption>
</figure>
</div>
<ul>
<li><strong>Figure 1(a):</strong> 전체 인과 모형을 나타냅니다. <img src="https://latex.codecogs.com/png.latex?W">는 교란 요인(confounder) 역할을 합니다.</li>
<li><strong>Figure 1(b):</strong> 조건 <img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D%20%5Cperp%20Z_%7Bx%5E*%7D%20%7C%20W">를 확인하기 위한 부분 그래프입니다. <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Z">에서 나가는 화살표를 제거했을 때(mutilated graph), <img src="https://latex.codecogs.com/png.latex?W">가 <img src="https://latex.codecogs.com/png.latex?Y">와 <img src="https://latex.codecogs.com/png.latex?Z">를 d-separation 시킨다면 식별 조건이 충족됩니다.</li>
</ul>
</section>
</section>
<section id="identification-in-markovian-models-observational-data" class="level1">
<h1>5. Identification in Markovian Models (Observational Data)</h1>
<p>실험적 개입(<img src="https://latex.codecogs.com/png.latex?do">-operator)이 없는 순수 관찰 데이터(Markovian Model)에서도 NDE를 식별할 수 있습니다. Markovian 모델에서는 모든 변수들이 자신의 부모 변수들에 의존하며, 오차항들이 서로 독립입니다[cite: 81].</p>
<section id="assupmtions" class="level2">
<h2 class="anchored" data-anchor-id="assupmtions">5.1. Assupmtions</h2>
<p>Markovian 모델에서는 다음 세 가지 관계가 성립합니다[cite: 81, 82, 83, 84, 85]. 1. <img src="https://latex.codecogs.com/png.latex?P(Y_%7Bxz%7D=y)%20=%20P(y%20%5Cmid%20x,%20z)"> (여기서 <img src="https://latex.codecogs.com/png.latex?X%20%5Ccup%20Z">는 <img src="https://latex.codecogs.com/png.latex?Y">의 부모 집합) 2. <img src="https://latex.codecogs.com/png.latex?P(Z_%7Bx%5E*%7D=z)%20=%20%5Csum_s%20P(z%20%5Cmid%20x%5E*,%20s)P(s)"> 3. <img src="https://latex.codecogs.com/png.latex?P(Y_%7Bx,%20Z_%7Bx%5E*%7D%7D=y)%20=%20%5Csum_s%20%5Csum_z%20P(y%20%5Cmid%20x,%20z)P(z%20%5Cmid%20x%5E*,%20s)P(s)"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?S">는 <img src="https://latex.codecogs.com/png.latex?Z">의 부모 변수 집합(단, <img src="https://latex.codecogs.com/png.latex?X"> 제외) 혹은 Backdoor Criterion을 만족하는 집합을 의미합니다.</p>
</section>
<section id="mediation-formula" class="level2">
<h2 class="anchored" data-anchor-id="mediation-formula">5.2. Mediation Formula</h2>
<p>위 관계들을 종합하면, 비실험 데이터(Non-experimental data)로부터 NDE를 추정하는 <strong>Mediation Formula</strong>를 얻습니다[cite: 87].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANDE(x,%20x%5E*;%20Y)%20=%20%5Csum_s%20%5Csum_z%20%5B%5Cmathbb%7BE%7D(Y%20%5Cmid%20x,%20z)%20-%20%5Cmathbb%7BE%7D(Y%20%5Cmid%20x%5E*,%20z)%5D%20P(z%20%5Cmid%20x%5E*,%20s)%20P(s)%0A"></p>
<p>이 공식은 관찰된 조건부 확률 <img src="https://latex.codecogs.com/png.latex?P(y%7Cx,z)">와 <img src="https://latex.codecogs.com/png.latex?P(z%7Cx,s)">, 그리고 공변량의 분포 <img src="https://latex.codecogs.com/png.latex?P(s)">만을 사용하여 인과적 직접 효과를 계산할 수 있게 해줍니다.</p>
</section>
</section>
<section id="summary" class="level1">
<h1>6. Summary</h1>
<ul>
<li><strong>CDE</strong>는 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 특정 값으로 <strong>강제 고정</strong>했을 때의 효과입니다.</li>
<li><strong>NDE</strong>는 중개 변수 <img src="https://latex.codecogs.com/png.latex?Z">가 <strong>자연스럽게 변하도록 두었을 때</strong>(단, 기준 처치 <img src="https://latex.codecogs.com/png.latex?x%5E*"> 하에서의 분포를 따름)의 직접적인 효과입니다.</li>
<li>NDE는 <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%20Z_%7Bx%5E*%7D%7D">라는 중첩된 반사실을 포함하므로 식별이 까다롭지만, <img src="https://latex.codecogs.com/png.latex?Y_%7Bxz%7D%20%5Cperp%20Z_%7Bx%5E*%7D%20%7C%20W">라는 조건 하에서 식별 가능합니다.</li>
<li>Markovian 모델에서는 <strong>Mediation Formula</strong>를 통해 관찰 데이터만으로 NDE를 계산할 수 있습니다.</li>
</ul>
<hr>
<p><strong>References</strong> Pearl, J. (2001). <em>Direct and Indirect Effects</em>. (Based on the provided appendix PDF)</p>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/LA02/part-02/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] Appendix 02. Direct and Indirect Effects (Part 3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/LA02/part-03/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>인과추론(Causal Inference)의 핵심 목표 중 하나는 단순히 원인(<img src="https://latex.codecogs.com/png.latex?X">)이 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 미치는 <strong>총 효과(Total Effect)</strong>를 추정하는 것을 넘어, 그 효과가 발생하는 <strong>메커니즘(Mechanism)</strong>을 규명하는 것입니다. 이를 위해 우리는 효과를 <strong>직접 효과(Direct Effect)</strong>와 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">를 경유하는 <strong>간접 효과(Indirect Effect)</strong>로 분해하게 됩니다.</p>
<p>[cite_start]이번 포스트에서는 강의 자료의 “Natural Indirect Effects: Formulation” 및 “Identification” 파트(Slide 2-6)를 중심으로, <strong>Natural Indirect Effect(NIE)</strong>의 정의와 <strong>Total Effect(TE)</strong>와의 관계, 그리고 관측 데이터로부터 이를 식별(Identification)하는 과정을 정리합니다[cite: 1, 2, 4].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-03/images/mediation_dag_basic.png" class="img-fluid figure-img"></p>
<figcaption>Figure: 매개 분석(Mediation Analysis)을 위한 인과 그래프(DAG). <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 직접 경로와 <img src="https://latex.codecogs.com/png.latex?Z">를 거쳐가는 간접 경로를 보여준다.</figcaption>
</figure>
</div>
<hr>
</section>
<section id="natural-indirect-effects-nie-formulation" class="level2">
<h2 class="anchored" data-anchor-id="natural-indirect-effects-nie-formulation">2. Natural Indirect Effects (NIE): Formulation</h2>
<section id="definition-using-nested-counterfactuals" class="level3">
<h3 class="anchored" data-anchor-id="definition-using-nested-counterfactuals">2.1. Definition using Nested Counterfactuals</h3>
<p>Pearl의 구조적 인과 모형(Structural Causal Model) 프레임워크에서 <strong>자연 간접 효과(Natural Indirect Effect, NIE)</strong>는 다음과 같이 정의됩니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANIE(x,x%5E%7B*%7D;Y,u)%20=%20Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D(u)%7D(u)%20-%20Y_%7Bx%5E%7B*%7D%7D(u)%0A"></p>
<p>[cite_start]이 수식은 얼핏 복잡해 보이지만, 의미를 뜯어보면 직관적입니다[cite: 7, 8].</p>
<ul>
<li><strong>기준점(<img src="https://latex.codecogs.com/png.latex?x%5E*">):</strong> 처치변수 <img src="https://latex.codecogs.com/png.latex?X">가 기준값 <img src="https://latex.codecogs.com/png.latex?x%5E*">일 때의 결과 <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*%7D">를 생각합니다. (직접 경로와 간접 경로 모두 <img src="https://latex.codecogs.com/png.latex?X=x%5E*"> 상태)</li>
<li><strong>가상의 개입:</strong> 이제 <strong>직접 경로</strong>인 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">는 <img src="https://latex.codecogs.com/png.latex?x%5E*">로 고정하되, <strong>간접 경로</strong>를 결정하는 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">만 처치군 값 <img src="https://latex.codecogs.com/png.latex?x">를 받았을 때의 값(<img src="https://latex.codecogs.com/png.latex?Z_x">)으로 바꿉니다.</li>
<li><strong>Nested Counterfactual:</strong> <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D(u)%7D">는 “내가 약을 먹지 않았지만(<img src="https://latex.codecogs.com/png.latex?X=x%5E*">), 내 몸의 매개 변수들(혈압 등 <img src="https://latex.codecogs.com/png.latex?Z">)은 약을 먹었을 때와 똑같이 변했다면(<img src="https://latex.codecogs.com/png.latex?Z_x">) 결과(<img src="https://latex.codecogs.com/png.latex?Y">)는 어땠을까?”라는 질문에 답합니다.</li>
</ul>
<p>따라서 이 둘의 차이는 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Z">를 변화시킴으로써 발생하는 순수한 간접 효과를 나타냅니다.</p>
</section>
<section id="average-natural-indirect-effect" class="level3">
<h3 class="anchored" data-anchor-id="average-natural-indirect-effect">2.2. Average Natural Indirect Effect</h3>
<p>[cite_start]개별 단위(unit <img src="https://latex.codecogs.com/png.latex?u">)에 대한 효과를 모집단 전체로 확장하면 <strong>평균 자연 간접 효과</strong>는 다음과 같이 기댓값의 차이로 정의됩니다[cite: 9, 10].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANIE(x,x%5E%7B*%7D;Y)%20=%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D%7D)%0A"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D%7D)">는 <img src="https://latex.codecogs.com/png.latex?X=x%5E*">일 때의 평균 잠재적 결과(Potential Outcome)이며, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D%7D)">는 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">로 고정된 상태에서 <img src="https://latex.codecogs.com/png.latex?Z">만 <img src="https://latex.codecogs.com/png.latex?X=x">일 때의 분포를 따를 때의 평균 결과입니다.</p>
<hr>
</section>
</section>
<section id="decomposition-of-total-effect" class="level2">
<h2 class="anchored" data-anchor-id="decomposition-of-total-effect">3. Decomposition of Total Effect</h2>
<p>인과 효과 분해(Decomposition)의 가장 큰 매력은 총 효과(TE)를 자연 직접 효과(NDE)와 자연 간접 효과(NIE)의 합으로 표현할 수 있다는 점입니다. [cite_start]강의 자료는 이 관계를 대수적으로 유도합니다[cite: 13].</p>
<section id="definitions-of-effects" class="level3">
<h3 class="anchored" data-anchor-id="definitions-of-effects">3.1. Definitions of Effects</h3>
<p>[cite_start]세 가지 주요 효과를 수식으로 정의하면 다음과 같습니다[cite: 14].</p>
<ol type="1">
<li><strong>Total Effect (TE):</strong> <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x%5E*">에서 <img src="https://latex.codecogs.com/png.latex?x">로 변할 때의 전체 효과 <img src="https://latex.codecogs.com/png.latex?TE(x,x%5E%7B*%7D;Y)%20=%20%5Cmathbb%7BE%7D(Y_%7Bx%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D%7D)"></li>
<li><strong>Natural Direct Effect (NDE):</strong> 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?x%5E*"> 수준(<img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D">)으로 고정된 상태에서, <img src="https://latex.codecogs.com/png.latex?X">만 <img src="https://latex.codecogs.com/png.latex?x">로 변할 때의 효과 <img src="https://latex.codecogs.com/png.latex?NDE(x,x%5E%7B*%7D;Y)%20=%20%5Cmathbb%7BE%7D(Y_%7Bx,Z_%7Bx%5E%7B*%7D%7D%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D%7D)"></li>
<li><strong>Natural Indirect Effect (NIE):</strong> <img src="https://latex.codecogs.com/png.latex?X">는 <img src="https://latex.codecogs.com/png.latex?x%5E*">로 고정하고, <img src="https://latex.codecogs.com/png.latex?Z">만 <img src="https://latex.codecogs.com/png.latex?x"> 수준(<img src="https://latex.codecogs.com/png.latex?Z_x">)으로 변할 때의 효과 <img src="https://latex.codecogs.com/png.latex?NIE(x,x%5E%7B*%7D;Y)%20=%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E%7B*%7D%7D)"></li>
</ol>
</section>
<section id="algebraic-derivation" class="level3">
<h3 class="anchored" data-anchor-id="algebraic-derivation">3.2. Algebraic Derivation</h3>
<p>강의 자료는 흥미로운 대수적 관계를 통해 TE, NDE, NIE를 연결합니다. [cite_start]먼저, “역방향” NIE를 고려해 봅시다[cite: 19].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A-NIE(x%5E%7B*%7D,x;Y)%20=%20-%5B%5Cmathbb%7BE%7D(Y_%7Bx,Z_%7Bx%5E%7B*%7D%7D%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%7D)%5D%20=%20%5Cmathbb%7BE%7D(Y_%7Bx%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx,Z_%7Bx%5E%7B*%7D%7D%7D)%0A"></p>
<p>[cite_start]이제 TE를 NDE와 위 식을 사용하여 다시 써보면 다음과 같습니다[cite: 20].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0ATE(x,%20x%5E*;%20Y)%20&amp;=%20%5Cmathbb%7BE%7D(Y_x)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E*%7D)%20%5C%5C%0A&amp;=%20%5Cunderbrace%7B%5B%5Cmathbb%7BE%7D(Y_%7Bx,%20Z_%7Bx%5E*%7D%7D)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx%5E*%7D)%5D%7D_%7BNDE(x,%20x%5E*)%7D%20+%20%5Cunderbrace%7B%5B%5Cmathbb%7BE%7D(Y_x)%20-%20%5Cmathbb%7BE%7D(Y_%7Bx,%20Z_%7Bx%5E*%7D%7D)%5D%7D_%7B-NIE(x%5E*,%20x)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>[cite_start]따라서, 다음과 같은 관계가 성립합니다[cite: 26].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ATE(x,x%5E%7B*%7D;Y)%20=%20NDE(x,x%5E%7B*%7D;Y)%20-%20NIE(x%5E%7B*%7D,x;Y)%0A"></p>
<p>[cite_start]이 관계는 다소 생소해 보일 수 있지만(<img src="https://latex.codecogs.com/png.latex?-NIE(x%5E*,%20x)"> 항 때문), 결과적으로 우리가 흔히 사용하는 <strong>Standard Additive Relation</strong>을 도출해냅니다[cite: 27, 28].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ATE(x,x%5E%7B*%7D;Y)%20=%20NDE(x,x%5E%7B*%7D;Y)%20+%20NIE(x,x%5E%7B*%7D;Y)%0A"></p>
<blockquote class="blockquote">
<p><strong>Note:</strong> <img src="https://latex.codecogs.com/png.latex?NIE(x,%20x%5E*)%20=%20-NIE(x%5E*,%20x)">라는 가정(즉, 효과의 대칭성)은 선형 모델 등 특정 조건에서 성립하거나, 혹은 효과의 정의를 “Total Effect = Direct + Indirect”가 되도록 맞추는 과정에서 자연스럽게 해석될 수 있습니다. 핵심은 <strong>총 효과가 두 경로의 합으로 쪼개진다</strong>는 직관입니다.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-03/images/te_decomposition_diagram.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Total Effect의 분해 과정 시각화. 전체 효과(<img src="https://latex.codecogs.com/png.latex?Y_x%20-%20Y_%7Bx%5E*%7D">)는 직접 경로를 통한 효과(<img src="https://latex.codecogs.com/png.latex?Y_%7Bx,Z_%7Bx%5E*%7D%7D%20-%20Y_%7Bx%5E*%7D">)와 간접 경로를 통한 효과(<img src="https://latex.codecogs.com/png.latex?Y_%7Bx,Z_x%7D%20-%20Y_%7Bx,Z_%7Bx%5E*%7D%7D">)의 합으로 표현됨을 보여준다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="identification-strategy" class="level2">
<h2 class="anchored" data-anchor-id="identification-strategy">4. Identification Strategy</h2>
<p>이제 이론적으로 정의된 <img src="https://latex.codecogs.com/png.latex?NIE">를 실제 데이터로 어떻게 추정(Identification)할 것인지가 문제입니다. <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E%7B*%7D,Z_%7Bx%7D%7D">와 같은 반사실적(Counterfactual) 항은 현실에서 관측할 수 없기 때문입니다.</p>
<section id="identification-conditions" class="level3">
<h3 class="anchored" data-anchor-id="identification-conditions">4.1. Identification Conditions</h3>
<p>[cite_start]강의 자료에서는 식별을 위해 다음과 같은 조건부 독립성을 가정합니다[cite: 31, 32]. <img src="https://latex.codecogs.com/png.latex?W">가 <img src="https://latex.codecogs.com/png.latex?X">나 <img src="https://latex.codecogs.com/png.latex?Z">의 자손(descendant)이 아닌 공변량 집합일 때, 다음이 성립해야 합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bx%5E%7B*%7Dz%7D%20%5Cperp%20%5C!%5C!%5C!%20%5Cperp%20Z_%7Bx%7D%20%5Cmid%20W%0A"></p>
<p>이 가정은 <strong>“교란 요인 <img src="https://latex.codecogs.com/png.latex?W">를 통제했을 때, 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">를 결정하는 메커니즘과, <img src="https://latex.codecogs.com/png.latex?Z">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?Y">를 결정하는 메커니즘이 서로 독립적이어야 한다”</strong>는 것을 의미합니다. 즉, <img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20Y"> 관계에 우리가 모르는 교란 요인(Unobserved Confounder)이 없어야 한다는 강력한 가정입니다.</p>
</section>
<section id="the-mediation-formula" class="level3">
<h3 class="anchored" data-anchor-id="the-mediation-formula">4.2. The Mediation Formula</h3>
<p>[cite_start]위 가정이 성립하면, NIE는 관측 가능한 확률분포들의 조합으로 식별 가능합니다[cite: 34].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANIE(x,x%5E%7B*%7D;Y)%20=%20%5Csum_%7Bw%7D%5Csum_%7Bz%7D%20%5Cmathbb%7BE%7D(Y%20%5Cmid%20x%5E%7B*%7D,z,%20w)%20%5BP(z%20%5Cmid%20x,%20w)%20-%20P(z%20%5Cmid%20x%5E%7B*%7D,%20w)%5D%20P(w)%0A"></p>
<p>[cite_start]이 식은 다음과 같은 요소들로 구성됩니다[cite: 35]: 1. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D(Y%20%5Cmid%20x%5E%7B*%7D,z,%20w)">: <img src="https://latex.codecogs.com/png.latex?X">는 기준값 <img src="https://latex.codecogs.com/png.latex?x%5E*">로, <img src="https://latex.codecogs.com/png.latex?Z">는 특정 값 <img src="https://latex.codecogs.com/png.latex?z">로 주어졌을 때의 <img src="https://latex.codecogs.com/png.latex?Y">의 기댓값 (Outcome Model) 2. <img src="https://latex.codecogs.com/png.latex?P(z%20%5Cmid%20x,%20w)%20-%20P(z%20%5Cmid%20x%5E%7B*%7D,%20w)">: 처치(<img src="https://latex.codecogs.com/png.latex?x">)와 대조(<img src="https://latex.codecogs.com/png.latex?x%5E*">) 조건 하에서 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">가 값 <img src="https://latex.codecogs.com/png.latex?z">를 가질 확률의 차이 (Mediator Model) 3. <img src="https://latex.codecogs.com/png.latex?P(w)">: 공변량 <img src="https://latex.codecogs.com/png.latex?W">에 대한 가중 합</p>
</section>
<section id="simplification-in-markovian-models" class="level3">
<h3 class="anchored" data-anchor-id="simplification-in-markovian-models">4.3. Simplification in Markovian Models</h3>
<p>[cite_start]만약 모델이 <strong>Markovian</strong>이라면 (즉, 숨겨진 교란 요인이 없고 변수 간 관계가 DAG로 완벽히 표현된다면), 식은 훨씬 간단해집니다[cite: 36, 37].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ANIE(x,x%5E%7B*%7D;Y)%20=%20%5Csum_%7Bz%7D%20%5Cmathbb%7BE%7D(Y%20%5Cmid%20x%5E%7B*%7D,%20z)%20%5BP(z%20%5Cmid%20x)%20-%20P(z%20%5Cmid%20x%5E%7B*%7D)%5D%0A"></p>
<p>이 <strong>Mediation Formula</strong>는 매개 분석에서 가장 널리 쓰이는 공식 중 하나로, 복잡한 반사실적 개념 없이도 회귀분석 등을 통해 <img src="https://latex.codecogs.com/png.latex?E(Y%7CX,Z)">와 <img src="https://latex.codecogs.com/png.latex?P(Z%7CX)">만 추정하면 간접 효과를 계산할 수 있게 해줍니다.</p>
<hr>
</section>
</section>
<section id="summary-checklist" class="level2">
<h2 class="anchored" data-anchor-id="summary-checklist">5. Summary &amp; Checklist</h2>
<p>이번 포스트에서는 강의 자료의 내용을 바탕으로 <strong>Natural Indirect Effect</strong>의 정의부터 식별 전략까지를 살펴보았습니다.</p>
<ul>
<li><strong>Definition:</strong> NIE는 직접 경로를 차단(<img src="https://latex.codecogs.com/png.latex?X=x%5E*">)하고 간접 경로만 활성화(<img src="https://latex.codecogs.com/png.latex?Z_%7Bx%7D">)했을 때의 효과입니다.</li>
<li><strong>Decomposition:</strong> <img src="https://latex.codecogs.com/png.latex?TE%20=%20NDE%20+%20NIE">의 관계를 가지며, 이는 효과가 경로별로 합산됨을 의미합니다.</li>
<li><strong>Identification:</strong> <img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*z%7D%20%5Cperp%20Z_x%20%7C%20W"> 가정 하에, Mediation Formula를 통해 관측 데이터로 NIE를 추정할 수 있습니다.</li>
</ul>
<section id="content-checklist" class="level3">
<h3 class="anchored" data-anchor-id="content-checklist">Content Checklist</h3>
<p>아래는 강의 자료의 핵심 내용이 본 포스트에 포함되었는지 확인하는 체크리스트입니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 27%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Key Concept</th>
<th style="text-align: center;">Covered</th>
<th style="text-align: center;">Source Slide</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Definitions</strong></td>
<td style="text-align: left;">NIE Formulation (<img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*,%20Z_x%7D%20-%20Y_%7Bx%5E*%7D">)</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 2</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Average NIE Definition</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 2</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Relations</strong></td>
<td style="text-align: left;">TE, NDE, NIE Definitions</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 3</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Decomposition Algebra (<img src="https://latex.codecogs.com/png.latex?TE%20=%20NDE%20-%20NIE_%7Brev%7D">)</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 4-5</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">Additive Relation (<img src="https://latex.codecogs.com/png.latex?TE%20=%20NDE%20+%20NIE">)</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 5</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Identification</strong></td>
<td style="text-align: left;">Independence Assumption (<img src="https://latex.codecogs.com/png.latex?Y_%7Bx%5E*z%7D%20%5Cperp%20Z_x%20%7C%20W">)</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 6</td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;">General Identification Formula (Sum over <img src="https://latex.codecogs.com/png.latex?w,%20z">)</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 6</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">Markovian Simplification</td>
<td style="text-align: center;">✅</td>
<td style="text-align: center;">Slide 6</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>References:</strong> 본 포스트는 제공된 강의 자료 [Appendix. Direct and Indirect Effects.pdf]를 바탕으로 작성되었습니다.</p>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/LA02/part-03/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] Appendix 02. Direct and Indirect Effects (Part 4)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/LA02/part-04/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>인과추론(Causal Inference)에서 가장 흥미롭고도 도전적인 과제 중 하나는 ’효과를 분해(Decomposition)’하는 것입니다. [cite_start]우리가 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 전체 효과(Total Effect)를 알았다면, 그 다음 질문은 자연스럽게 “그 효과가 직접적인 경로로 발생했는가, 아니면 매개변수 <img src="https://latex.codecogs.com/png.latex?Z">를 거쳐서 발생했는가?”로 이어집니다[cite: 1, 2].</p>
<p>전통적인 방식(Baron &amp; Kenny 등)은 선형 모델에 의존하지만, 현실의 데이터 생성 과정은 비선형적이고 상호작용(Interaction)이 가득합니다. Judea Pearl은 이러한 한계를 극복하기 위해 <strong>경로 특정 효과(Path-Specific Effects, PSE)</strong>라는 일반화된 프레임워크를 제안했습니다. [cite_start]이 글에서는 PDF 자료를 바탕으로 비선형 모델에서도 적용 가능한 직접·간접 효과의 정의와 이를 뒷받침하는 수학적 구조, 그리고 인과적 공리(Axioms)를 깊이 있게 다룹니다[cite: 4, 5].</p>
<hr>
</section>
<section id="path-specific-effects-pse" class="level1">
<h1>2. Path-Specific Effects (PSE)</h1>
<section id="the-motivation-beyond-simple-mediation" class="level2">
<h2 class="anchored" data-anchor-id="the-motivation-beyond-simple-mediation">2.1. The Motivation: Beyond Simple Mediation</h2>
<p>단순한 매개 모형(Simple Mediation Model)에서는 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y"> (직접 경로)와 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z%20%5Cto%20Y"> (간접 경로)만이 존재합니다. 하지만 실제 인과 구조는 훨씬 복잡할 수 있습니다. [cite_start]변수 간의 화살표가 얽혀 있는 상황에서 우리가 관심 있는 <strong>특정 경로들의 집합(subset of paths)</strong>만을 분리해내어 그 효과를 추정하고 싶을 때, <strong>Path-Specific Effect</strong> 개념이 필요합니다[cite: 5, 43].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-04/images/mediation_complexity.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: 기본적인 매개 모형(좌)과 다중 경로가 존재하는 복잡한 모형(우). 왼쪽 그림에서는 효과가 직접 효과와 <img src="https://latex.codecogs.com/png.latex?Z">를 경유하는 간접 효과로 명확히 나뉘지만, 오른쪽 그림과 같이 <img src="https://latex.codecogs.com/png.latex?X">에서 <img src="https://latex.codecogs.com/png.latex?Y">로 가는 경로가 <img src="https://latex.codecogs.com/png.latex?Z">, <img src="https://latex.codecogs.com/png.latex?W"> 등 여러 변수를 거치며 복잡하게 얽힌 경우, 특정 경로(굵은 선)만을 분리하여 정의할 필요가 있다.</figcaption>
</figure>
</div>
</section>
<section id="definition-and-formalization" class="level2">
<h2 class="anchored" data-anchor-id="definition-and-formalization">2.2. Definition and Formalization</h2>
<p>[cite_start]Path-Specific Effect를 정의하기 위해 Pearl은 <strong>변수 고정(Variable fixing)</strong>이 아닌 <strong>경로 스위칭(Path switching)</strong>이라는 개념을 도입했습니다[cite: 62, 63].</p>
<section id="step-1-edge-subgraph의-정의" class="level3">
<h3 class="anchored" data-anchor-id="step-1-edge-subgraph의-정의">Step 1: Edge-Subgraph의 정의</h3>
<p>우선 전체 인과 그래프(Causal Graph)를 <img src="https://latex.codecogs.com/png.latex?G">라고 합시다. [cite_start]우리가 효과를 분석하고 싶은 경로들로만 이루어진 하위 그래프(Edge-subgraph)를 <img src="https://latex.codecogs.com/png.latex?g">라고 정의합니다[cite: 44]. [cite_start]이때, 모델의 각 변수 <img src="https://latex.codecogs.com/png.latex?X_i">에 대해 부모 변수 집합 <img src="https://latex.codecogs.com/png.latex?PA_i">를 두 부분으로 나눌 수 있습니다[cite: 46, 47]:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APA_i%20=%20%5C%7B%20PA_i(g),%20PA_i(%5Cbar%7Bg%7D)%20%5C%7D%0A"></p>
<ul>
<li>[cite_start]<img src="https://latex.codecogs.com/png.latex?PA_i(g)">: 그래프 <img src="https://latex.codecogs.com/png.latex?g">에 포함된 엣지를 통해 <img src="https://latex.codecogs.com/png.latex?X_i">에 연결된 부모 변수들 (활성화된 경로) [cite: 48]</li>
<li>[cite_start]<img src="https://latex.codecogs.com/png.latex?PA_i(%5Cbar%7Bg%7D)">: 그래프 <img src="https://latex.codecogs.com/png.latex?g">에 포함되지 않은 엣지를 통해 연결된 부모 변수들 (비활성화된 경로) [cite: 48]</li>
</ul>
</section>
<section id="step-2-modified-structural-equations" class="level3">
<h3 class="anchored" data-anchor-id="step-2-modified-structural-equations">Step 2: Modified Structural Equations</h3>
<p>이제 <strong>경로 특정 효과</strong>를 계산하기 위해 구조방정식(Structural Model)을 수정합니다. [cite_start]원래 모델 <img src="https://latex.codecogs.com/png.latex?M">에서의 구조방정식이 <img src="https://latex.codecogs.com/png.latex?x_i%20=%20f_i(pa_i,%20u_i)">라면, 수정된 모델 <img src="https://latex.codecogs.com/png.latex?M_%7Bg%7D%5E*">에서의 새로운 함수 <img src="https://latex.codecogs.com/png.latex?f_i%5E*">는 다음과 같이 정의됩니다[cite: 49, 50].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Af_i%5E*(pa_i,%20u;%20g)%20=%20f_i(pa_i(g),%20pa_i%5E*(%5Cbar%7Bg%7D),%20u)%0A"></p>
<p>이 수식의 의미는 매우 직관적입니다: 1. <strong>관심 있는 경로(<img src="https://latex.codecogs.com/png.latex?g">)</strong>를 통해 들어오는 입력(<img src="https://latex.codecogs.com/png.latex?pa_i(g)">)은 <strong>처치 <img src="https://latex.codecogs.com/png.latex?X=x"></strong>의 영향을 받아 변하게 둡니다. 2. [cite_start]<strong>관심 없는 경로(<img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bg%7D">)</strong>를 통해 들어오는 입력(<img src="https://latex.codecogs.com/png.latex?pa_i%5E*(%5Cbar%7Bg%7D)">)은 <strong>기준값 <img src="https://latex.codecogs.com/png.latex?X=x%5E*"></strong>일 때의 값으로 고정합니다[cite: 51].</p>
<p>[cite_start]즉, <img src="https://latex.codecogs.com/png.latex?pa_i%5E*(%5Cbar%7Bg%7D)%20=%20PA_i(%5Cbar%7Bg%7D)_%7Bx%5E*%7D"> 입니다[cite: 51].</p>
</section>
<section id="step-3-the-g-specific-effect-definition" class="level3">
<h3 class="anchored" data-anchor-id="step-3-the-g-specific-effect-definition">Step 3: The g-Specific Effect Definition</h3>
<p>[cite_start]최종적으로, 기준값 <img src="https://latex.codecogs.com/png.latex?x%5E*"> 대비 처치 <img src="https://latex.codecogs.com/png.latex?x">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 <strong><img src="https://latex.codecogs.com/png.latex?g">-specific effect</strong>는 수정된 모델 <img src="https://latex.codecogs.com/png.latex?H_%7Bg%7D%5E*">에서의 전체 효과(Total Effect)로 정의됩니다[cite: 52, 53].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ASE_%7Bg%7D(x,%20x%5E*;%20Y,%20u)_%7BM%7D%20=%20TE(x,%20x%5E*;%20Y,%20u)_%7BM_%7Bg%7D%5E*%7D%0A"></p>
<p>[cite_start]이 정의는 비선형 모델이나 상호작용 항이 있는 경우에도 직접 효과와 간접 효과를 일관성 있게 정의할 수 있게 해줍니다[cite: 63].</p>
<hr>
</section>
</section>
</section>
<section id="nested-counterfactuals-a-visual-interpretation" class="level1">
<h1>3. Nested Counterfactuals: A Visual Interpretation</h1>
<p>이 개념을 명확히 이해하기 위해, PDF 자료에 제시된 일명 “Kite Graph” (또는 Diamond Graph) 예시를 살펴봅시다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-04/images/kite_graph_structure.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Path-Specific Effect를 설명하기 위한 인과 그래프. <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?W">, <img src="https://latex.codecogs.com/png.latex?Z">에 영향을 주고, <img src="https://latex.codecogs.com/png.latex?W">와 <img src="https://latex.codecogs.com/png.latex?Z">가 서로 상호작용하며 최종적으로 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 주는 구조이다. 굵은 화살표는 우리가 효과를 측정하고자 하는 관심 경로(edge-subgraph <img src="https://latex.codecogs.com/png.latex?g">)를 나타낸다.</figcaption>
</figure>
</div>
<p>위 그림과 같은 구조에서 특정 경로의 효과를 계산하려면, 우리는 가상의 세계(Counterfactual world)를 상상해야 합니다. [cite_start]아래 그림은 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?x">(녹색)일 때와 <img src="https://latex.codecogs.com/png.latex?x%5E*">(적색)일 때의 신호가 어떻게 섞여서 <img src="https://latex.codecogs.com/png.latex?Y">에 도달하는지를 보여주는 <strong>Counterfactual Graph</strong>입니다[cite: 55].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/LA02/part-04/images/nested_counterfactuals_color.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: 중첩된 반사실적 상황(Nested Counterfactuals)을 표현한 그래프. 녹색 화살표는 처치 값 <img src="https://latex.codecogs.com/png.latex?x">가 전파되는 경로를, 적색 화살표는 기준 값 <img src="https://latex.codecogs.com/png.latex?x%5E*">가 전파되는 경로를 의미한다. <img src="https://latex.codecogs.com/png.latex?W"> 노드는 <img src="https://latex.codecogs.com/png.latex?x%5E*">의 직접적 영향과 <img src="https://latex.codecogs.com/png.latex?Z_x">(녹색 경로로 온 <img src="https://latex.codecogs.com/png.latex?Z">)의 영향을 동시에 받으므로 <img src="https://latex.codecogs.com/png.latex?W_%7Bx%5E*,%20Z_x%7D">로 표기된다. 최종 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 이러한 혼합된 신호들의 복합적인 결과물이다.</figcaption>
</figure>
</div>
<p>이 그림에서 가장 주목해야 할 점은 <strong>Recanting Witness</strong> 문제와 관련된 복잡한 항입니다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7BW_%7Bx%5E*,%20Z_x%7D,%20Z_%7Bx%5E*%7D%7D%0A"></p>
<p>[cite_start]이 수식을 분해해보면 다음과 같습니다[cite: 59]: 1. <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?x">의 영향을 받습니다 (<img src="https://latex.codecogs.com/png.latex?Z_x">, 녹색 경로). 2. <img src="https://latex.codecogs.com/png.latex?W">는 <img src="https://latex.codecogs.com/png.latex?X">로부터는 <img src="https://latex.codecogs.com/png.latex?x%5E*">의 영향을 받지만(적색 경로), <img src="https://latex.codecogs.com/png.latex?Z">로부터는 <img src="https://latex.codecogs.com/png.latex?x">의 영향을 받은 값(<img src="https://latex.codecogs.com/png.latex?Z_x">)을 입력받습니다. 즉, <img src="https://latex.codecogs.com/png.latex?W">는 <img src="https://latex.codecogs.com/png.latex?W_%7Bx%5E*,%20Z_x%7D">가 됩니다. 3. 최종적으로 <img src="https://latex.codecogs.com/png.latex?Y">는 <img src="https://latex.codecogs.com/png.latex?W">로부터는 혼합된 신호를 받고, <img src="https://latex.codecogs.com/png.latex?Z">로부터는 순수한 <img src="https://latex.codecogs.com/png.latex?x%5E*">의 영향(<img src="https://latex.codecogs.com/png.latex?Z_%7Bx%5E*%7D">)을 받습니다.</p>
<p>이러한 <strong>중첩된 반사실(Nested Counterfactuals)</strong>의 구조를 이해하는 것은 매개 분석에서 교란 요인을 통제하고 순수한 메커니즘을 식별하는 데 필수적입니다. [cite_start]Pearl은 Markovian 모델에서는 이러한 조건들이 항상 만족되어 추정이 가능함을 보였습니다[cite: 64, 65].</p>
<hr>
</section>
<section id="axioms-of-causal-inference" class="level1">
<h1>4. Axioms of Causal Inference</h1>
<p>이러한 복잡한 반사실적 조작이 논리적으로 타당하려면, 몇 가지 기본적인 공리(Axioms)가 성립해야 합니다. [cite_start]PDF의 Appendix 부분에서는 다음 세 가지 핵심 성질을 제시합니다[cite: 68, 73, 76].</p>
<section id="composition-합성" class="level2">
<h2 class="anchored" data-anchor-id="composition-합성">4.1. Composition (합성)</h2>
<p>어떤 변수 <img src="https://latex.codecogs.com/png.latex?W">를 강제로 <img src="https://latex.codecogs.com/png.latex?w">라는 값으로 고정했다고 가정합시다. [cite_start]만약 우리가 <img src="https://latex.codecogs.com/png.latex?X">에 개입했을 때 <img src="https://latex.codecogs.com/png.latex?W">가 자연스럽게 갖게 될 값이 이미 <img src="https://latex.codecogs.com/png.latex?w">라면, <img src="https://latex.codecogs.com/png.latex?W">를 <img src="https://latex.codecogs.com/png.latex?w">로 강제하는 개입은 <img src="https://latex.codecogs.com/png.latex?Y">에 아무런 추가적인 영향을 주지 않는다는 원칙입니다[cite: 69, 70, 71].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AW_x(u)%20=%20w%20%5Cimplies%20Y_%7Bxw%7D(u)%20=%20Y_x(u)%0A"></p>
<p>즉, “이미 발생할 일이었다면, 강제로 시켜도 결과는 같다”는 뜻입니다.</p>
</section>
<section id="consistency-일치성" class="level2">
<h2 class="anchored" data-anchor-id="consistency-일치성">4.2. Consistency (일치성)</h2>
<p>가장 기본적이면서도 중요한 가정입니다. [cite_start]만약 관측된 데이터에서 <img src="https://latex.codecogs.com/png.latex?X=x">라면, 그때 관측된 <img src="https://latex.codecogs.com/png.latex?Y">는 <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?x">로 강제했을 때의 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y_x">와 같아야 합니다[cite: 74].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX(u)%20=%20x%20%5Cimplies%20Y(u)%20=%20Y_x(u)%0A"></p>
</section>
<section id="effectiveness-and-reversibility" class="level2">
<h2 class="anchored" data-anchor-id="effectiveness-and-reversibility">4.3. Effectiveness and Reversibility</h2>
<ul>
<li>[cite_start]<strong>Effectiveness (유효성):</strong> 변수 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?W">를 각각 <img src="https://latex.codecogs.com/png.latex?x,%20w">로 고정하면, <img src="https://latex.codecogs.com/png.latex?X">의 값은 확실히 <img src="https://latex.codecogs.com/png.latex?x">가 됩니다[cite: 77]. <img src="https://latex.codecogs.com/png.latex?X_%7Bxw%7D(u)%20=%20x"></li>
<li>[cite_start]<strong>Reversibility (가역성):</strong> 만약 <img src="https://latex.codecogs.com/png.latex?X">를 고정했을 때 <img src="https://latex.codecogs.com/png.latex?Y,%20W">가 각각 <img src="https://latex.codecogs.com/png.latex?y,%20w">가 되고, 이 상태들이 서로 모순되지 않는다면 원래의 효과 <img src="https://latex.codecogs.com/png.latex?Y_x">는 <img src="https://latex.codecogs.com/png.latex?y">로 유지됩니다[cite: 78]. <img src="https://latex.codecogs.com/png.latex?(Y_%7Bxw%7D(u)%20=%20y)%20%5Cland%20(W_%7Bxy%7D(u)%20=%20w)%20%5Cimplies%20Y_x(u)%20=%20y"></li>
</ul>
<hr>
</section>
</section>
<section id="conclusion" class="level1">
<h1>5. Conclusion</h1>
<p>Pearl의 <strong>Path-Specific Effects</strong> 프레임워크는 단순히 변수를 통제하는 것을 넘어, 인과 경로를 정밀하게 수술(surgery)하듯 분리해내는 강력한 도구입니다.</p>
<ol type="1">
<li>[cite_start]<strong>비선형성 해결:</strong> 선형 가정 없이도 직접/간접 효과를 정의할 수 있습니다[cite: 63].</li>
<li>[cite_start]<strong>조작적 해석:</strong> 간접 효과에 대한 명확한 조작적(operational) 해석을 제공합니다[cite: 66].</li>
<li>[cite_start]<strong>일반화:</strong> Markovian 모델 등 특정 조건 하에서, 실험 데이터나 관측 데이터로부터 이러한 효과를 일관되게 추정할 수 있습니다[cite: 64, 65].</li>
</ol>
<p>이러한 이론적 토대는 복잡한 사회 현상이나 생물학적 메커니즘을 규명하는 데 있어 단순 회귀분석이 줄 수 없는 깊은 통찰을 제공합니다.</p>
<hr>
<section id="verification-checklist" class="level3">
<h3 class="anchored" data-anchor-id="verification-checklist">Verification Checklist</h3>
<ul>
<li><strong>내용 포함 여부:</strong>
<ul class="task-list">
<li><label><input type="checkbox" checked="">Path-Specific Effects의 정의 및 <img src="https://latex.codecogs.com/png.latex?SE_g"> 수식 (PDF p.5)</label></li>
<li><label><input type="checkbox" checked="">Edge-subgraph 및 부모 변수 파티션 <img src="https://latex.codecogs.com/png.latex?PA_i(g)"> (PDF p.5)</label></li>
<li><label><input type="checkbox" checked="">수정된 구조방정식 <img src="https://latex.codecogs.com/png.latex?f_i%5E*"> (PDF p.5)</label></li>
<li><label><input type="checkbox" checked="">Nested Counterfactuals 및 Kite Graph 설명 (PDF p.4, 6)</label></li>
<li><label><input type="checkbox" checked="">Axioms (Composition, Consistency, Effectiveness, Reversibility) (PDF p.8-10)</label></li>
</ul></li>
<li><strong>생략된 내용:</strong>
<ul>
<li>없음. PDF의 Overview부터 Appendix의 Axiom까지 모든 핵심 슬라이드 내용을 서술적으로 재구성함.</li>
</ul></li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/LA02/part-04/</guid>
  <pubDate>Tue, 03 Feb 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Paper Review] DAG-GNN: DAG Structure Learning with Graph Neural Networks</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/</link>
  <description><![CDATA[ 





<section id="introduction-the-challenge-of-structure-learning" class="level1">
<h1>1. Introduction: The Challenge of Structure Learning</h1>
<ul>
<li><p>인과추론(Causal Inference)과 머신러닝의 교차점에서, 데이터의 생성 과정을 설명하는 <strong>방향성 비순환 그래프(Directed Acyclic Graph, DAG)</strong>를 학습하는 것은 매우 중요한 문제입니다. 이를 <strong>Structure Learning</strong>이라고 부릅니다.</p></li>
<li><p>Bayesian Network(BN)의 구조인 DAG는 변수 간의 조건부 독립성을 표현하며, Pearl(1988) 이후 의학, 유전학, 경제학 등 다양한 분야에서 인과관계를 파악하는 도구로 사용되어 왔습니다.</p></li>
<li><p>하지만 데이터의 결합 분포(Joint Distribution)로부터 “Faithful”한 DAG를 찾아내는 것은 악명 높은 난제입니다.</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>Faithfulness란?</strong></p>
<p>그래프 <img src="https://latex.codecogs.com/png.latex?G">와 결합 분포 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">가 서로 “Faithful”하다는 것은, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">에서 성립하는 모든 조건부 독립성이 그래프 <img src="https://latex.codecogs.com/png.latex?G">에서도 (d-separation을 통해) 나타나고, 그 역도 성립함을 의미합니다.</p>
</blockquote>
<section id="the-combinatorial-problem-기존의-한계" class="level2">
<h2 class="anchored" data-anchor-id="the-combinatorial-problem-기존의-한계">The Combinatorial Problem (기존의 한계)</h2>
<ul>
<li><p>DAG 구조 학습이 어려운 근본적인 이유는 <strong>탐색 공간(Search Space)의 방대함</strong> 때문입니다.</p></li>
<li><p>노드(변수)의 개수가 늘어날 때, 가능한 그래프의 수는 초지수적(Superexponential)으로 증가합니다.</p></li>
<li><p>이는 NP-hard 문제로 알려져 있습니다.</p></li>
<li><p>전통적인 접근 방식은 크게 두 가지로 나뉩니다:</p></li>
<li><ol type="1">
<li><strong>Score-based Methods:</strong></li>
</ol>
<ul>
<li>가능한 그래프 구조에 대해 점수(BIC, BDeu 등)를 매기고, 이 점수를 최적화하는 그래프를 찾습니다.</li>
<li>하지만 그래프는 반드시 Acyclic(비순환)이어야 한다는 조합적 제약 조건(Combinatorial Constraint) 때문에, 전역 최적해를 찾는 것이 매우 어렵습니다.</li>
<li>따라서 탐욕적 탐색(Greedy Search)이나 트리 구조(Tree-structure) 가정 같은 근사법을 사용해야 했습니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Constraint-based Methods:</strong></li>
</ol>
<ul>
<li>변수 간의 조건부 독립성 검정(Independence Test)을 수행하여 엣지를 연결하거나 제거합니다 (예: PC Algorithm).</li>
<li>데이터 효율성이나 다중 가설 검정의 오류 문제 등이 존재합니다.</li>
</ul></li>
</ul>
</section>
<section id="the-paradigm-shift-from-discrete-to-continuous" class="level2">
<h2 class="anchored" data-anchor-id="the-paradigm-shift-from-discrete-to-continuous">The Paradigm Shift: From Discrete to Continuous</h2>
<ul>
<li>최근 <strong>Zheng et al.&nbsp;(2018)</strong>의 연구(흔히 <strong>NOTEARS</strong>로 알려짐)는 이 분야에 혁신적인 돌파구를 마련했습니다.</li>
<li>그들은 조합적 문제였던 “Acyclicity Constraint”를 미분 가능한 연속 함수 형태로 재정의했습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(A)%20=%20%5Ctext%7BTr%7D(e%5E%7BA%20%5Ccirc%20A%7D)%20-%20d%20=%200%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?A">는 인접 행렬(Adjacency Matrix), <img src="https://latex.codecogs.com/png.latex?d">는 노드의 개수입니다.</li>
<li>이 제약 조건 덕분에 구조 학습 문제는 이제 <strong>연속 최적화(Continuous Optimization)</strong> 문제로 변환되어, 경사 하강법(Gradient Descent)과 같은 표준적인 최적화 기법을 사용할 수 있게 되었습니다.</li>
</ul>
<section id="limitation-of-existing-continuous-methods" class="level3">
<h3 class="anchored" data-anchor-id="limitation-of-existing-continuous-methods">Limitation of Existing Continuous Methods</h3>
<ul>
<li>하지만 Zheng et al.&nbsp;(2018)의 접근법에도 한계가 있었습니다:
<ul>
<li><ol type="1">
<li><strong>선형성 가정 (Linearity Assumption):</strong> 기본적으로 선형 구조 방정식 모델(Linear SEM)을 가정합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>분포의 제약:</strong> 최소제곱(Least-squares) 손실 함수를 사용하므로, 실제 데이터의 복잡한 분포를 반영하기 어렵습니다.</li>
</ol></li>
</ul></li>
<li>현실 세계의 데이터는 비선형적 관계를 가지며, 단순한 선형 모델로는 포착할 수 없는 복잡한 메커니즘으로 생성됩니다.</li>
</ul>
</section>
</section>
<section id="dag-gnn-a-deep-generative-approach" class="level2">
<h2 class="anchored" data-anchor-id="dag-gnn-a-deep-generative-approach">DAG-GNN: A Deep Generative Approach</h2>
<ul>
<li>본 논문(Yu et al., 2019)은 딥러닝의 강력한 표현력을 활용하여 기존의 선형 가정을 극복하고자 합니다. 저자들은 <strong>DAG-GNN</strong>이라는 새로운 아키텍처를 제안합니다.</li>
</ul>
<section id="key-idea-vae-graph-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="key-idea-vae-graph-neural-networks">Key Idea: VAE + Graph Neural Networks</h3>
<ul>
<li><p>이 모델의 핵심은 <strong>Variational Autoencoder (VAE)</strong> 프레임워크에 <strong>Graph Neural Network (GNN)</strong>을 결합한 것입니다.</p></li>
<li><p><strong>Deep Generative Model:</strong> 신경망은 “Universal Approximator”입니다. 이를 통해 변수 간의 복잡한 비선형 관계를 모델링합니다.</p></li>
<li><p><strong>Encoder/Decoder Parameterization:</strong> VAE의 인코더와 디코더를 일반적인 MLP가 아닌, 특별히 설계된 <strong>GNN</strong>으로 파라미터화합니다.</p></li>
<li><p><strong>Evidence Lower Bound (ELBO):</strong> 모델의 목적 함수(Score)는 VAE의 ELBO가 됩니다. 이는 데이터의 우도(Likelihood)를 최대화하는 방향으로 학습됨을 의미합니다.</p></li>
</ul>
</section>
<section id="a-new-acyclicity-constraint" class="level3">
<h3 class="anchored" data-anchor-id="a-new-acyclicity-constraint">A New Acyclicity Constraint</h3>
<ul>
<li><p>또한, 저자들은 NOTEARS에서 제안된 행렬 지수(Matrix Exponential) 제약 조건 대신, 딥러닝 프레임워크에서 구현하기 더 용이하고 수치적으로 안정적인 <strong>다항식(Polynomial) 형태의 새로운 제약 조건</strong>을 제안합니다.</p></li>
<li><p><strong>NOTEARS</strong> (Zheng et al., 2018): <img src="https://latex.codecogs.com/png.latex?Tr(e%5E%7BA%20%5Ccirc%20A%7D)%20-%20d%20=%200"></p></li>
<li><p><strong>DAG-GNN 제안:</strong> 수치적 안정성을 높인 변형된 형태를 사용합니다.</p></li>
</ul>
</section>
</section>
<section id="main-contributions" class="level2">
<h2 class="anchored" data-anchor-id="main-contributions">Main Contributions</h2>
<ul>
<li><p>이 논문의 Introduction에서 강조하는 주요 기여점은 다음과 같이 네 가지로 요약할 수 있습니다.</p></li>
<li><ol type="1">
<li><strong>Deep Generative Model 기반 접근:</strong></li>
</ol>
<ul>
<li>기존의 Linear SEM을 넘어, VAE를 사용하여 데이터의 복잡한 비선형 분포를 포착하고 샘플링할 수 있는 모델을 제안했습니다.</li>
<li>그래프 구조(Weighted Adjacency Matrix)는 잠재 변수가 아니라, 신경망 파라미터와 함께 학습되는 명시적인 파라미터로 설정됩니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>다양한 데이터 타입 지원:</strong></li>
</ol>
<ul>
<li>VAE 프레임워크의 특성상, 디코더의 출력 분포(Likelihood)를 적절히 설정함으로써 연속형 변수뿐만 아니라 이산형(Discrete) 변수도 자연스럽게 처리할 수 있습니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>벡터 값 노드(Vector-valued Nodes) 지원:</strong></li>
</ol>
<ul>
<li>GNN을 사용하므로 각 노드가 단순 스칼라 값이 아닌 벡터 값을 가질 수 있습니다. 이는 각 노드가 여러 특징(Feature)을 가지는 복잡한 시나리오에 적용 가능함을 의미합니다.</li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>개선된 Acyclicity Constraint:</strong></li>
</ol>
<ul>
<li>기존의 행렬 지수 제약 조건이 자동 미분(Automatic Differentiation) 라이브러리에서 구현하기 까다로울 수 있다는 점을 지적하며, 더 실용적이고 수치적으로 안정적인 다항식 기반의 대안을 제시했습니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="background-and-related-work" class="level1">
<h1>2. Background and Related Work</h1>
<ul>
<li>DAG-GNN이 제안하는 새로운 방법론을 이해하기 위해서는, 먼저 <strong>구조 학습(Structure Learning)</strong>이 무엇인지, 그리고 지금까지 연구자들이 이 난제를 해결하기 위해 어떤 접근 방식을 취해왔는지 살펴볼 필요가 있습니다.</li>
<li>본 포스트에서는 논문의 <strong>Background</strong> 섹션을 바탕으로 인과 그래프 학습의 기술적 흐름을 정리합니다.</li>
</ul>
<section id="problem-definition-faithfulness-structure-learning" class="level2">
<h2 class="anchored" data-anchor-id="problem-definition-faithfulness-structure-learning">Problem Definition: Faithfulness &amp; Structure Learning</h2>
<ul>
<li>가장 먼저 정립해야 할 개념은 데이터와 그래프 사이의 관계입니다.</li>
<li>DAG(Directed Acyclic Graph) <img src="https://latex.codecogs.com/png.latex?G">와 결합 분포(Joint Distribution) <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">가 서로 <strong>Faithful</strong>하다는 것은 다음을 의미합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Faithfulness Condition (Pearl, 1988)</strong></p>
<p>분포 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">에서 성립하는 모든 조건부 독립성(Conditional Independence)이 그래프 <img src="https://latex.codecogs.com/png.latex?G">에서의 <strong>d-separation</strong> 조건에 의해 정확히 함의(entail)될 때, 그리고 그 역도 성립할 때 <img src="https://latex.codecogs.com/png.latex?G">와 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D">는 Faithful하다고 합니다.</p>
</blockquote>
<ul>
<li><strong>구조 학습(Structure Learning)</strong>이란, 미지의 분포로부터 생성된 i.i.d. 샘플 데이터 <img src="https://latex.codecogs.com/png.latex?D">가 주어졌을 때, 이 분포와 Faithful한 관계에 있는 (미지의) DAG <img src="https://latex.codecogs.com/png.latex?G">를 복원해내는 과정을 말합니다.</li>
</ul>
</section>
<section id="traditional-approaches-the-era-of-discrete-search" class="level2">
<h2 class="anchored" data-anchor-id="traditional-approaches-the-era-of-discrete-search">Traditional Approaches: The Era of Discrete Search</h2>
<ul>
<li>전통적으로 DAG를 학습하는 알고리즘은 크게 <strong>Score-based</strong> 방법과 <strong>Constraint-based</strong> 방법으로 나뉩니다.</li>
</ul>
<section id="score-based-approaches" class="level3">
<h3 class="anchored" data-anchor-id="score-based-approaches">1) Score-based Approaches</h3>
<p>이 접근법은 그래프의 ’적합도’를 평가하는 점수(Score)를 정의하고, 이 점수를 최적화하는 그래프 구조를 탐색합니다.</p>
<ul>
<li><strong>점수 기준 (Score Criteria):</strong> 주로 베이지안 관점의 점수들이 사용됩니다. 대표적으로 BDeu, BIC(Bayesian Information Criterion) 등이 있으며, 이들은 <strong>Decomposable</strong>(분해 가능), <strong>Consistent</strong>(일치성), <strong>Score Equivalent</strong>(점수 등가성) 등의 좋은 수학적 성질을 가집니다.</li>
<li><strong>탐색 알고리즘 (Search Procedures):</strong> 가능한 모든 그래프를 탐색하는 것은 불가능하므로 다양한 전략이 사용됩니다.
<ul>
<li><em>Hill-climbing:</em> 지역적 최적해를 찾아가는 탐욕적 방법 (Heckerman et al., 1995 등)</li>
<li><em>Forward-backward search</em> (Chickering, 2002)</li>
<li><em>Dynamic Programming</em> (Singh &amp; Moore, 2005 등)</li>
<li><em>A* Search:</em> 최단 경로 탐색 알고리즘 응용 (Yuan &amp; Malone, 2013)</li>
<li><em>Integer Programming:</em> 정수 계획법을 통한 최적화 (Cussens, 2011 등)</li>
</ul></li>
</ul>
</section>
<section id="constraint-based-approaches" class="level3">
<h3 class="anchored" data-anchor-id="constraint-based-approaches">2) Constraint-based Approaches</h3>
<ul>
<li><p>이 방식은 변수 쌍 사이의 조건부 독립성 검정(Independence Test)을 수행하여 엣지의 존재 여부를 결정합니다.</p></li>
<li><p><strong>대표 알고리즘:</strong> SGS, PC (Spirtes et al.), IC (Pearl), FCI (Zhang) 등이 있습니다.</p></li>
<li><p>독립성 검정 결과를 바탕으로 엣지를 제거하거나 방향을 설정하여 그래프를 완성합니다.</p></li>
</ul>
</section>
<section id="hybrid-approaches-approximations" class="level3">
<h3 class="anchored" data-anchor-id="hybrid-approaches-approximations">3) Hybrid Approaches &amp; Approximations</h3>
<ul>
<li><p>순수한 Score-based나 Constraint-based 방법의 단점을 보완하기 위한 시도들도 있습니다.</p></li>
<li><p><strong>Hybrid:</strong> MMHC (Tsamardinos et al., 2003)와 같이 Constraint-based 방법으로 후보군을 줄인 뒤 Score-based로 최적화하는 방식입니다.</p></li>
<li><p><strong>Approximations:</strong> 탐색 공간을 줄이기 위해 Tree-width를 제한하거나(Nie et al., 2014), 트리 구조를 가정하는(Chow &amp; Liu, 1968) 등의 가정을 도입하기도 합니다.</p></li>
</ul>
</section>
</section>
<section id="the-computational-bottleneck" class="level2">
<h2 class="anchored" data-anchor-id="the-computational-bottleneck">The Computational Bottleneck</h2>
<ul>
<li><p>이러한 전통적 방법론들이 직면한 가장 큰 문제는 <strong>NP-Hardness</strong>입니다.</p></li>
<li><p>변수의 수가 늘어남에 따라 가능한 DAG의 수는 초지수적(Superexponential)으로 증가합니다.</p></li>
<li><p>따라서 많은 알고리즘이 이산형 변수(Discrete variables)로 대상을 한정하거나, 변수들이 결합 가우시안 분포(Jointly Gaussian)를 따른다고 가정해야만 했습니다.</p></li>
</ul>
</section>
<section id="the-paradigm-shift-continuous-optimization" class="level2">
<h2 class="anchored" data-anchor-id="the-paradigm-shift-continuous-optimization">The Paradigm Shift: Continuous Optimization</h2>
<ul>
<li><p>최근 <strong>Zheng et al.&nbsp;(2018)</strong>의 연구(NOTEARS)는 이 분야에 새로운 패러다임을 제시했습니다.</p></li>
<li><p><strong>핵심 아이디어:</strong> 이산적인 탐색 과정(Discrete Search Procedure)을 <strong>등식 제약조건(Equality Constraint)이 있는 연속 최적화 문제</strong>로 변환했습니다.</p></li>
<li><p><strong>장점:</strong> 경사 하강법(Gradient Descent)과 같은 연속 최적화 기법을 사용하여 DAG 구조를 학습할 수 있게 되었습니다.</p></li>
<li><p><strong>한계:</strong> 이 접근법은 구조 복원 성능이 우수하지만, 설명의 편의를 위해 <strong>선형 구조 방정식 모델(Linear SEM)</strong>에만 적용된다는 한계가 있었습니다.</p></li>
</ul>
</section>
<section id="neural-network-approaches" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-approaches">Neural Network Approaches</h2>
<ul>
<li><p>최근에는 신경망(Neural Network)을 이용한 접근 방식도 등장하기 시작했습니다.</p></li>
<li><p><strong>GAN-style Approach (Kalainathan et al., 2018):</strong></p>
<ul>
<li>각 변수마다 별도의 생성 모델(Generative Model)을 두고, 생성된 샘플과 실제 데이터의 분포를 구분하는 판별자(Discriminator)를 두는 GAN 스타일의 방법론입니다.</li>
<li><strong>한계:</strong> 확장성(Scalability)은 좋아 보이지만, 결정적으로 <strong>비순환성(Acyclicity)이 강제되지 않는다</strong>는 문제가 있습니다. 즉, 학습된 결과가 DAG임을 보장할 수 없습니다.</li>
</ul></li>
</ul>
<blockquote class="blockquote">
<p><strong>Summary:</strong> 기존의 전통적 방법은 탐색 공간 문제로 확장이 어렵고, 최근 등장한 연속 최적화 방법(NOTEARS)은 선형성에 갇혀 있으며, 기존의 딥러닝 접근(GAN)은 DAG 구조를 보장하지 못합니다. <strong>DAG-GNN</strong>은 이러한 한계점들을 극복하기 위해 제안되었습니다.</p>
</blockquote>
<hr>
</section>
</section>
<section id="neural-dag-structure-learning" class="level1">
<h1>3. Neural DAG Structure Learning</h1>
<ul>
<li>본 논문은 선형 구조 방정식 모델(Linear SEM)을 일반화(Generalize)하여 딥러닝 기반의 생성 모델(Deep Generative Model)을 구축하는 것을 목표로 합니다.</li>
<li>첫 단계로, 가장 기본이 되는 <strong>Linear SEM</strong>의 수식적 구조와 그 의미를 먼저 명확히 짚고 넘어가겠습니다.</li>
</ul>
<section id="linear-structural-equation-model-linear-sem" class="level2">
<h2 class="anchored" data-anchor-id="linear-structural-equation-model-linear-sem">3.1. Linear Structural Equation Model (Linear SEM)</h2>
<ul>
<li>우리가 찾고자 하는 DAG(Directed Acyclic Graph)의 구조는 <strong>가중치가 있는 인접 행렬(Weighted Adjacency Matrix)</strong> <img src="https://latex.codecogs.com/png.latex?A">로 표현됩니다.</li>
</ul>
<section id="notation-및-정의" class="level3">
<h3 class="anchored" data-anchor-id="notation-및-정의">Notation 및 정의</h3>
<ul>
<li><p>먼저 모델링에 필요한 변수들을 정의합니다.</p></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?m"></strong>: 노드(변수)의 개수입니다.</p></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20m%7D"></strong>:</p>
<ul>
<li>DAG의 가중치 인접 행렬입니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?A_%7Bij%7D">가 0이 아니라면 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 가는 엣지가 존재함을 의미합니다.</li>
</ul></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D"></strong>:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?m">개의 변수에 대한 데이터 행렬입니다.</li>
<li>일반적인 문헌에서 변수는 스칼라(<img src="https://latex.codecogs.com/png.latex?d=1">)로 취급되지만, 본 논문에서는 이를 <img src="https://latex.codecogs.com/png.latex?d">-차원 벡터로 일반화(Generalize)하여 <strong>Vector-valued Node</strong>를 다룹니다.</li>
<li>각 행(Row)은 하나의 변수(노드)에 대응하고, 각 열(Column)은 해당 변수의 특징(Feature) 혹은 샘플 차원을 의미합니다.</li>
</ul></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?Z%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D"></strong>:</p>
<ul>
<li>노이즈 행렬(Noise Matrix)입니다.</li>
<li>외생 변수(Exogenous variable)에 해당합니다.</li>
</ul></li>
</ul>
</section>
<section id="선형-관계식-the-model" class="level3">
<h3 class="anchored" data-anchor-id="선형-관계식-the-model">선형 관계식 (The Model)</h3>
<ul>
<li>Linear SEM은 변수 <img src="https://latex.codecogs.com/png.latex?X">가 부모 변수들의 선형 결합(Linear Combination)과 노이즈 <img src="https://latex.codecogs.com/png.latex?Z">의 합으로 생성된다고 가정합니다. 이를 행렬식으로 표현하면 다음과 같습니다.</li>
</ul>
<p><span id="eq-(1)"><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20A%5ET%20X%20+%20Z%0A%5Ctag%7B1%7D"></span></p>
<ul>
<li>이 식은 <strong>“현재 노드의 값(<img src="https://latex.codecogs.com/png.latex?X">)은 부모 노드들의 값(<img src="https://latex.codecogs.com/png.latex?A%5ET%20X">)에 가중치를 곱해 더한 뒤, 고유한 노이즈(<img src="https://latex.codecogs.com/png.latex?Z">)를 더한 것과 같다”</strong>는 인과적 메커니즘을 나타냅니다.</li>
</ul>
</section>
<section id="from-structure-to-generation-derivation" class="level3">
<h3 class="anchored" data-anchor-id="from-structure-to-generation-derivation">From Structure to Generation (Derivation)</h3>
<ul>
<li>이제 이 구조 방정식(Structural Equation)을 데이터를 생성하는 <strong>생성 모델(Generative Model)</strong>의 관점으로 전환해보겠습니다.</li>
<li>이를 위해서는 <img src="https://latex.codecogs.com/png.latex?Z">에서 <img src="https://latex.codecogs.com/png.latex?X">를 만들어내는 과정으로 수식을 변형해야 합니다.</li>
</ul>
<section id="adjacency-matrix" class="level4">
<h4 class="anchored" data-anchor-id="adjacency-matrix">Adjacency Matrix</h4>
<ul>
<li><p><strong>사이클 부재 (Acyclicity):</strong> DAG의 가장 중요한 성질 중 하나는, 그래프 내에 사이클(Cycle)이 없기 때문에 모든 노드를 <strong>위상 정렬(Topological Order)</strong> 순서로 나열할 수 있다는 점입니다.</p></li>
<li><p><strong>행렬의 형태:</strong> 노드들을 위상 정렬 순서(<img src="https://latex.codecogs.com/png.latex?%5Cpi">)대로 재배열하면, 인접 행렬 <img src="https://latex.codecogs.com/png.latex?A">는 <strong>엄격한 상삼각 행렬(Strictly Upper Triangular Matrix)</strong>이 됩니다.</p>
<ul>
<li>즉, 대각 성분과 그 아래 성분들이 모두 0이 됩니다 (<img src="https://latex.codecogs.com/png.latex?A_%7Bij%7D%20=%200%20%5Ctext%7B%20for%20%7D%20i%20%5Cge%20j">).</li>
<li>이는 어떤 노드도 자기 자신이나 자신의 후손(descendant)으로부터 영향을 받지 않음을 수학적으로 보장합니다.</li>
</ul></li>
<li><p>위상 정렬된 그래프의 인접 행렬 <img src="https://latex.codecogs.com/png.latex?A">는 다음과 같은 형태를 띱니다. 대각선(Diagonal)을 포함한 하단 부분이 모두 <strong>0</strong>이 되는 것이 핵심입니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AA_%7B%5Cpi%7D%20=%0A%5Cbegin%7Bbmatrix%7D%0A%5Ccolor%7Bred%7D0%20&amp;%20w_%7B12%7D%20&amp;%20w_%7B13%7D%20&amp;%20%5Ccdots%20&amp;%20w_%7B1d%7D%20%5C%5C%0A%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccolor%7Bred%7D0%20&amp;%20w_%7B23%7D%20&amp;%20%5Ccdots%20&amp;%20w_%7B2d%7D%20%5C%5C%0A%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccolor%7Bred%7D0%20&amp;%20%5Ccdots%20&amp;%20w_%7B3d%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccolor%7Bblue%7D0%20&amp;%20%5Ccdots%20&amp;%20%5Ccolor%7Bred%7D0%0A%5Cend%7Bbmatrix%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Ccolor%7Bred%7D%7B0%7D"> (대각 성분): <img src="https://latex.codecogs.com/png.latex?A_%7Bii%7D%20=%200">. 자기 자신으로 돌아오는 루프(Self-loop)가 없음.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ccolor%7Bblue%7D%7B0%7D"> (하삼각 성분): <img src="https://latex.codecogs.com/png.latex?A_%7Bji%7D%20=%200%20%5Cquad%20(%5Ctext%7Bwhere%20%7D%20j%20%3E%20i)">. 역방향(미래에서 과거로) 연결이 없음.</li>
<li><strong>결론:</strong> <img src="https://latex.codecogs.com/png.latex?i%20%5Cge%20j"> 인 모든 성분에 대해 <img src="https://latex.codecogs.com/png.latex?A_%7Bij%7D%20=%200"> 입니다.</li>
</ul>
</section>
<section id="transformation" class="level4">
<h4 class="anchored" data-anchor-id="transformation">Transformation</h4>
<ul>
<li>식 (1)을 <img src="https://latex.codecogs.com/png.latex?X">에 대해 정리하는 과정을 단계별로 유도해보겠습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AX%20-%20A%5ET%20X%20&amp;=%20Z%20%5C%5C%0A(I%20-%20A%5ET)X%20&amp;=%20Z%20%5C%5C%0AX%20&amp;=%20(I%20-%20A%5ET)%5E%7B-1%7D%20Z%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="equivalence-with-ancestral-sampling" class="level4">
<h4 class="anchored" data-anchor-id="equivalence-with-ancestral-sampling">Equivalence with Ancestral Sampling</h4>
<p><span id="eq-(2)"><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20(I%20-%20A%5ET)%5E%7B-1%7D%20Z%0A%5Ctag%7B2%7D"></span></p>
<ul>
<li>이 식은 <strong>“DAG를 따르는 Ancestral Sampling”</strong> 과정을 수학적으로 압축해 놓은 형태입니다. 그 이유를 단계별로 살펴보겠습니다.</li>
</ul>
<section id="ancestral-sampling이란" class="level5">
<h5 class="anchored" data-anchor-id="ancestral-sampling이란">Ancestral Sampling이란?</h5>
<ul>
<li><strong>Ancestral Sampling(조상 샘플링)</strong>은 베이지안 네트워크에서 데이터를 생성하는 가장 표준적인 방법입니다.</li>
<li>인과관계의 흐름(부모 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 자식)에 따라 순차적으로 값을 결정하는 방식입니다.
<ul>
<li><ol type="1">
<li><strong>Top-down:</strong> 부모가 없는 루트 노드(조상)의 값을 먼저 노이즈(<img src="https://latex.codecogs.com/png.latex?Z">)로부터 결정합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Propagation:</strong> 그 결정된 값이 자식 노드로 전파되어, 자식 노드의 값 결정에 영향을 줍니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li>이 과정을 위상 정렬(Topological Sort) 순서대로 끝까지 반복합니다.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="neumann-series를-통한-연결" class="level5">
<h5 class="anchored" data-anchor-id="neumann-series를-통한-연결">Neumann Series를 통한 연결</h5>
<ul>
<li>식 (2) 의 역행렬 부분 <img src="https://latex.codecogs.com/png.latex?(I%20-%20A%5ET)%5E%7B-1%7D">을 <strong>노이만 급수(Neumann Series)</strong>를 이용해 전개합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A(I%20-%20A%5ET)%5E%7B-1%7D%20=%20I%20+%20A%5ET%20+%20(A%5ET)%5E2%20+%20(A%5ET)%5E3%20+%20%5Cdots%0A"></p>
<ul>
<li>이 전개식을 식 2에 대입하면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20%5Cunderbrace%7BI%20%5Ccdot%20Z%7D_%7B%5Ctext%7BSelf%7D%7D%20+%20%5Cunderbrace%7BA%5ET%20%5Ccdot%20Z%7D_%7B%5Ctext%7BParents%7D%7D%20+%20%5Cunderbrace%7B(A%5ET)%5E2%20%5Ccdot%20Z%7D_%7B%5Ctext%7BGrandparents%7D%7D%20+%20%5Cdots%0A"></p>
</section>
<section id="행렬-거듭제곱ak의-의미" class="level5">
<h5 class="anchored" data-anchor-id="행렬-거듭제곱ak의-의미">행렬 거듭제곱(<img src="https://latex.codecogs.com/png.latex?A%5Ek">)의 의미</h5>
<ul>
<li>여기서 행렬의 거듭제곱 항들은 그래프 이론에서 <strong>경로(Path)</strong>의 개념과 일치합니다.
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?I"> (0-hop):</strong> 자기 자신의 고유한 노이즈(<img src="https://latex.codecogs.com/png.latex?Z">)입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?A%5ET"> (1-hop):</strong> 부모 노드들로부터 직접 받는 영향입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?(A%5ET)%5E2"> (2-hop):</strong> 부모를 거쳐서 오는 <strong>조부모(Grandparents)</strong>의 영향입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?(A%5ET)%5Ek"> (<img src="https://latex.codecogs.com/png.latex?k">-hop):</strong> <img src="https://latex.codecogs.com/png.latex?k">단계를 거쳐서 오는 <strong><img src="https://latex.codecogs.com/png.latex?k">대 조상</strong>들의 영향입니다.</li>
</ul></li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?X%20=%20(I%20-%20A%5ET)%5E%7B-1%7D%20Z">라는 수식은 <strong>“나의 값(<img src="https://latex.codecogs.com/png.latex?X">)은 내 고유한 성향(<img src="https://latex.codecogs.com/png.latex?Z">)뿐만 아니라, 부모, 조부모 등 모든 조상들의 영향력이 누적되어 형성된 것이다”</strong>라는 Ancestral Sampling의 철학을 행렬 연산 한 번으로 표현한 것입니다.</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="보충: 왜 거듭제곱이 경로인가? (수학적 귀납법 증명)">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>보충: 왜 거듭제곱이 경로인가? (수학적 귀납법 증명)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>행렬 <img src="https://latex.codecogs.com/png.latex?A">의 거듭제곱 <img src="https://latex.codecogs.com/png.latex?A%5Ek">의 <img src="https://latex.codecogs.com/png.latex?(i,%20j)"> 성분이 <strong>노드 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 가는 길이 <img src="https://latex.codecogs.com/png.latex?k">인 모든 경로의 가중치 합</strong>임을 수학적 귀납법으로 간단히 증명할 수 있습니다.</p>
<p><strong>명제 <img src="https://latex.codecogs.com/png.latex?P(k)">:</strong> <img src="https://latex.codecogs.com/png.latex?(A%5Ek)_%7Bij%7D">는 <img src="https://latex.codecogs.com/png.latex?i%20%5Cto%20j">로 가는 길이 <img src="https://latex.codecogs.com/png.latex?k">인 경로들의 가중치 합이다.</p>
<p><strong>1. 기초 단계 (Base Case, <img src="https://latex.codecogs.com/png.latex?k=1">):</strong> 정의에 의해 <img src="https://latex.codecogs.com/png.latex?(A%5E1)_%7Bij%7D%20=%20A_%7Bij%7D">입니다. 이는 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 직접 연결된 엣지(길이 1)의 가중치이므로 명제 <img src="https://latex.codecogs.com/png.latex?P(1)">은 참입니다.</p>
<p><strong>2. 귀납 가정 (Inductive Step):</strong> 임의의 자연수 <img src="https://latex.codecogs.com/png.latex?k">에 대해 <img src="https://latex.codecogs.com/png.latex?P(k)">가 참이라고 가정합니다. 즉, <img src="https://latex.codecogs.com/png.latex?(A%5Ek)_%7Bit%7D">는 <img src="https://latex.codecogs.com/png.latex?i%20%5Cto%20t">로 가는 길이 <img src="https://latex.codecogs.com/png.latex?k">인 경로의 합입니다.</p>
<p>이제 <img src="https://latex.codecogs.com/png.latex?k+1">일 때를 살펴봅니다. 행렬 곱셈의 정의에 따라: <img src="https://latex.codecogs.com/png.latex?%0A(A%5E%7Bk+1%7D)_%7Bij%7D%20=%20(A%5Ek%20%5Ccdot%20A)_%7Bij%7D%20=%20%5Csum_%7Bt%7D%20(A%5Ek)_%7Bit%7D%20%5Ccdot%20A_%7Btj%7D%0A"></p>
<p>이 수식의 의미를 해석해 보면: * <img src="https://latex.codecogs.com/png.latex?(A%5Ek)_%7Bit%7D">: <img src="https://latex.codecogs.com/png.latex?i">에서 중간 노드 <img src="https://latex.codecogs.com/png.latex?t">까지 가는 길이 <img src="https://latex.codecogs.com/png.latex?k">인 경로 (귀납 가정) * <img src="https://latex.codecogs.com/png.latex?A_%7Btj%7D">: <img src="https://latex.codecogs.com/png.latex?t">에서 도착 노드 <img src="https://latex.codecogs.com/png.latex?j">로 가는 길이 <img src="https://latex.codecogs.com/png.latex?1">인 엣지 * <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bt%7D">: 가능한 모든 중간 경유지 <img src="https://latex.codecogs.com/png.latex?t">에 대해 합산</p>
<p>즉, <strong>“<img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?t">까지 <img src="https://latex.codecogs.com/png.latex?k">걸음으로 간 뒤, <img src="https://latex.codecogs.com/png.latex?t">에서 <img src="https://latex.codecogs.com/png.latex?j">로 1걸음 더 가는 모든 경우의 수”</strong>를 더한 것이므로, 이는 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 가는 <strong>길이 <img src="https://latex.codecogs.com/png.latex?k+1">인 모든 경로의 합</strong>과 같습니다.</p>
<p><strong>3. 결론:</strong> 수학적 귀납법에 의해 모든 자연수 <img src="https://latex.codecogs.com/png.latex?k">에 대해 명제 <img src="https://latex.codecogs.com/png.latex?P(k)">는 참입니다. 따라서 <img src="https://latex.codecogs.com/png.latex?(A%5ET)%5Ek">는 <img src="https://latex.codecogs.com/png.latex?k">단계를 거친 조상들의 영향력을 의미하게 됩니다.</p>
</div>
</div>
</div>
</section>
</section>
<section id="결론-vae-도입의-동기" class="level4">
<h4 class="anchored" data-anchor-id="결론-vae-도입의-동기">결론: VAE 도입의 동기</h4>
<ul>
<li>이러한 관점은 DAG 구조 학습 문제를 새로운 시각으로 바라보게 합니다.</li>
<li>복잡한 인과관계 추론 문제를 <strong>“노이즈 <img src="https://latex.codecogs.com/png.latex?Z">를 데이터 <img src="https://latex.codecogs.com/png.latex?X">로 매핑하는 인코더/디코더를 학습하는 문제”</strong>로 치환할 수 있습니다.</li>
<li>이것이 바로 저자들이 생성 모델의 대표 주자인 <strong>VAE(Variational Autoencoder)</strong>를 도입하여, 인코더와 디코더를 통해 <img src="https://latex.codecogs.com/png.latex?A">를 학습하고자 한 핵심적인 <strong>동기(Motivation)</strong>입니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="proposed-graph-neural-network-model" class="level2">
<h2 class="anchored" data-anchor-id="proposed-graph-neural-network-model">3.2. Proposed Graph Neural Network Model</h2>
<ul>
<li>앞선 섹션에서 우리는 Linear SEM이 다음과 같이 노이즈 <img src="https://latex.codecogs.com/png.latex?Z">를 데이터 <img src="https://latex.codecogs.com/png.latex?X">로 변환하는 선형 과정으로 표현됨을 확인했습니다.</li>
<li>본 섹션에서는 이 수식을 <strong>Graph Neural Network(GNN)</strong>의 관점에서 재해석하고, 이를 바탕으로 비선형 관계까지 포착할 수 있는 <strong>DAG-GNN</strong>만의 독창적인 아키텍처를 검토합니다.</li>
</ul>
<section id="linear-sem-as-a-graph-neural-network" class="level3">
<h3 class="anchored" data-anchor-id="linear-sem-as-a-graph-neural-network">Linear SEM as a Graph Neural Network</h3>
<ul>
<li>저자들은 위 식 (2)를 딥러닝 커뮤니티의 관점에서 다음과 같은 일반적인 함수 꼴로 바라봅니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20f_A(Z)%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?f_A">는 그래프 구조(인접 행렬 <img src="https://latex.codecogs.com/png.latex?A">)에 의해 파라미터화된 함수입니다.</li>
<li>즉, <strong>“노드 특징(Feature)인 <img src="https://latex.codecogs.com/png.latex?Z">를 입력받아, 그래프 구조를 통과시켜 고차원 표현인 <img src="https://latex.codecogs.com/png.latex?X">를 반환하는 과정”</strong>으로 해석할 수 있습니다.</li>
</ul>
<section id="existing-gnn-architectures" class="level4">
<h4 class="anchored" data-anchor-id="existing-gnn-architectures">Existing GNN Architectures</h4>
<ul>
<li>대부분의 최신 GNN 모델들(GCN, GraphSAGE, GAT 등)도 이와 유사한 형태를 띱니다.</li>
<li>예를 들어, 널리 쓰이는 <strong>GCN (Graph Convolutional Network)</strong>의 수식은 다음과 같습니다. <img src="https://latex.codecogs.com/png.latex?%0AX%20=%20%5Chat%7BA%7D%20%5Ccdot%20%5Ctext%7BReLU%7D(%5Chat%7BA%7D%20Z%20W%5E1)%20%5Ccdot%20W%5E2%0A">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7BA%7D">: 정규화된 인접 행렬 (Normalized Adjacency Matrix)</li>
<li><img src="https://latex.codecogs.com/png.latex?W%5E1,%20W%5E2">: 학습 가능한 가중치 행렬</li>
</ul></li>
<li>하지만 일반적인 GNN은 주어진 고정된 그래프 위에서 노드 임베딩을 학습하는 것이 목표인 반면, 우리의 목표는 <strong>그래프 구조 <img src="https://latex.codecogs.com/png.latex?A"> 자체를 학습</strong>하는 것입니다.</li>
</ul>
</section>
</section>
<section id="the-dag-gnn-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-dag-gnn-architecture">The DAG-GNN Architecture</h3>
<ul>
<li>저자들은 Linear SEM의 구조적 특성(<img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7D">)을 그대로 계승하면서, 신경망의 표현력을 더하기 위해 <strong>새로운 GNN 아키텍처</strong>를 제안합니다.</li>
</ul>
<section id="the-proposed-equation" class="level4">
<h4 class="anchored" data-anchor-id="the-proposed-equation">The Proposed Equation</h4>
<p>제안하는 모델의 핵심 수식은 다음과 같습니다.</p>
<p><span id="eq-(3)"><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20f_2%20%5Cleft(%20(I%20-%20A%5ET)%5E%7B-1%7D%20f_1(Z)%20%5Cright)%0A%5Ctag%7B3%7D"></span></p>
<ul>
<li><p>이 수식은 세 단계의 변환 과정으로 구성됩니다:</p></li>
<li><ol type="1">
<li><strong>Transforming Noise (<img src="https://latex.codecogs.com/png.latex?f_1(Z)">):</strong></li>
</ol>
<ul>
<li>입력 노이즈 <img src="https://latex.codecogs.com/png.latex?Z">를 비선형 함수 <img src="https://latex.codecogs.com/png.latex?f_1"> (MLP 등)을 통해 변환합니다.</li>
<li>이는 단순한 가우시안 노이즈가 아닌 복잡한 잠재 분포를 표현하기 위함입니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Structural Aggregation (<img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7D">):</strong></li>
</ol>
<ul>
<li>변환된 신호들이 DAG 구조 <img src="https://latex.codecogs.com/png.latex?A">에 따라 전파(Propagation)됩니다.</li>
<li>이 부분은 Linear SEM의 인과적 흐름을 그대로 따르며, 부모 노드의 영향력이 자식 노드로 전달되는 과정을 수학적으로 구현합니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>Transforming into Data Space (<img src="https://latex.codecogs.com/png.latex?f_2(%5Ccdot)">):</strong></li>
</ol>
<ul>
<li>구조적 정보가 반영된 신호를 다시 비선형 함수 <img src="https://latex.codecogs.com/png.latex?f_2">를 통해 관측 데이터 공간(<img src="https://latex.codecogs.com/png.latex?X">)으로 매핑합니다.</li>
</ul></li>
</ul>
</section>
</section>
<section id="generalizing-the-linear-sem-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="generalizing-the-linear-sem-interpretation">Generalizing the Linear SEM (Interpretation)</h3>
<ul>
<li><p>이 모델이 중요한 이유는, 이것이 기존의 <strong>Linear SEM을 비선형(Non-linear)으로 일반화(Generalize)</strong>한 형태이기 때문입니다.</p></li>
<li><p>만약 <img src="https://latex.codecogs.com/png.latex?f_2">가 <strong>역함수(Invertible)를 가진다</strong>고 가정해봅시다. 그렇다면 식 (3)의 양변에 <img src="https://latex.codecogs.com/png.latex?f_2%5E%7B-1%7D">를 취하고 정리하여 다음과 같은 관계를 유도할 수 있습니다.</p></li>
</ul>
<section id="derivation" class="level4">
<h4 class="anchored" data-anchor-id="derivation">Derivation</h4>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AX%20&amp;=%20f_2((I%20-%20A%5ET)%5E%7B-1%7D%20f_1(Z))%20%5C%5C%0Af_2%5E%7B-1%7D(X)%20&amp;=%20(I%20-%20A%5ET)%5E%7B-1%7D%20f_1(Z)%20%5C%5C%0A(I%20-%20A%5ET)%20f_2%5E%7B-1%7D(X)%20&amp;=%20f_1(Z)%20%5C%5C%0Af_2%5E%7B-1%7D(X)%20-%20A%5ET%20f_2%5E%7B-1%7D(X)%20&amp;=%20f_1(Z)%20%5C%5C%0Af_2%5E%7B-1%7D(X)%20&amp;=%20A%5ET%20f_2%5E%7B-1%7D(X)%20+%20f_1(Z)%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="generalized-sem" class="level4">
<h4 class="anchored" data-anchor-id="generalized-sem">Generalized SEM</h4>
<ul>
<li>결과적으로 다음과 같은 <strong>Generalized SEM</strong> 식을 얻게 됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7Bf_2%5E%7B-1%7D(X)%7D_%7B%5Ctext%7BTransformed%20Data%7D%7D%20=%20A%5ET%20%5Cunderbrace%7Bf_2%5E%7B-1%7D(X)%7D_%7B%5Ctext%7BParents%7D%7D%20+%20%5Cunderbrace%7Bf_1(Z)%7D_%7B%5Ctext%7BTransformed%20Noise%7D%7D%0A"></p>
<ul>
<li><strong>Linear SEM (<img src="https://latex.codecogs.com/png.latex?X%20=%20A%5ET%20X%20+%20Z">)과의 비교:</strong>
<ul>
<li>Linear SEM은 데이터 <img src="https://latex.codecogs.com/png.latex?X"> 자체가 선형 결합을 이룹니다.</li>
<li>DAG-GNN은 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 적절히 비선형 변환한 <strong><img src="https://latex.codecogs.com/png.latex?f_2%5E%7B-1%7D(X)"> 공간에서 선형 관계(<img src="https://latex.codecogs.com/png.latex?A%5ET">)가 성립</strong>한다고 가정합니다.</li>
<li>또한 노이즈 역시 단순 합이 아니라 <img src="https://latex.codecogs.com/png.latex?f_1(Z)"> 형태로 비선형적으로 결합됩니다.</li>
</ul></li>
<li>이러한 설계를 통해 DAG-GNN은 변수 간의 관계가 복잡한 비선형일 때도, 이를 잠재 공간(Latent Space)에서의 구조적 관계로 포착할 수 있게 됩니다.</li>
</ul>
</section>
</section>
<section id="note-on-implementation" class="level3">
<h3 class="anchored" data-anchor-id="note-on-implementation">Note on Implementation</h3>
<ul>
<li>저자들은 <img src="https://latex.codecogs.com/png.latex?f_1">과 <img src="https://latex.codecogs.com/png.latex?f_2">를 구체적으로 어떤 신경망으로 구현할지에 대해서는 후속 섹션으로 미루고 있습니다.</li>
<li>다만 중요한 제약 사항 하나를 언급합니다:</li>
</ul>
<blockquote class="blockquote">
<p>“<img src="https://latex.codecogs.com/png.latex?f_2">의 마지막 활성화 함수(Activation function)는 반드시 변수 <img src="https://latex.codecogs.com/png.latex?X">의 타입(Domain)과 일치해야 한다.”</p>
</blockquote>
<ul>
<li>예를 들어, <img src="https://latex.codecogs.com/png.latex?X">가 실수형(Continuous)이라면 Identity 함수를, 이진형(Binary)이라면 Sigmoid 등을 사용해야 한다는 뜻입니다. 이는 추후 논의될 <strong>Discrete Variable</strong> 처리를 위한 포석입니다.</li>
</ul>
<hr>
</section>
</section>
<section id="model-learning-with-variational-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="model-learning-with-variational-autoencoder">3.3. Model Learning with Variational Autoencoder</h2>
<ul>
<li>이제 우리의 목표는 주어진 데이터 <img src="https://latex.codecogs.com/png.latex?X%5E1,%20%5Cdots,%20X%5En">을 가장 잘 설명하는 모델 파라미터(신경망 가중치 및 그래프 구조 <img src="https://latex.codecogs.com/png.latex?A">)를 찾는 것입니다.</li>
<li>본 섹션에서는 이를 위해 <strong>Variational Autoencoder (VAE)</strong> 프레임워크를 도입하는 과정과 그 수학적 배경을 상세히 다룹니다.</li>
</ul>
<section id="the-challenge-of-intractability" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge-of-intractability">The Challenge of Intractability</h3>
<ul>
<li><p>일반적으로 확률 모델의 학습은 관측 데이터의 <strong>로그 우도(Log-Likelihood)</strong>, 또는 <strong>로그 증거(Log-Evidence)</strong>를 최대화하는 방향으로 진행됩니다.</p></li>
<li><p>데이터 샘플 <img src="https://latex.codecogs.com/png.latex?X%5E1,%20%5Cdots,%20X%5En">이 주어졌을 때, 평균 로그 증거는 다음과 같습니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20%5Clog%20p(X%5Ek)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bk=1%7D%5En%20%5Clog%20%5Cint%20p(X%5Ek%20%7C%20Z)%20p(Z)%20%5C,%20dZ%0A"></p>
<ul>
<li>여기서 문제가 발생합니다.
<ul>
<li>우변의 적분 <img src="https://latex.codecogs.com/png.latex?%5Cint%20p(X%5Ek%20%7C%20Z)%20p(Z)%20%5C,%20dZ">는 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">의 모든 가능한 값에 대해 주변화(Marginalization)를 수행해야 합니다.</li>
<li>하지만 <img src="https://latex.codecogs.com/png.latex?Z">가 고차원이거나 <img src="https://latex.codecogs.com/png.latex?p(X%7CZ)">가 복잡한 신경망(Neural Network)으로 구성된 경우, 이 적분은 해석적으로 구하는 것이 불가능하며(Intractable), 수치적으로 근사하기에도 계산 비용이 매우 큽니다.</li>
</ul></li>
<li>따라서 저자들은 이 문제를 해결하기 위해 <strong>변분 베이즈(Variational Bayes)</strong> 방법론을 도입합니다.</li>
</ul>
</section>
<section id="the-evidence-lower-bound-elbo" class="level3">
<h3 class="anchored" data-anchor-id="the-evidence-lower-bound-elbo">The Evidence Lower Bound (ELBO)</h3>
<ul>
<li>계산 불가능한 사후 분포 <img src="https://latex.codecogs.com/png.latex?p(Z%7CX)">를 근사하기 위해, 우리는 다루기 쉬운 분포인 <strong>변분 사후 분포(Variational Posterior)</strong> <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">를 도입합니다.</li>
</ul>
<section id="derivation-of-elbo" class="level4">
<h4 class="anchored" data-anchor-id="derivation-of-elbo">Derivation of ELBO</h4>
<ul>
<li><p>로그 증거 <img src="https://latex.codecogs.com/png.latex?%5Clog%20p(X)">에서 출발하여 <strong>ELBO(Evidence Lower Bound)</strong>를 유도하는 과정은 다음과 같습니다. (편의상 <img src="https://latex.codecogs.com/png.latex?X%5Ek">를 <img src="https://latex.codecogs.com/png.latex?X">로 표기합니다.)</p></li>
<li><ol type="1">
<li><strong>로그 내부의 분모/분자에 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)"> 곱하기:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cbegin%7Baligned%7D%0A%20%20%5Clog%20p(X)%20&amp;=%20%5Clog%20%5Cint%20p(X%7CZ)%20p(Z)%20%5C,%20dZ%20%5C%5C%20%20%0A%20%20&amp;=%20%5Clog%20%5Cint%20p(X,%20Z)%20%5C,%20dZ%20%5C%5C%0A%20%20&amp;=%20%5Clog%20%5Cint%20p(X,%20Z)%20%5Cfrac%7Bq(Z%7CX)%7D%7Bq(Z%7CX)%7D%20%5C,%20dZ%0A%20%20%5Cend%7Baligned%7D%0A%20%20"></li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>기댓값 형태로 변환:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cbegin%7Baligned%7D%0A%20%20&amp;=%20%5Clog%20%5Cint%20%5Cfrac%7Bp(X,%20Z)%7D%7Bq(Z%7CX)%7D%20q(Z%7CX)%20%5C,%20dZ%20%5C%5C%0A%20%20&amp;=%20%5Clog%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5Cleft%5B%20%5Cfrac%7Bp(X,%20Z)%7D%7Bq(Z%7CX)%7D%20%5Cright%5D%0A%20%20%5Cend%7Baligned%7D%0A%20%20"></li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>젠센 부등식(Jensen’s Inequality) 적용:</strong></li>
</ol>
<ul>
<li>로그 함수는 오목(Concave) 함수이므로 <img src="https://latex.codecogs.com/png.latex?%5Clog(%5Cmathbb%7BE%7D%5BY%5D)%20%5Cge%20%5Cmathbb%7BE%7D%5B%5Clog(Y)%5D">가 성립합니다. <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cge%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5Cleft%5B%20%5Clog%20%5Cfrac%7Bp(X,%20Z)%7D%7Bq(Z%7CX)%7D%20%5Cright%5D%0A%20%20"></li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>항 분리 및 정리 (ELBO의 도출):</strong></li>
</ol>
<ul>
<li>로그의 성질을 이용해 항을 분리하고, <img src="https://latex.codecogs.com/png.latex?Z">와 관련된 항들을 묶어 정리합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cbegin%7Baligned%7D%0A%20%20%5Cmathcal%7BL%7D%20&amp;=%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5Cleft%5B%20%5Clog%20%5Cfrac%7Bp(X%7CZ)p(Z)%7D%7Bq(Z%7CX)%7D%20%5Cright%5D%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5CBig%5B%20%5Clog%20p(X%7CZ)%20+%20%5Cunderbrace%7B%5Clog%20p(Z)%20-%20%5Clog%20q(Z%7CX)%7D_%7B%5Ctext%7BLatent%20Variable%20Terms%7D%7D%20%5CBig%5D%0A%20%20%5Cend%7Baligned%7D%0A%20%20"></p>
<ul>
<li>여기서 <strong>KL Divergence(Kullback-Leibler Divergence)</strong>의 정의를 도입하여 식을 간결하게 정리할 수 있습니다.</li>
<li>두 확률분포 <img src="https://latex.codecogs.com/png.latex?q">와 <img src="https://latex.codecogs.com/png.latex?p">의 차이를 측정하는 KL Divergence는 다음과 같이 정의됩니다. <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(q%20%7C%7C%20p)%20=%20%5Cmathbb%7BE%7D_%7Bx%20%5Csim%20q%7D%20%5B%20%5Clog%20q(x)%20-%20%5Clog%20p(x)%20%5D"></li>
<li>위 식의 뒷부분(<img src="https://latex.codecogs.com/png.latex?%5Clog%20p(Z)%20-%20%5Clog%20q(Z%7CX)">)은 KL Divergence 정의와 부호가 반대입니다. 따라서 마이너스(<img src="https://latex.codecogs.com/png.latex?-">)를 밖으로 빼내어 형태를 맞춥니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Cbegin%7Baligned%7D%0A%20%20&amp;=%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5B%5Clog%20p(X%7CZ)%5D%20-%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5B%5Cunderbrace%7B%5Clog%20q(Z%7CX)%20-%20%5Clog%20p(Z)%7D_%7BD_%7B%5Ctext%7BKL%7D%7D(q%7C%7Cp)%20%5Ctext%7B%20Form%7D%7D%20%5D%20%5C%5C%0A%20%20&amp;=%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5B%5Clog%20p(X%7CZ)%5D%20-%20D_%7B%5Ctext%7BKL%7D%7D(q(Z%7CX)%20%7C%7C%20p(Z))%0A%20%20%5Cend%7Baligned%7D%0A%20%20"></p>
<ul>
<li>최종적으로 식 (4)에 해당하는 <strong>ELBO(Evidence Lower Bound)</strong>를 얻게 됩니다.</li>
</ul></li>
</ul>
<p><span id="eq-(4)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7BELBO%7D%7D%20%5Cequiv%20-D_%7B%5Ctext%7BKL%7D%7D%20%5CBig(%20q(Z%7CX%5Ek)%20%5C,%7C%7C%5C,%20p(Z)%20%5CBig)%20+%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX%5Ek)%7D%20%5CBig%5B%20%5Clog%20p(X%5Ek%7CZ)%20%5CBig%5D%0A%5Ctag%7B4%7D"></span></p>
</section>
<section id="interpretation-of-elbo" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-of-elbo">Interpretation of ELBO</h4>
<ul>
<li><p>식 (4)는 두 가지 직관적인 항으로 구성됩니다.</p></li>
<li><ol type="1">
<li><strong>Reconstruction Loss:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7Bq(Z%7CX%5Ek)%7D%20%5B%5Clog%20p(X%5Ek%7CZ)%5D"></li>
</ol>
<ul>
<li>잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">로부터 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 복원할 확률(Likelihood)을 최대화합니다.</li>
<li>Autoencoder의 복원 오차 최소화와 대응됩니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Regularization Term:</strong> <img src="https://latex.codecogs.com/png.latex?-D_%7B%5Ctext%7BKL%7D%7D(q(Z%7CX%5Ek)%20%7C%7C%20p(Z))"></li>
</ol>
<ul>
<li>우리가 근사한 사후 분포 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">가 사전 분포 <img src="https://latex.codecogs.com/png.latex?p(Z)">(일반적으로 표준 정규분포)와 얼마나 다른지를 측정합니다.</li>
<li>이 차이를 최소화(음수이므로 최대화)하여, 잠재 공간이 과도하게 찌그러지는 것을 방지합니다.</li>
</ul></li>
<li><p>결론적으로, 실제 로그 증거와 ELBO의 차이는 KL Divergence <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(q(Z%7CX)%20%7C%7C%20p(Z%7CX))%20%5Cge%200"> 만큼 발생하므로, <strong>ELBO를 최대화하는 것은 로그 증거의 하한(Lower Bound)을 최대화하는 것</strong>과 같습니다.</p></li>
</ul>
</section>
</section>
<section id="architecture-encoder-and-decoder" class="level3">
<h3 class="anchored" data-anchor-id="architecture-encoder-and-decoder">Architecture: Encoder and Decoder</h3>
<ul>
<li>VAE 프레임워크를 DAG-GNN에 적용하기 위해, Encoder와 Decoder를 구체적인 신경망 구조로 정의해야 합니다.</li>
</ul>
<section id="decoder-generative-model" class="level4">
<h4 class="anchored" data-anchor-id="decoder-generative-model">Decoder (Generative Model)</h4>
<ul>
<li>Decoder는 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">에서 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 생성하는 역할을 합니다.</li>
<li>이는 3.2절에서 정의한 <strong>Generalized SEM (식 3)</strong>과 정확히 일치합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BDecoder:%20%7D%20%5Cquad%20X%20=%20f_2%20%5Cleft(%20(I%20-%20A%5ET)%5E%7B-1%7D%20f_1(Z)%20%5Cright)%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7D"> 항은 <img src="https://latex.codecogs.com/png.latex?Z">의 정보가 그래프 구조를 따라 퍼져나가며(Propagation) <img src="https://latex.codecogs.com/png.latex?X">를 형성하는 과정을 담당합니다.</li>
</ul>
</section>
<section id="encoder-inference-model" class="level4">
<h4 class="anchored" data-anchor-id="encoder-inference-model">Encoder (Inference Model)</h4>
<ul>
<li>Encoder는 관측된 데이터 <img src="https://latex.codecogs.com/png.latex?X">로부터 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 추론하는 역할을 합니다.</li>
<li>저자들은 Decoder의 역연산 개념을 적용하여 다음과 같은 <strong>Encoder 구조</strong>를 제안합니다.</li>
</ul>
<p><span id="eq-(5)"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BEncoder:%20%7D%20%5Cquad%20Z%20=%20f_4%20%5Cleft(%20(I%20-%20A%5ET)%20f_3(X)%20%5Cright)%0A%5Ctag%7B5%7D"></span></p>
<ul>
<li>이 수식의 의미는 다음과 같습니다:
<ul>
<li><ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?f_3(X)">:</strong></li>
</ol>
<ul>
<li>데이터 <img src="https://latex.codecogs.com/png.latex?X">를 비선형 변환합니다. 개념적으로 Decoder의 <img src="https://latex.codecogs.com/png.latex?f_2">의 역함수 역할을 수행합니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?(I%20-%20A%5ET)">:</strong></li>
</ol>
<ul>
<li>Decoder에서는 역행렬 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7D">을 사용해 정보를 확산시켰다면, Encoder에서는 그 역연산인 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)">를 곱합니다.</li>
<li>이는 섞여 있는 정보들로부터 <strong>부모 노드의 영향을 제거</strong>하여 독립적인 노이즈(Latent factor)를 발라내는 과정으로 해석할 수 있습니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?f_4(%5Ccdot)">:</strong></li>
</ol>
<ul>
<li>최종적으로 비선형 변환을 통해 <img src="https://latex.codecogs.com/png.latex?Z"> 공간으로 매핑합니다. 개념적으로 Decoder의 <img src="https://latex.codecogs.com/png.latex?f_1">의 역함수 역할을 합니다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="parameterization" class="level4">
<h4 class="anchored" data-anchor-id="parameterization">Parameterization</h4>
<ul>
<li><strong>함수:</strong> <img src="https://latex.codecogs.com/png.latex?f_1,%20f_2"> (Decoder)와 <img src="https://latex.codecogs.com/png.latex?f_3,%20f_4"> (Encoder)는 모두 MLP(Multi-Layer Perceptron)로 파라미터화됩니다.</li>
<li><strong>분포:</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">와 <img src="https://latex.codecogs.com/png.latex?p(X%7CZ)">는 각각 가우시안 분포 등을 가정하며, 신경망은 이 분포의 평균(<img src="https://latex.codecogs.com/png.latex?%5Cmu">)과 분산(<img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">)을 출력하도록 설계됩니다.</li>
<li>구체적인 분포의 형태와 활성화 함수는 데이터 <img src="https://latex.codecogs.com/png.latex?X">의 타입(연속형 vs 이산형)에 따라 결정됩니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="architecture-and-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="architecture-and-loss-function">3.4. Architecture and Loss Function</h2>
<ul>
<li>이전 섹션에서 우리는 DAG 구조 학습을 위한 VAE 프레임워크를 정의했습니다.</li>
<li>이제 추상적인 수식(<img src="https://latex.codecogs.com/png.latex?f_1,%20f_2,%20f_3,%20f_4">)을 넘어, 실제로 모델을 어떻게 구현하고 학습시킬지 구체적인 <strong>Architecture</strong>와 <strong>Loss Function</strong>을 정의할 차례입니다.</li>
<li>이 과정에서 우리는 입력 데이터 <img src="https://latex.codecogs.com/png.latex?X">와 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">의 확률 분포를 가정하고, 이를 바탕으로 ELBO(Evidence Lower Bound)를 계산 가능한 수식으로 유도합니다.</li>
</ul>
<section id="distribution-specifications" class="level3">
<h3 class="anchored" data-anchor-id="distribution-specifications">Distribution Specifications</h3>
<ul>
<li>모델 학습을 위해서는 변수들의 확률 분포를 명시해야 합니다.</li>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Z">는 모두 <img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20d"> 차원의 행렬입니다 (<img src="https://latex.codecogs.com/png.latex?m">: 노드 수, <img src="https://latex.codecogs.com/png.latex?d">: 특징 차원).</li>
</ul>
<section id="prior-distribution-pz" class="level4">
<h4 class="anchored" data-anchor-id="prior-distribution-pz">Prior Distribution <img src="https://latex.codecogs.com/png.latex?p(Z)"></h4>
<ul>
<li>잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">의 사전 분포(Prior)는 가장 일반적인 가정인 <strong>Standard Matrix Normal</strong> 분포를 따릅니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap(Z)%20=%20%5Cmathcal%7BMN%7D_%7Bm%20%5Ctimes%20d%7D(0,%20I,%20I)%0A"></p>
<ul>
<li>이는 <img src="https://latex.codecogs.com/png.latex?Z">의 모든 원소 <img src="https://latex.codecogs.com/png.latex?Z_%7Bij%7D">가 평균이 0이고 분산이 1인 독립적인 정규분포(i.i.d. Gaussian)를 따른다는 것을 의미하며, 계산의 편의성을 위해 다음과 같이 요소별(element-wise) 분포로 취급할 수 있습니다. <img src="https://latex.codecogs.com/png.latex?p(Z_%7Bij%7D)%20=%20%5Cmathcal%7BN%7D(0,%201)"></li>
</ul>
</section>
</section>
<section id="encoder-architecture-inference-model" class="level3">
<h3 class="anchored" data-anchor-id="encoder-architecture-inference-model">Encoder Architecture (Inference Model)</h3>
<ul>
<li><p>Encoder는 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 입력받아 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">의 분포 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">를 추론합니다.</p></li>
<li><p><strong>가정:</strong> <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">는 <strong>Factored Gaussian</strong> (대각 공분산을 갖는 정규분포)을 따른다고 가정합니다.</p></li>
<li><p><strong>구성:</strong> 평균 행렬 <img src="https://latex.codecogs.com/png.latex?M_Z%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">와 표준편차 행렬 <img src="https://latex.codecogs.com/png.latex?S_Z%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">를 출력합니다.</p></li>
<li><p><strong>함수 매핑:</strong></p>
<ul>
<li>앞선 섹션의 <img src="https://latex.codecogs.com/png.latex?f_3"> (데이터 변환): <strong>MLP (Multi-Layer Perceptron)</strong></li>
<li>앞선 섹션의 <img src="https://latex.codecogs.com/png.latex?f_4"> (잠재 공간 매핑): <strong>Identity Mapping</strong></li>
</ul></li>
<li><p>이를 수식으로 표현하면 다음과 같습니다.</p></li>
</ul>
<p><span id="eq-(6)"><img src="https://latex.codecogs.com/png.latex?%0A%5BM_Z%20%7C%20%5Clog%20S_Z%5D%20=%20%5Cunderbrace%7B(I%20-%20A%5ET)%7D_%7B%5Ctext%7BStructure%20Removal%7D%7D%20%5Cunderbrace%7B%5Ctext%7BMLP%7D(X,%20W%5E1,%20W%5E2)%7D_%7B%5Ctext%7BFeature%20Transform%7D%7D%0A%5Ctag%7B6%7D"></span></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BMLP%7D(X,%20W%5E1,%20W%5E2)%20:=%20%5Ctext%7BReLU%7D(X%20W%5E1)%20W%5E2"> 입니다.</li>
</ul>
<section id="interpretation" class="level4">
<h4 class="anchored" data-anchor-id="interpretation">Interpretation</h4>
<ul>
<li>식 (6)을 보면 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)"> 연산이 MLP <strong>다음에</strong> 적용됩니다.</li>
<li>이는 MLP를 통해 데이터의 비선형 특징을 추출한 뒤, <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)"> 선형 변환을 통해 변수 간의 인과적 종속성(Parent effect)을 제거(Decorrelation)하여 독립적인 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 만들어내겠다는 의도입니다.</li>
</ul>
</section>
</section>
<section id="decoder-architecture-generative-model" class="level3">
<h3 class="anchored" data-anchor-id="decoder-architecture-generative-model">Decoder Architecture (Generative Model)</h3>
<ul>
<li><p>Decoder는 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">로부터 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 복원(Reconstruction)합니다.</p></li>
<li><p><strong>가정:</strong> <img src="https://latex.codecogs.com/png.latex?p(X%7CZ)"> 역시 <strong>Factored Gaussian</strong>을 따른다고 가정합니다.</p></li>
<li><p><strong>구성:</strong> 평균 행렬 <img src="https://latex.codecogs.com/png.latex?M_X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">와 표준편차 행렬 <img src="https://latex.codecogs.com/png.latex?S_X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">를 출력합니다.</p></li>
<li><p><strong>함수 매핑:</strong></p>
<ul>
<li>앞선 섹션의 <img src="https://latex.codecogs.com/png.latex?f_1"> (노이즈 변환): <strong>Identity Mapping</strong></li>
<li>앞선 섹션의 <img src="https://latex.codecogs.com/png.latex?f_2"> (데이터 복원): <strong>MLP</strong></li>
</ul></li>
<li><p>이를 수식으로 표현하면 다음과 같습니다.</p></li>
</ul>
<p><span id="eq-(7)"><img src="https://latex.codecogs.com/png.latex?%0A%5BM_X%20%7C%20%5Clog%20S_X%5D%20=%20%5Cunderbrace%7B%5Ctext%7BMLP%7D%7D_%7B%5Ctext%7BData%20Generation%7D%7D%20%5Cleft(%20%5Cunderbrace%7B(I%20-%20A%5ET)%5E%7B-1%7D%20Z%7D_%7B%5Ctext%7BStructure%20Propagation%7D%7D,%20W%5E3,%20W%5E4%20%5Cright)%0A%5Ctag%7B7%7D"></span></p>
<section id="interpretation-1" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-1">Interpretation</h4>
<ul>
<li>저자들은 <img src="https://latex.codecogs.com/png.latex?f_1">과 <img src="https://latex.codecogs.com/png.latex?f_2">의 위치를 바꾸어 실험해 보았으나, 현재의 설계(내부에 Identity, 외부에 MLP)가 성능이 더 좋았다고 보고합니다.</li>
<li>그 이유는 식 (7)의 설계가 <strong>Linear SEM의 구조적 변환 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7DZ">를 강조</strong>하기 때문입니다.</li>
<li>구조적 전파(Propagation)가 먼저 일어난 뒤 MLP가 비선형성을 입히는 방식이 비선형 데이터 생성 과정을 더 잘 포착한다는 것입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/dag_gnn_architecture_schematic.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: DAG-GNN의 전체 아키텍처 (Continuous Variables). 입력 <img src="https://latex.codecogs.com/png.latex?X">가 MLP와 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)">를 거쳐 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">의 통계량(<img src="https://latex.codecogs.com/png.latex?M_Z,%20S_Z">)으로 인코딩되고, 샘플링된 <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?(I-A%5ET)%5E%7B-1%7D">와 MLP를 거쳐 다시 <img src="https://latex.codecogs.com/png.latex?X">의 통계량(<img src="https://latex.codecogs.com/png.latex?M_X,%20S_X">)으로 디코딩된다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="loss-function-derivation-elbo" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-derivation-elbo">Loss Function Derivation (ELBO)</h3>
<ul>
<li>이제 정의된 분포(<img src="https://latex.codecogs.com/png.latex?p(Z),%20q(Z%7CX),%20p(X%7CZ)">)를 바탕으로, VAE의 목적 함수인 <strong>ELBO</strong>를 구체적인 수식으로 유도해 봅시다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7BELBO%7D%7D%20=%20-D_%7B%5Ctext%7BKL%7D%7D(q(Z%7CX)%20%7C%7C%20p(Z))%20+%20%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%5B%5Clog%20p(X%7CZ)%5D%0A"></p>
<section id="kl-divergence-term-regularization" class="level4">
<h4 class="anchored" data-anchor-id="kl-divergence-term-regularization">KL Divergence Term (Regularization)</h4>
<ul>
<li><p>이 항은 근사 분포 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">가 사전 분포 <img src="https://latex.codecogs.com/png.latex?p(Z)">와 얼마나 다른지를 측정합니다.</p></li>
<li><p>두 분포가 모두 가우시안일 경우, 복잡한 적분 없이도 파라미터(<img src="https://latex.codecogs.com/png.latex?M_Z,%20S_Z">)만으로 계산 가능한 <strong>Closed Form(닫힌 해)</strong>이 존재합니다.</p></li>
<li><p><strong>가정:</strong></p>
<ul>
<li><strong>Variational Posterior:</strong> <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)%20=%20%5Cmathcal%7BN%7D(M_Z,%20S_Z%5E2)"> (Factored Gaussian)</li>
<li><strong>Prior:</strong> <img src="https://latex.codecogs.com/png.latex?p(Z)%20=%20%5Cmathcal%7BN%7D(0,%20I)"> (Standard Normal)</li>
</ul></li>
<li><p>모든 변수가 독립(Independent)이므로, 단일 원소 <img src="https://latex.codecogs.com/png.latex?z%20%5Csim%20q(z)%20=%20%5Cmathcal%7BN%7D(%5Cmu,%20%5Csigma%5E2)">와 <img src="https://latex.codecogs.com/png.latex?p(z)%20=%20%5Cmathcal%7BN%7D(0,%201)"> 사이의 KL Divergence를 먼저 유도한 뒤 합산하면 됩니다.</p></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="상세 유도: 두 가우시안 사이의 KL Divergence">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>상세 유도: 두 가우시안 사이의 KL Divergence
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>1. 정의:</strong> <img src="https://latex.codecogs.com/png.latex?D_%7B%5Ctext%7BKL%7D%7D(q%7C%7Cp)%20=%20%5Cmathbb%7BE%7D_%7Bz%20%5Csim%20q%7D%20%5B%5Clog%20q(z)%20-%20%5Clog%20p(z)%5D"></p>
<p><strong>2. 로그 확률밀도함수 전개:</strong> <img src="https://latex.codecogs.com/png.latex?%5Clog%20q(z)%20=%20-%5Cfrac%7B1%7D%7B2%7D%5Clog(2%5Cpi)%20-%20%5Clog%5Csigma%20-%20%5Cfrac%7B(z-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D"> <img src="https://latex.codecogs.com/png.latex?%5Clog%20p(z)%20=%20-%5Cfrac%7B1%7D%7B2%7D%5Clog(2%5Cpi)%20-%20%5Cfrac%7Bz%5E2%7D%7B2%7D"></p>
<p><strong>3. 차이 계산:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Clog%20q(z)%20-%20%5Clog%20p(z)%20&amp;=%20%5Cleft(%20-%5Clog%5Csigma%20-%20%5Cfrac%7B(z-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%20%5Cright)%20-%20%5Cleft(%20-%20%5Cfrac%7Bz%5E2%7D%7B2%7D%20%5Cright)%20%5C%5C%0A&amp;=%20-%5Clog%5Csigma%20+%20%5Cfrac%7B1%7D%7B2%7Dz%5E2%20-%20%5Cfrac%7B(z-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p><strong>4. 기댓값(<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_q">) 취하기:</strong> <img src="https://latex.codecogs.com/png.latex?z%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu,%20%5Csigma%5E2)">일 때, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5Bz%5E2%5D%20=%20%5Cmu%5E2%20+%20%5Csigma%5E2"> 이고 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B(z-%5Cmu)%5E2%5D%20=%20%5Csigma%5E2"> 임을 이용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D_q%20%5B%5Cdots%5D%20&amp;=%20-%5Clog%5Csigma%20+%20%5Cfrac%7B1%7D%7B2%7D(%5Cmu%5E2%20+%20%5Csigma%5E2)%20-%20%5Cfrac%7B%5Csigma%5E2%7D%7B2%5Csigma%5E2%7D%20%5C%5C%0A&amp;=%20-%5Clog%5Csigma%20+%20%5Cfrac%7B1%7D%7B2%7D%5Cmu%5E2%20+%20%5Cfrac%7B1%7D%7B2%7D%5Csigma%5E2%20-%20%5Cfrac%7B1%7D%7B2%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20(%5Csigma%5E2%20+%20%5Cmu%5E2%20-%202%5Clog%5Csigma%20-%201)%0A%5Cend%7Baligned%7D%0A"></p>
</div>
</div>
</div>
<ul>
<li>위의 스칼라 유도 결과를 행렬 전체(<img src="https://latex.codecogs.com/png.latex?m%20%5Ctimes%20d">)에 대해 합산하면 다음과 같습니다.</li>
</ul>
<p><span id="eq-(8)"><img src="https://latex.codecogs.com/png.latex?%0AD_%7B%5Ctext%7BKL%7D%7D%5CBig(q(Z%7CX)%20%5C,%7C%7C%5C,%20p(Z)%5CBig)%20=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20%5Cleft(%20%5Cunderbrace%7B(S_Z)_%7Bij%7D%5E2%7D_%7B%5Csigma%5E2%7D%20+%20%5Cunderbrace%7B(M_Z)_%7Bij%7D%5E2%7D_%7B%5Cmu%5E2%7D%20-%20%5Cunderbrace%7B2%5Clog(S_Z)_%7Bij%7D%7D_%7B2%5Clog%5Csigma%7D%20-%201%20%5Cright)%0A%5Ctag%7B8%7D"></span></p>
<ul>
<li><strong>의의:</strong> 이 식은 적분(Sampling)이 필요 없으므로 계산이 매우 빠르고, 역전파(Backpropagation)를 통한 미분이 용이하여 안정적인 학습을 가능하게 합니다.</li>
<li><strong>역할:</strong> 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">가 평균 0, 분산 1인 분포에서 너무 멀어지지 않도록 강제하는 <strong>Regularizer</strong> 역할을 수행합니다.</li>
</ul>
</section>
<section id="reconstruction-term-likelihood" class="level4">
<h4 class="anchored" data-anchor-id="reconstruction-term-likelihood">Reconstruction Term (Likelihood)</h4>
<ul>
<li>두 번째 항은 모델이 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">로부터 관측 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 얼마나 잘 복원하는지를 나타내는 <strong>복원 오차(Reconstruction Error)</strong>입니다.</li>
<li>이 수식이 유도되는 과정은 <strong>Factored Gaussian 가정</strong>에 의해 다음과 같이 논리적으로 전개됩니다.</li>
</ul>
<section id="step-1-factored-gaussian-가정-행렬-to-스칼라-분해" class="level5">
<h5 class="anchored" data-anchor-id="step-1-factored-gaussian-가정-행렬-to-스칼라-분해"><strong>Step 1: Factored Gaussian 가정 (행렬 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 스칼라 분해)</strong></h5>
<ul>
<li><p>우리가 구해야 할 것은 전체 데이터 행렬 <img src="https://latex.codecogs.com/png.latex?X">에 대한 우도 <img src="https://latex.codecogs.com/png.latex?P(X%7CZ)">입니다.</p></li>
<li><p>앞서 우리는 <img src="https://latex.codecogs.com/png.latex?p(X%7CZ)">가 <strong>Factored Gaussian</strong>을 따른다고 가정했습니다.</p></li>
<li><p>이는 <img src="https://latex.codecogs.com/png.latex?Z">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?X">의 각 원소 <img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D">가 서로 <strong>조건부 독립(Conditionally Independent)</strong>임을 의미합니다.</p></li>
<li><p>따라서 결합 확률(Joint Probability)은 개별 스칼라 확률들의 곱으로 분해됩니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(X%7CZ)%20=%20%5Cprod_%7Bi=1%7D%5Em%20%5Cprod_%7Bj=1%7D%5Ed%20p(X_%7Bij%7D%20%7C%20Z)%0A"></p>
</section>
<section id="step-2-로그-변환과-덧셈으로의-전환-prod-to-sum" class="level5">
<h5 class="anchored" data-anchor-id="step-2-로그-변환과-덧셈으로의-전환-prod-to-sum"><strong>Step 2: 로그 변환과 덧셈으로의 전환 (<img src="https://latex.codecogs.com/png.latex?%5Cprod%20%5Cto%20%5Csum">)</strong></h5>
<ul>
<li>목적 함수는 <strong>로그 우도(Log-Likelihood)</strong>입니다. 양변에 로그를 취하면, 거대한 곱셈이 덧셈(Summation)으로 변환됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Clog%20P(X%7CZ)%20=%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20%5Clog%20p(X_%7Bij%7D%20%7C%20Z)%0A"></p>
<ul>
<li>이제 문제는 복잡한 행렬 연산에서 <strong>“개별 요소(<img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D">)의 스칼라 가우시안 로그 우도를 구해서 더하는 문제”</strong>로 단순화되었습니다.</li>
</ul>
</section>
<section id="step-3-스칼라-가우시안-로그-우도-계산" class="level5">
<h5 class="anchored" data-anchor-id="step-3-스칼라-가우시안-로그-우도-계산"><strong>Step 3: 스칼라 가우시안 로그 우도 계산</strong></h5>
<ul>
<li>단일 변수 <img src="https://latex.codecogs.com/png.latex?x">가 평균 <img src="https://latex.codecogs.com/png.latex?%5Cmu">, 표준편차 <img src="https://latex.codecogs.com/png.latex?%5Csigma">인 정규분포를 따를 때, 그 로그 확률밀도함수는 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Clog%20p(x%20%7C%20%5Cmu,%20%5Csigma)%20&amp;=%20%5Clog%20%5Cleft(%20%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%20e%5E%7B-%5Cfrac%7B(x-%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%7D%20%5Cright)%20%5C%5C%0A&amp;=%20%5Cunderbrace%7B-%5Clog(%5Csqrt%7B2%5Cpi%7D)%7D_%7B%5Ctext%7BConstant%20%7D%20c%7D%20-%20%5Clog%20%5Csigma%20-%20%5Cfrac%7B(x%20-%20%5Cmu)%5E2%7D%7B2%5Csigma%5E2%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>이 식을 위 Step 2의 합산 기호 안에 대입합니다.</li>
</ul>
</section>
<section id="step-4-몬테카를로-근사-monte-carlo-approximation" class="level5">
<h5 class="anchored" data-anchor-id="step-4-몬테카를로-근사-monte-carlo-approximation"><strong>Step 4: 몬테카를로 근사 (Monte Carlo Approximation)</strong></h5>
<ul>
<li><p>마지막으로 기댓값 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D">를 계산하기 위해, 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">를 <img src="https://latex.codecogs.com/png.latex?L">번 샘플링하여 그 평균으로 적분을 근사합니다.</p></li>
<li><p>이 모든 단계를 종합하면 식 (9)를 얻게 됩니다.</p></li>
</ul>
<p><span id="eq-(9)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5CBig%5B%20%5Clog%20p(X%7CZ)%20%5CBig%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BL%7D%20%5Csum_%7Bl=1%7D%5EL%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20%5Cleft(%20%5Cunderbrace%7B-%20%5Cfrac%7B(X_%7Bij%7D%20-%20(M_X%5E%7B(l)%7D)_%7Bij%7D)%5E2%7D%7B2(S_X%5E%7B(l)%7D)_%7Bij%7D%5E2%7D%7D_%7B%5Ctext%7BWeighted%20MSE%7D%7D%20%5Cunderbrace%7B-%20%5Clog(S_X%5E%7B(l)%7D)_%7Bij%7D%7D_%7B%5Ctext%7BUncertainty%20Penalty%7D%7D%20%5Cright)%20-%20c%0A%5Ctag%7B9%7D"></span></p>
<ul>
<li><strong>해석:</strong> 이 수식은 본질적으로 <strong>가중치(분산의 역수)가 적용된 MSE</strong>와, 모델이 불확실성(분산)을 무작정 키우는 것을 막는 <strong>Penalty(<img src="https://latex.codecogs.com/png.latex?%5Clog%20S_X">)</strong>의 합입니다.</li>
</ul>
</section>
</section>
</section>
<section id="a-note-on-latent-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="a-note-on-latent-dimensions">A Note on Latent Dimensions</h3>
<ul>
<li>Linear SEM에서는 <img src="https://latex.codecogs.com/png.latex?Z">를 단순한 “Noise”로 보기에 <img src="https://latex.codecogs.com/png.latex?X">와 차원이 같아야 했습니다.</li>
<li>하지만 VAE 프레임워크에서 <img src="https://latex.codecogs.com/png.latex?Z">는 <strong>Latent Factor</strong>로 해석됩니다.</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?Z">의 열(column) 차원을 <img src="https://latex.codecogs.com/png.latex?X">의 차원 <img src="https://latex.codecogs.com/png.latex?d">와 다르게 설정할 수 있습니다.</li>
<li>만약 데이터의 내재적 차원(Intrinsic Dimension)이 작다고 판단되면, <img src="https://latex.codecogs.com/png.latex?Z">의 차원을 줄여서 모델의 파라미터 수(<img src="https://latex.codecogs.com/png.latex?W%5E2,%20W%5E3">)를 줄이고 효율적인 표현을 학습할 수 있습니다.</li>
</ul>
<hr>
</section>
</section>
<section id="discrete-variables" class="level2">
<h2 class="anchored" data-anchor-id="discrete-variables">3.5. Discrete Variables</h2>
<ul>
<li><p>현실 세계의 인과관계 데이터는 키, 몸무게 같은 연속형(Continuous) 변수뿐만 아니라, 질병 유무, 성별, 등급과 같은 <strong>이산형(Discrete) 변수</strong>로 구성된 경우가 많습니다.</p></li>
<li><p>DAG-GNN의 가장 큰 장점 중 하나는 VAE(Variational Autoencoder) 프레임워크를 기반으로 하기 때문에, 데이터의 타입에 따라 <strong>우도(Likelihood) 분포만 적절히 교체</strong>해주면 자연스럽게 다양한 데이터 타입을 처리할 수 있다는 점입니다.</p></li>
<li><p>이번 포스트에서는 DAG-GNN이 이산형 변수를 어떻게 모델링하는지 그 수식적 변형 과정을 살펴보겠습니다.</p></li>
</ul>
<section id="data-representation-one-hot-encoding" class="level3">
<h3 class="anchored" data-anchor-id="data-representation-one-hot-encoding">Data Representation (One-Hot Encoding)</h3>
<ul>
<li><p>이산형 변수를 처리하기 위해 데이터 표현 방식부터 정의합니다.</p></li>
<li><p><strong>가정:</strong> 각 변수는 크기(Cardinality)가 <img src="https://latex.codecogs.com/png.latex?d">인 유한한 지지 집합(Finite support)을 가집니다.</p></li>
<li><p><strong>입력 <img src="https://latex.codecogs.com/png.latex?X">:</strong> <img src="https://latex.codecogs.com/png.latex?X">의 각 행(변수)은 <strong>One-Hot Vector</strong>로 표현됩니다.</p>
<ul>
<li>즉, “On” 위치(값이 1인 인덱스)가 해당 변수의 범주(Category)를 나타냅니다.</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D"> 차원을 유지합니다.</li>
</ul></li>
</ul>
</section>
<section id="encoder-and-prior-unchanged" class="level3">
<h3 class="anchored" data-anchor-id="encoder-and-prior-unchanged">Encoder and Prior (Unchanged)</h3>
<ul>
<li>이산형 데이터를 다룸에도 불구하고 <strong>Encoder(Inference Model)와 Prior는 연속형 모델과 동일</strong>하게 유지됩니다.</li>
<li><ol type="1">
<li><strong>Prior <img src="https://latex.codecogs.com/png.latex?p(Z)">:</strong> 여전히 Standard Matrix Normal <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BMN%7D(0,%20I,%20I)">을 따릅니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Posterior <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">:</strong> Factored Gaussian 분포를 가정합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Encoder 함수:</strong> 식 (6)의 구조를 그대로 사용합니다.</li>
</ol></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5BM_Z%20%7C%20%5Clog%20S_Z%5D%20=%20(I%20-%20A%5ET)%20%5Ctext%7BMLP%7D(X)%0A"></p>
<ul>
<li>이는 <strong>잠재 공간(Latent Space) <img src="https://latex.codecogs.com/png.latex?Z">는 여전히 연속적인 공간</strong>으로 남겨두고, 이 공간에서 그래프 구조 학습과 변분 추론을 수행하겠다는 의도입니다.</li>
</ul>
</section>
<section id="decoder-modification-categorical-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="decoder-modification-categorical-likelihood">Decoder Modification (Categorical Likelihood)</h3>
<ul>
<li>변화가 필요한 부분은 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">에서 다시 데이터 <img src="https://latex.codecogs.com/png.latex?X">를 복원하는 <strong>Decoder(Generative Model)</strong> 파트입니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?X">가 이산형이므로, 더 이상 가우시안 분포를 가정할 수 없습니다.</li>
</ul>
<section id="distribution-assumption" class="level4">
<h4 class="anchored" data-anchor-id="distribution-assumption">Distribution Assumption</h4>
<ul>
<li><p>우리는 우도 <img src="https://latex.codecogs.com/png.latex?p(X%7CZ)">를 <strong>Factored Categorical Distribution</strong>으로 가정합니다.</p></li>
<li><p><strong>출력:</strong> 확률 행렬 <img src="https://latex.codecogs.com/png.latex?P_X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D"></p></li>
<li><p>각 행(Row)은 해당 변수가 각 범주에 속할 확률을 나타내는 확률 벡터(Probability Vector)가 됩니다.</p></li>
</ul>
</section>
<section id="architecture-change" class="level4">
<h4 class="anchored" data-anchor-id="architecture-change">Architecture Change</h4>
<ul>
<li><p>이를 구현하기 위해, Decoder의 마지막 변환 함수 <img src="https://latex.codecogs.com/png.latex?f_2">를 <strong>Softmax</strong> 함수로 변경합니다.</p></li>
<li><p>기존 (Continuous): <img src="https://latex.codecogs.com/png.latex?f_2%20=%20%5Ctext%7BMLP%7D"> (Identity mapping for output range)</p></li>
<li><p><strong>변경 (Discrete):</strong> <img src="https://latex.codecogs.com/png.latex?f_2%20=%20%5Ctext%7Bsoftmax%7D(%5Ctext%7BMLP%7D)"></p></li>
<li><p>수식으로 표현하면 다음과 같습니다:</p></li>
</ul>
<p><span id="eq-(10)"><img src="https://latex.codecogs.com/png.latex?%0AP_X%20=%20%5Ctext%7Bsoftmax%7D%20%5Cleft(%20%5Ctext%7BMLP%7D%20%5Cbig(%20(I%20-%20A%5ET)%5E%7B-1%7D%20Z,%20W%5E3,%20W%5E4%20%5Cbig)%20%5Cright)%0A%5Ctag%7B10%7D"></span></p>
<ul>
<li>여기서 <code>softmax</code>는 각 행(Row-wise)에 대해 적용되어, 각 변수의 범주별 확률 합이 1이 되도록 만듭니다.</li>
</ul>
</section>
</section>
<section id="loss-function-modification-cross-entropy" class="level3">
<h3 class="anchored" data-anchor-id="loss-function-modification-cross-entropy">Loss Function Modification (Cross-Entropy)</h3>
<ul>
<li><p>목적 함수인 ELBO(Evidence Lower Bound)의 두 항 중, KL Divergence 항은 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)">와 <img src="https://latex.codecogs.com/png.latex?p(Z)">가 변하지 않았으므로 식 (8) 그대로 유지됩니다.</p></li>
<li><p>하지만 <strong>Reconstruction Term (Likelihood)</strong>은 가우시안 로그 우도(MSE 형태)에서 <strong>Categorical 로그 우도</strong>로 변경되어야 합니다. 이는 머신러닝에서 흔히 쓰이는 <strong>Cross-Entropy Loss</strong>와 형태가 같습니다.</p></li>
</ul>
<section id="derivation-1" class="level4">
<h4 class="anchored" data-anchor-id="derivation-1">Derivation</h4>
<ul>
<li>Categorical 분포의 로그 우도는 관측된 클래스(<img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D=1">)의 예측 확률(<img src="https://latex.codecogs.com/png.latex?P_%7BX_%7Bij%7D%7D">)에 로그를 취한 값입니다. 이를 몬테카를로 샘플링을 적용하여 정리하면 식 (11)을 얻습니다. <span id="eq-(11)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7Bq(Z%7CX)%7D%20%5CBig%5B%20%5Clog%20p(X%7CZ)%20%5CBig%5D%20%5Capprox%20%5Cfrac%7B1%7D%7BL%7D%20%5Csum_%7Bl=1%7D%5EL%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20X_%7Bij%7D%20%5Clog%20(P_X%5E%7B(l)%7D)_%7Bij%7D%0A%5Ctag%7B11%7D"></span>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?L">: 몬테카를로 샘플 개수</li>
<li><img src="https://latex.codecogs.com/png.latex?X_%7Bij%7D">: 실제 데이터의 One-hot 값 (0 또는 1)</li>
<li><img src="https://latex.codecogs.com/png.latex?(P_X%5E%7B(l)%7D)_%7Bij%7D">: Decoder가 예측한 <img src="https://latex.codecogs.com/png.latex?l">번째 샘플의 확률 값</li>
<li>이 식은 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?P_X"> 사이의 Cross-Entropy를 계산하여, 모델이 실제 데이터의 범주를 정확하게 예측하도록 학습시킵니다.</li>
</ul></li>
</ul>
</section>
</section>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>DAG-GNN은 데이터 타입에 따라 모델의 핵심 구조(Encoder, Graph Operations, Latent Space)를 변경할 필요 없이, <strong>Decoder의 출력층(Softmax)과 손실 함수(Cross-Entropy)</strong>만 유연하게 교체하여 이산형 변수를 처리합니다.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">구분</th>
<th style="text-align: left;">연속형 (Continuous)</th>
<th style="text-align: left;">이산형 (Discrete)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Input <img src="https://latex.codecogs.com/png.latex?X"></strong></td>
<td style="text-align: left;">Real Values (<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">)</td>
<td style="text-align: left;">One-hot Vectors (<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20d%7D">)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Prior / Encoder</strong></td>
<td style="text-align: left;">Gaussian / MLP</td>
<td style="text-align: left;">Gaussian / MLP (동일)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Likelihood</strong></td>
<td style="text-align: left;">Gaussian <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BN%7D(M_X,%20S_X)"></td>
<td style="text-align: left;">Categorical <img src="https://latex.codecogs.com/png.latex?P_X"></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Output Function (<img src="https://latex.codecogs.com/png.latex?f_2">)</strong></td>
<td style="text-align: left;">Identity / MLP</td>
<td style="text-align: left;">Softmax</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Reconstruction Loss</strong></td>
<td style="text-align: left;">Mean Squared Error (approx)</td>
<td style="text-align: left;">Cross Entropy</td>
</tr>
</tbody>
</table>
<ul>
<li>이러한 설계는 다양한 형태의 변수가 섞여 있는(Mixed type) 실제 데이터셋에도 쉽게 확장 적용할 수 있는 가능성을 보여줍니다.</li>
</ul>
<hr>
</section>
</section>
<section id="connection-to-linear-sem" class="level2">
<h2 class="anchored" data-anchor-id="connection-to-linear-sem">3.6. Connection to Linear SEM</h2>
<ul>
<li><p>지금까지 우리는 Linear SEM에서 출발하여 비선형성을 더하고(Non-linearity), VAE 프레임워크를 입혀(Probabilistic) DAG-GNN을 완성했습니다.</p></li>
<li><p>이제 저자들은 <strong>“역방향 사고(Reverse Thought Flow)”</strong>를 통해, DAG-GNN의 껍질을 하나씩 벗겨내면 결국 기존의 <strong>Linear SEM (Zheng et al., 2018, NOTEARS)</strong>과 수학적으로 완전히 일치함을 보입니다.</p></li>
<li><p>이 과정은 DAG-GNN이 근본 없는 블랙박스 모델이 아니라, 기존의 최적화 기반 구조 학습 이론을 <strong>확장(Extension)</strong>한 것임을 증명하는 중요한 이론적 토대가 됩니다.</p></li>
</ul>
<section id="step-1-from-vae-to-plain-autoencoder" class="level3">
<h3 class="anchored" data-anchor-id="step-1-from-vae-to-plain-autoencoder">Step 1: From VAE to Plain Autoencoder</h3>
<ul>
<li>가장 먼저, 확률적(Probabilistic) 모델인 VAE에서 변분(Variational) 요소를 제거하여 결정론적(Deterministic)인 <strong>Plain Autoencoder</strong>로 축소해 봅시다.</li>
</ul>
<section id="deterministic-setup" class="level4">
<h4 class="anchored" data-anchor-id="deterministic-setup">Deterministic Setup</h4>
<ul>
<li><p>확률 분포 <img src="https://latex.codecogs.com/png.latex?q(Z%7CX)"> 대신, 입력 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때 잠재 변수 <img src="https://latex.codecogs.com/png.latex?Z">가 고정된 값으로 결정된다고 가정합니다. 또한 비선형 함수 <img src="https://latex.codecogs.com/png.latex?f_1%20%5Cdots%20f_4">는 그대로 유지합니다.</p></li>
<li><p><strong>Encoder (식 5 기반):</strong> <img src="https://latex.codecogs.com/png.latex?Z%20=%20f_4((I%20-%20A%5ET)%20f_3(X))"></p></li>
<li><p><strong>Decoder (식 3 기반):</strong> <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D%20=%20f_2((I%20-%20A%5ET)%5E%7B-1%7D%20f_1(Z))"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">는 Decoder에 의해 복원된 값을 의미합니다.</li>
</ul></li>
</ul>
</section>
<section id="correspondence-of-loss-functions" class="level4">
<h4 class="anchored" data-anchor-id="correspondence-of-loss-functions">Correspondence of Loss Functions</h4>
<ul>
<li>일반적인 Autoencoder가 최소화하려는 손실 함수(Sample Loss)는 <strong>복원 오차(Reconstruction Error)</strong>와 <strong>잠재 변수 규제(Regularization)</strong>의 합으로 표현됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7BAE%7D%7D%20=%20%5Cunderbrace%7B%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20(X_%7Bij%7D%20-%20%5Chat%7BX%7D_%7Bij%7D)%5E2%7D_%7B%5Ctext%7BReconstruction%7D%7D%20+%20%5Cunderbrace%7B%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20Z_%7Bij%7D%5E2%7D_%7B%5Ctext%7BRegularization%7D%7D%0A"></p>
<ul>
<li><p>이 결정론적 손실 함수는 VAE의 <strong>ELBO</strong>와 정확히 대응됩니다:</p></li>
<li><ol type="1">
<li><strong>Reconstruction Term:</strong></li>
</ol>
<ul>
<li>ELBO의 복원 정확도 항(식 9)에서, <img src="https://latex.codecogs.com/png.latex?S_X"> (Decoder 분산)를 1로 고정하고 <img src="https://latex.codecogs.com/png.latex?M_X"> (Decoder 평균)를 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">로 두면, 로그 우도 최대화는 곧 <strong>MSE(Mean Squared Error) 최소화</strong>와 같아집니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Regularization Term:</strong></li>
</ol>
<ul>
<li>ELBO의 KL Divergence 항(식 8)에서, <img src="https://latex.codecogs.com/png.latex?S_Z"> (Encoder 분산)를 1로 고정하고 <img src="https://latex.codecogs.com/png.latex?M_Z"> (Encoder 평균)를 <img src="https://latex.codecogs.com/png.latex?Z">로 두면, KL 항은 <img src="https://latex.codecogs.com/png.latex?%5Csum%20Z_%7Bij%7D%5E2">에 비례하게 됩니다. 이는 <strong>L2 Regularization</strong>과 같습니다.</li>
</ul></li>
</ul>
</section>
</section>
<section id="step-2-from-nonlinear-to-linear-the-core-derivation" class="level3">
<h3 class="anchored" data-anchor-id="step-2-from-nonlinear-to-linear-the-core-derivation">Step 2: From Nonlinear to Linear (The Core Derivation)</h3>
<ul>
<li>이제 두 번째 단계로, 모델의 <strong>비선형성(Non-linearity)</strong>을 제거해 봅시다. 즉, 모든 활성화 함수와 MLP를 걷어냅니다.</li>
</ul>
<section id="linear-assumptions" class="level4">
<h4 class="anchored" data-anchor-id="linear-assumptions">Linear Assumptions</h4>
<ul>
<li><p>모든 매핑 함수 <img src="https://latex.codecogs.com/png.latex?f_1,%20f_2,%20f_3,%20f_4">를 <strong>항등 함수(Identity Mapping)</strong>로 가정합니다.</p></li>
<li><p>그렇다면 Encoder와 Decoder는 다음과 같이 단순한 선형 변환이 됩니다.</p></li>
<li><p><strong>Linear Encoder:</strong> <img src="https://latex.codecogs.com/png.latex?Z%20=%20(I%20-%20A%5ET)%20X"></p></li>
<li><p><strong>Linear Decoder:</strong> <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D%20=%20(I%20-%20A%5ET)%5E%7B-1%7D%20Z"></p></li>
</ul>
</section>
<section id="perfect-reconstruction" class="level4">
<h4 class="anchored" data-anchor-id="perfect-reconstruction">Perfect Reconstruction</h4>
<p>위 두 식을 결합하기 위해 Decoder 식의 <img src="https://latex.codecogs.com/png.latex?Z"> 자리에 Encoder 식을 대입합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Chat%7BX%7D%20&amp;=%20(I%20-%20A%5ET)%5E%7B-1%7D%20%5Cleft(%20(I%20-%20A%5ET)%20X%20%5Cright)%20%5C%5C%0A&amp;=%20%5Cunderbrace%7B(I%20-%20A%5ET)%5E%7B-1%7D%20(I%20-%20A%5ET)%7D_%7BI%7D%20X%20%5C%5C%0A&amp;=%20X%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>즉, 선형 모델 하에서는 입력 <img src="https://latex.codecogs.com/png.latex?X">가 손실 없이 완벽하게 복원(<img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D%20=%20X">)됩니다.</li>
<li>따라서 손실 함수의 첫 번째 항인 <strong>Reconstruction Error는 0</strong>이 되어 사라집니다.</li>
</ul>
</section>
</section>
<section id="deriving-the-notears-loss" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-notears-loss">Deriving the NOTEARS Loss</h3>
<ul>
<li>이제 남은 것은 두 번째 항인 <strong>Regularization Term</strong> 뿐입니다.</li>
<li>여기에 Linear Encoder 식 <img src="https://latex.codecogs.com/png.latex?Z%20=%20(I%20-%20A%5ET)X">를 대입하여 정리해 봅시다.</li>
</ul>
<p><span id="eq-(12)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathcal%7BL%7D_%7B%5Ctext%7BLinear%7D%7D%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5Csum_%7Bi=1%7D%5Em%20%5Csum_%7Bj=1%7D%5Ed%20Z_%7Bij%7D%5E2%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5C%7C%20Z%20%5C%7C_F%5E2%20%5Cquad%20(%5Ctext%7BFrobenius%20Norm%7D)%20%5C%5C%0A&amp;=%20%5Cfrac%7B1%7D%7B2%7D%20%5C%7C%20(I%20-%20A%5ET)%20X%20%5C%7C_F%5E2%0A%5Cend%7Baligned%7D%0A%5Ctag%7B12%7D"></span></p>
<section id="result-and-interpretation" class="level4">
<h4 class="anchored" data-anchor-id="result-and-interpretation">Result and Interpretation</h4>
<ul>
<li>유도된 최종 식 (12) <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5C%7C%20(I%20-%20A%5ET)%20X%20%5C%7C_F%5E2">는 정확히 <strong>Zheng et al.&nbsp;(2018)</strong>이 제안한 <strong>NOTEARS 알고리즘의 손실 함수(Least-squares loss)</strong>와 일치합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>의미 (Insight):</strong> * Linear SEM(NOTEARS)은 모델이 완벽하게 복원된다고 가정하고, 노이즈 <img src="https://latex.codecogs.com/png.latex?Z">의 크기(L2 norm)를 최소화하는 문제로 해석될 수 있습니다. * <strong>DAG-GNN</strong>은 이를 확장하여, <strong>“완벽한 복원이 불가능한(비선형/노이즈 존재) 상황”</strong>까지 고려하기 위해 Reconstruction Loss 항을 추가하고, 비선형 변환을 도입한 일반화된 모델입니다.</p>
</blockquote>
</section>
</section>
<section id="summary-1" class="level3">
<h3 class="anchored" data-anchor-id="summary-1">Summary</h3>
<ul>
<li><ol type="1">
<li><strong>DAG-GNN (VAE + Nonlinear)</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdownarrow"> (Variational 제거: <img src="https://latex.codecogs.com/png.latex?S_Z,%20S_X%20%5Cto%201">)</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Deterministic Autoencoder (MSE + L2 Reg)</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdownarrow"> (Nonlinearity 제거: <img src="https://latex.codecogs.com/png.latex?f%20%5Cto%20Identity">)</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>Linear Model (Perfect Reconstruction, Reg only)</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cdownarrow"> (<img src="https://latex.codecogs.com/png.latex?Z%20=%20(I-A%5ET)X"> 대입)</li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>Linear SEM Loss (Zheng et al., 2018)</strong></li>
</ol></li>
<li>이로써 DAG-GNN은 Linear SEM의 탄탄한 이론적 기반 위에 서 있으면서도, 딥러닝의 표현력을 통해 더 복잡한 데이터 분포를 학습할 수 있는 모델임이 증명되었습니다.</li>
</ul>
<hr>
</section>
</section>
<section id="acyclicity-constraint" class="level2">
<h2 class="anchored" data-anchor-id="acyclicity-constraint">3.7. Acyclicity Constraint</h2>
<ul>
<li><p>앞선 섹션들에서 우리는 VAE 기반의 손실 함수(ELBO)와 선형/비선형 모델링을 정의했습니다.</p></li>
<li><p>하지만 여기에는 치명적인 허점이 하나 있습니다.</p></li>
<li><p>ELBO를 최대화하든, Least-squares loss를 최소화하든, 학습된 인접 행렬 <img src="https://latex.codecogs.com/png.latex?A">가 <strong>DAG(비순환 그래프)</strong>라는 보장이 없다는 점입니다.</p></li>
<li><p>그래프 <img src="https://latex.codecogs.com/png.latex?G">가 인과관계 모델이 되기 위해서는 반드시 사이클(Cycle)이 없어야 합니다.</p></li>
<li><p>이번 포스트에서는 이 조합적(Combinatorial) 제약 조건을 어떻게 연속적인(Continuous) 수식으로 변환하여 최적화 과정에 통합했는지 살펴봅니다.</p></li>
</ul>
<section id="motivation-trace-and-cycles" class="level3">
<h3 class="anchored" data-anchor-id="motivation-trace-and-cycles">Motivation: Trace and Cycles</h3>
<ul>
<li>그래프 이론에서 인접 행렬의 거듭제곱은 경로(Path)와 깊은 연관이 있습니다.</li>
</ul>
<section id="path-counting-logic" class="level4">
<h4 class="anchored" data-anchor-id="path-counting-logic">Path Counting Logic</h4>
<ul>
<li><p>가중치가 있는 인접 행렬 <img src="https://latex.codecogs.com/png.latex?A">에 대해, 요소별 제곱(Element-wise square)을 수행하여 비음수(Non-negative) 행렬 <img src="https://latex.codecogs.com/png.latex?B">를 정의해 봅시다 (<img src="https://latex.codecogs.com/png.latex?B%20=%20A%20%5Ccirc%20A">).</p></li>
<li><p>행렬 <img src="https://latex.codecogs.com/png.latex?B">의 <img src="https://latex.codecogs.com/png.latex?(i,%20j)"> 요소가 양수라면, 노드 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 가는 엣지가 존재함을 의미합니다.</p></li>
<li><p>행렬의 곱셈 성질에 따라, <img src="https://latex.codecogs.com/png.latex?B%5Ek">의 <img src="https://latex.codecogs.com/png.latex?(i,%20j)"> 요소가 양수라는 것은 <strong>노드 <img src="https://latex.codecogs.com/png.latex?i">에서 <img src="https://latex.codecogs.com/png.latex?j">로 가는 길이가 <img src="https://latex.codecogs.com/png.latex?k">인 경로가 존재함</strong>을 의미합니다.</p></li>
</ul>
</section>
<section id="detecting-cycles" class="level4">
<h4 class="anchored" data-anchor-id="detecting-cycles">Detecting Cycles</h4>
<ul>
<li><p>사이클이란 무엇일까요? 바로 <strong>자기 자신으로 돌아오는 경로(<img src="https://latex.codecogs.com/png.latex?i%20%5Cto%20%5Cdots%20%5Cto%20i">)</strong>입니다.</p></li>
<li><p>따라서, 어떤 정수 <img src="https://latex.codecogs.com/png.latex?k">에 대해 <img src="https://latex.codecogs.com/png.latex?B%5Ek">의 대각 성분(Diagonal element) <img src="https://latex.codecogs.com/png.latex?(B%5Ek)_%7Bii%7D">가 양수라면, 노드 <img src="https://latex.codecogs.com/png.latex?i">를 포함하는 길이 <img src="https://latex.codecogs.com/png.latex?k">의 사이클이 존재한다는 뜻입니다.</p></li>
<li><p>이 논리를 확장하면 다음과 같은 결론에 도달합니다.</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>“모든 <img src="https://latex.codecogs.com/png.latex?k%20%3E%200">에 대해 <img src="https://latex.codecogs.com/png.latex?B%5Ek">의 대각 성분이 모두 0이라면(즉, Trace가 0이라면), 그 그래프는 DAG이다.”</strong></p>
</blockquote>
</section>
</section>
<section id="the-matrix-exponential-previous-work" class="level3">
<h3 class="anchored" data-anchor-id="the-matrix-exponential-previous-work">The Matrix Exponential (Previous Work)</h3>
<ul>
<li><strong>Zheng et al.&nbsp;(2018)</strong>의 NOTEARS 알고리즘은 이 원리를 이용하여 <strong>행렬 지수(Matrix Exponential)</strong> 형태의 제약 조건을 제안했습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ah(A)%20=%20%5Ctext%7Btr%7D(e%5E%7BA%20%5Ccirc%20A%7D)%20-%20m%20=%200%0A"></p>
<ul>
<li>이 수식은 테일러 급수 전개를 통해 이해할 수 있습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ae%5EB%20=%20I%20+%20B%20+%20%5Cfrac%7BB%5E2%7D%7B2!%7D%20+%20%5Cfrac%7BB%5E3%7D%7B3!%7D%20+%20%5Cdots%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?B">의 모든 거듭제곱(<img src="https://latex.codecogs.com/png.latex?B%5Ek">)의 합을 포함하므로, 어떤 길이의 사이클이라도 존재한다면 <img src="https://latex.codecogs.com/png.latex?e%5EB">의 대각 성분 합(Trace)은 <img src="https://latex.codecogs.com/png.latex?m"> (항등 행렬 <img src="https://latex.codecogs.com/png.latex?I">의 Trace)보다 커지게 됩니다.</li>
<li>수학적으로 매우 우아(Elegant)하지만, 실제 딥러닝 프레임워크에서 구현할 때 두 가지 문제가 있습니다.
<ol type="1">
<li><strong>자동 미분 지원 미비:</strong> 모든 플랫폼이 행렬 지수의 미분을 효율적으로 지원하지 않습니다.</li>
<li><strong>수치적 불안정성:</strong> <img src="https://latex.codecogs.com/png.latex?e%5EB">는 값이 매우 빠르게 커지므로, 고유값(Eigenvalue)이 클 경우 오버플로우나 수치 오류가 발생하기 쉽습니다.</li>
</ol></li>
</ul>
</section>
<section id="proposed-solution-polynomial-constraint" class="level3">
<h3 class="anchored" data-anchor-id="proposed-solution-polynomial-constraint">Proposed Solution: Polynomial Constraint</h3>
<ul>
<li>저자들은 위 문제를 해결하기 위해, 행렬 지수 대신 <strong>다항식(Polynomial)</strong> 형태의 새로운 제약 조건을 제안합니다.</li>
</ul>
<section id="theorem-1-polynomial-acyclicity" class="level4">
<h4 class="anchored" data-anchor-id="theorem-1-polynomial-acyclicity">Theorem 1 (Polynomial Acyclicity)</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20m%7D">를 유향 그래프의 가중치 인접 행렬이라고 합시다.</li>
<li>임의의 양수 <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%3E%200">에 대하여, 다음 조건이 성립하면 그래프는 Acyclic입니다.</li>
</ul>
<p><span id="eq-(13)"><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Btr%7D%5Cleft%5B%20(I%20+%20%5Calpha%20A%20%5Ccirc%20A)%5Em%20%5Cright%5D%20-%20m%20=%200%0A%5Ctag%7B13%7D"></span></p>
</section>
<section id="derivation-proof-logic" class="level4">
<h4 class="anchored" data-anchor-id="derivation-proof-logic">Derivation &amp; Proof Logic</h4>
<ul>
<li><p>이 식이 성립하는 이유는 다음과 같습니다.</p></li>
<li><ol type="1">
<li><strong>최장 경로의 길이:</strong> 노드가 <img src="https://latex.codecogs.com/png.latex?m">개인 그래프에서 사이클이 없다면(DAG라면), 존재할 수 있는 경로의 최대 길이는 <img src="https://latex.codecogs.com/png.latex?m-1">입니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>이항 전개 (Binomial Expansion):</strong> <img src="https://latex.codecogs.com/png.latex?(I%20+%20%5Calpha%20B)%5Em">을 전개하면 다음과 같은 형태가 됩니다. <img src="https://latex.codecogs.com/png.latex?I%20+%20%5Cbinom%7Bm%7D%7B1%7D%5Calpha%20B%20+%20%5Cbinom%7Bm%7D%7B2%7D%5Calpha%5E2%20B%5E2%20+%20%5Cdots%20+%20%5Calpha%5Em%20B%5Em"></li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>포괄성:</strong> 이 식은 <img src="https://latex.codecogs.com/png.latex?I">부터 <img src="https://latex.codecogs.com/png.latex?B%5Em">까지의 모든 항을 양수 계수로 포함합니다.</li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>결론:</strong> 만약 그래프에 사이클이 있다면, <img src="https://latex.codecogs.com/png.latex?m"> 이하의 어떤 길이 <img src="https://latex.codecogs.com/png.latex?k">에 대해 <img src="https://latex.codecogs.com/png.latex?B%5Ek">의 Trace가 양수가 될 것입니다. 위 식은 <img src="https://latex.codecogs.com/png.latex?B%5E1">부터 <img src="https://latex.codecogs.com/png.latex?B%5Em">까지 모든 거듭제곱의 합을 검사하므로, 사이클이 하나라도 있다면 전체 Trace는 <img src="https://latex.codecogs.com/png.latex?m"> (<img src="https://latex.codecogs.com/png.latex?I">의 Trace)보다 반드시 커지게 됩니다.</li>
</ol></li>
<li><p>따라서 식 (13)을 0으로 만드는 제약 조건은 그래프가 DAG임을 보장합니다.</p></li>
</ul>
</section>
</section>
<section id="stability-analysis-why-polynomial" class="level3">
<h3 class="anchored" data-anchor-id="stability-analysis-why-polynomial">Stability Analysis (Why Polynomial?)</h3>
<ul>
<li>왜 <img src="https://latex.codecogs.com/png.latex?e%5EB"> 대신 <img src="https://latex.codecogs.com/png.latex?(I%20+%20%5Calpha%20B)%5Em">을 써야 할까요? 저자들은 <strong>Theorem 2</strong>를 통해 수치적 안정성을 증명합니다.</li>
</ul>
<section id="theorem-2-comparison" class="level4">
<h4 class="anchored" data-anchor-id="theorem-2-comparison">Theorem 2 (Comparison)</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20c/m%20%3E%200"> (단, <img src="https://latex.codecogs.com/png.latex?c">는 상수)라고 설정하면, 임의의 복소수 <img src="https://latex.codecogs.com/png.latex?%5Clambda">에 대해 다음 부등식이 성립합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A(1%20+%20%5Calpha%20%7C%5Clambda%7C)%5Em%20%5Cle%20e%5E%7Bc%7C%5Clambda%7C%7D%0A"></p>
</section>
<section id="interpretation-2" class="level4">
<h4 class="anchored" data-anchor-id="interpretation-2">Interpretation</h4>
<ul>
<li><strong>좌변:</strong> 제안된 다항식 제약 조건의 성장 속도와 관련됨.</li>
<li><strong>우변:</strong> 기존 행렬 지수 제약 조건의 성장 속도와 관련됨.</li>
<li>이 부등식은 <strong>다항식 제약 조건이 지수 함수보다 훨씬 완만하게 증가함</strong>을 보여줍니다.</li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?B">의 고유값(Eigenvalue) 크기가 클 때, 다항식 기반 제약 조건이 수치적 폭발(Numerical difficulty)을 겪을 위험이 훨씬 적습니다 (“less severe”).</li>
</ul>
</section>
<section id="practical-implementation" class="level4">
<h4 class="anchored" data-anchor-id="practical-implementation">Practical Implementation</h4>
<ul>
<li>실제 구현에서 <img src="https://latex.codecogs.com/png.latex?%5Calpha">는 하이퍼파라미터로 취급됩니다.</li>
<li>이론적으로 <img src="https://latex.codecogs.com/png.latex?%5Calpha">는 <img src="https://latex.codecogs.com/png.latex?B">의 가장 큰 고유값(Spectral radius)에 의존합니다.</li>
<li>Perron-Frobenius 정리에 따라, 비음수 행렬 <img src="https://latex.codecogs.com/png.latex?B">의 Spectral radius는 최대 행 합(Maximum row sum)에 의해 제한(Bounded)되므로, 이를 참고하여 <img src="https://latex.codecogs.com/png.latex?%5Calpha">를 설정할 수 있습니다.</li>
</ul>
</section>
</section>
<section id="summary-2" class="level3">
<h3 class="anchored" data-anchor-id="summary-2">Summary</h3>
<ul>
<li><p>DAG-GNN은 구조 학습의 핵심인 Acyclicity Constraint를 현대적인 딥러닝 환경에 맞게 재설계했습니다.</p></li>
<li><ol type="1">
<li><strong>기존:</strong> <img src="https://latex.codecogs.com/png.latex?h(A)%20=%20%5Ctext%7Btr%7D(e%5E%7BA%20%5Ccirc%20A%7D)%20-%20m%20=%200"> (NOTEARS)</li>
</ol>
<ul>
<li>우아하지만 구현이 어렵고 불안정할 수 있음.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>제안:</strong> <img src="https://latex.codecogs.com/png.latex?h(A)%20=%20%5Ctext%7Btr%7D((I%20+%20%5Calpha%20A%20%5Ccirc%20A)%5Em)%20-%20m%20=%200"> (DAG-GNN)</li>
</ol>
<ul>
<li><strong>Finite Power:</strong> <img src="https://latex.codecogs.com/png.latex?m">차수까지만 검사해도 충분함 (DAG의 성질).</li>
<li><strong>Stability:</strong> 지수 함수보다 완만하게 증가하여 수치적으로 안정적.</li>
<li><strong>Convenience:</strong> 일반적인 행렬 곱셈만으로 구현 가능하여 모든 딥러닝 프레임워크와 호환됨.</li>
</ul></li>
<li><p>이로써 DAG-GNN은 VAE를 통한 확률적 모델링, GNN을 통한 비선형성 확보, 그리고 Polynomial Constraint를 통한 구조적 보장까지 갖춘 완전한 프레임워크가 되었습니다.</p></li>
</ul>
<hr>
</section>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">3.8. Training</h2>
<ul>
<li>지금까지 우리는 DAG-GNN의 두 가지 핵심 기둥을 세웠습니다.
<ul>
<li><ol type="1">
<li><strong>Objective:</strong> 데이터를 잘 설명하기 위한 VAE의 손실 함수 (Negative ELBO).</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Constraint:</strong> 그래프가 DAG임을 보장하기 위한 다항식 제약 조건 (<img src="https://latex.codecogs.com/png.latex?h(A)%20=%200">).</li>
</ol></li>
</ul></li>
<li>이제 이 두 가지를 하나로 묶어 실제 학습을 수행하는 <strong>최적화 전략(Optimization Strategy)</strong>을 다룰 차례입니다.</li>
<li>이 문제는 전형적인 <strong>비선형 등식 제약 최적화(Nonlinear Equality-Constrained Optimization)</strong> 문제입니다.</li>
</ul>
<section id="problem-formulation" class="level3">
<h3 class="anchored" data-anchor-id="problem-formulation">Problem Formulation</h3>
<ul>
<li>전체 학습 문제는 다음과 같은 최적화 문제로 정식화됩니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmin_%7BA,%20%5Ctheta%7D%20%5Cquad%20&amp;%20f(A,%20%5Ctheta)%20%5Cequiv%20-L_%7B%5Ctext%7BELBO%7D%7D%20%5C%5C%0A%5Ctext%7Bs.t.%7D%20%5Cquad%20&amp;%20h(A)%20%5Cequiv%20%5Ctext%7Btr%7D%5B(I%20+%20%5Calpha%20A%20%5Ccirc%20A)%5Em%5D%20-%20m%20=%200%0A%5Cend%7Baligned%7D%0A">
<ul>
<li><strong>Objective <img src="https://latex.codecogs.com/png.latex?f(A,%20%5Ctheta)">:</strong> ELBO(Evidence Lower Bound)를 최대화하는 것은 Negative ELBO를 최소화하는 것과 같습니다.</li>
<li><strong>Constraint <img src="https://latex.codecogs.com/png.latex?h(A)">:</strong> 앞서 유도한 다항식 제약 조건으로, 이 값이 0이 되어야만 <img src="https://latex.codecogs.com/png.latex?A">가 DAG임이 보장됩니다.</li>
<li><strong>Unknowns:</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A">: 가중치 인접 행렬 (Weighted Adjacency Matrix)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Ctheta">: VAE를 구성하는 신경망의 파라미터들 (<img src="https://latex.codecogs.com/png.latex?%5C%7BW%5E1,%20W%5E2,%20W%5E3,%20W%5E4%5C%7D">).</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="the-augmented-lagrangian-method" class="level3">
<h3 class="anchored" data-anchor-id="the-augmented-lagrangian-method">The Augmented Lagrangian Method</h3>
<ul>
<li><p>이러한 제약 조건이 있는 최적화 문제를 풀기 위해, 저자들은 <strong>증강 라그랑주(Augmented Lagrangian)</strong> 방법을 사용합니다.</p></li>
<li><p>이는 표준적인 라그랑주 승수법에 페널티 항(Penalty term)을 추가하여 수치적 안정성과 수렴성을 높인 기법입니다 (Bertsekas, 1999).</p></li>
<li><p>정의된 증강 라그랑주 함수 <img src="https://latex.codecogs.com/png.latex?L_c">는 다음과 같습니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AL_c(A,%20%5Ctheta,%20%5Clambda)%20=%20f(A,%20%5Ctheta)%20+%20%5Clambda%20h(A)%20+%20%5Cfrac%7Bc%7D%7B2%7D%7Ch(A)%7C%5E2%0A"></p>
<ul>
<li>이 식은 세 가지 부분으로 구성됩니다:
<ul>
<li><ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?f(A,%20%5Ctheta)">:</strong> 원래의 목적 함수 (Negative ELBO).</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Clambda%20h(A)">:</strong> 표준 라그랑주 항. 여기서 <img src="https://latex.codecogs.com/png.latex?%5Clambda">는 라그랑주 승수(Lagrange Multiplier)입니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bc%7D%7B2%7D%7Ch(A)%7C%5E2">:</strong> 제약 조건 위반에 대한 2차 페널티(Quadratic Penalty) 항입니다. <img src="https://latex.codecogs.com/png.latex?c">는 페널티 파라미터(Penalty Parameter)입니다.</li>
</ol></li>
</ul></li>
</ul>
<section id="motivation" class="level4">
<h4 class="anchored" data-anchor-id="motivation">Motivation</h4>
<ul>
<li>왜 단순 라그랑주나 단순 페널티 기법을 쓰지 않고 이 둘을 섞었을까요?</li>
<li>단순 페널티 기법은 <img src="https://latex.codecogs.com/png.latex?c">를 무한대로 보내야만 제약 조건을 만족하는데, 이는 해 주변에서 함수를 매우 뾰족하게(Ill-conditioned) 만들어 최적화를 어렵게 합니다.</li>
<li>증강 라그랑주 방법은 <img src="https://latex.codecogs.com/png.latex?%5Clambda">의 도움을 받아, <img src="https://latex.codecogs.com/png.latex?c">가 적당히 크더라도 정확한 해로 수렴할 수 있게 해줍니다. 즉, <strong><img src="https://latex.codecogs.com/png.latex?c%20%5Cto%20%5Cinfty">일 때 <img src="https://latex.codecogs.com/png.latex?L_c">의 최소해는 제약 조건 <img src="https://latex.codecogs.com/png.latex?h(A)=0">을 만족하며 원래 목적 함수 <img src="https://latex.codecogs.com/png.latex?f">를 최소화</strong>하게 됩니다.</li>
</ul>
</section>
</section>
<section id="algorithm-iterative-update-rule" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-iterative-update-rule">Algorithm: Iterative Update Rule</h3>
<ul>
<li><p>학습은 <strong><img src="https://latex.codecogs.com/png.latex?c">를 점진적으로 증가</strong>시키면서 일련의 비제약 최적화 문제(Unconstrained optimization)를 푸는 방식으로 진행됩니다.</p></li>
<li><p>구체적인 알고리즘은 다음의 반복(Iteration) 과정으로 요약됩니다.</p></li>
<li><p>각 반복 단계 <img src="https://latex.codecogs.com/png.latex?k">에서 다음을 수행합니다:</p></li>
</ul>
<section id="step-1-primal-update-subproblem" class="level4">
<h4 class="anchored" data-anchor-id="step-1-primal-update-subproblem">Step 1: Primal Update (Subproblem)</h4>
<ul>
<li>현재 고정된 <img src="https://latex.codecogs.com/png.latex?%5Clambda%5Ek">와 <img src="https://latex.codecogs.com/png.latex?c%5Ek">에 대해, 증강 라그랑주 함수 <img src="https://latex.codecogs.com/png.latex?L_%7Bc%5Ek%7D">를 최소화하는 <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?%5Ctheta">를 찾습니다.</li>
</ul>
<p><span id="eq-(14)"><img src="https://latex.codecogs.com/png.latex?%0A(A%5Ek,%20%5Ctheta%5Ek)%20=%20%5Cunderset%7BA,%20%5Ctheta%7D%7B%5Ctext%7Bargmin%7D%7D%20%5C%20L_%7Bc%5Ek%7D(A,%20%5Ctheta,%20%5Clambda%5Ek)%0A%5Ctag%7B14%7D"></span></p>
<ul>
<li>이 단계(Subproblem)는 경사 하강법(Gradient Descent)과 같은 Blackbox Stochastic Optimization Solver(예: Adam)를 사용하여 해결합니다.</li>
<li>ELBO가 샘플 기반으로 정의되므로 확률적(Stochastic) 최적화가 적합합니다.</li>
</ul>
</section>
<section id="step-2-dual-update-lambda" class="level4">
<h4 class="anchored" data-anchor-id="step-2-dual-update-lambda">Step 2: Dual Update (<img src="https://latex.codecogs.com/png.latex?%5Clambda">)</h4>
<ul>
<li>제약 조건 위반 정도(<img src="https://latex.codecogs.com/png.latex?h(A%5Ek)">)를 반영하여 라그랑주 승수를 업데이트합니다.</li>
</ul>
<p><span id="eq-(15)"><img src="https://latex.codecogs.com/png.latex?%0A%5Clambda%5E%7Bk+1%7D%20=%20%5Clambda%5Ek%20+%20c%5Ek%20h(A%5Ek)%0A%5Ctag%7B15%7D"></span></p>
<ul>
<li>이 규칙은 쌍대 오름법(Dual Ascent)의 형태를 띠며, 제약 조건이 만족되지 않으면(즉 <img src="https://latex.codecogs.com/png.latex?h(A)%20%5Cneq%200">), <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 조정하여 다음 단계에서 해당 제약 조건이 더 중요하게 다뤄지도록 합니다.</li>
</ul>
</section>
<section id="step-3-penalty-parameter-update-c" class="level4">
<h4 class="anchored" data-anchor-id="step-3-penalty-parameter-update-c">Step 3: Penalty Parameter Update (<img src="https://latex.codecogs.com/png.latex?c">)</h4>
<ul>
<li>제약 조건 위반 정도가 충분히 줄어들지 않았다면, 페널티 강도 <img src="https://latex.codecogs.com/png.latex?c">를 증가시킵니다.</li>
</ul>
<p><span id="eq-(16)"><img src="https://latex.codecogs.com/png.latex?%0Ac%5E%7Bk+1%7D%20=%20%5Cbegin%7Bcases%7D%0A%5Ceta%20c%5Ek,%20&amp;%20%5Ctext%7Bif%20%7D%20%7Ch(A%5Ek)%7C%20%3E%20%5Cgamma%20%7Ch(A%5E%7Bk-1%7D)%7C%20%5C%5C%0Ac%5Ek,%20&amp;%20%5Ctext%7Botherwise%7D%0A%5Cend%7Bcases%7D%0A%5Ctag%7B16%7D"></span></p>
<ul>
<li><strong>조건 (<img src="https://latex.codecogs.com/png.latex?%7Ch(A%5Ek)%7C%20%3E%20%5Cgamma%20%7Ch(A%5E%7Bk-1%7D)%7C">):</strong> 이번 단계의 제약 위반 값이 이전 단계의 <img src="https://latex.codecogs.com/png.latex?%5Cgamma">배보다 크다는 것은, 위반 정도가 충분히(빠르게) 감소하지 않았음을 의미합니다.</li>
<li><strong>대응 (<img src="https://latex.codecogs.com/png.latex?c%20%5Cleftarrow%20%5Ceta%20c">):</strong> 이 경우 <img src="https://latex.codecogs.com/png.latex?c">를 <img src="https://latex.codecogs.com/png.latex?%5Ceta">배 키워서 제약 조건 위반에 대한 비용을 더 비싸게 만듭니다.</li>
</ul>
</section>
</section>
<section id="hyperparameters-and-implementation" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters-and-implementation">Hyperparameters and Implementation</h3>
<ul>
<li>논문에서는 이 알고리즘의 효과적인 작동을 위한 하이퍼파라미터 설정 값을 제안합니다.
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Ceta%20%3E%201">:</strong> 페널티 증가 비율. 논문에서는 <strong><img src="https://latex.codecogs.com/png.latex?%5Ceta%20=%2010"></strong>을 권장합니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cgamma%20%3C%201">:</strong> 허용 가능한 위반 감소율. 논문에서는 <strong><img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%201/4"></strong> (<img src="https://latex.codecogs.com/png.latex?0.25">)를 권장합니다.</li>
</ul></li>
<li>이 설정은 제약 조건 위반이 매 단계마다 최소 25%씩은 줄어들기를 기대하며, 그렇지 않을 경우 페널티를 10배로 강력하게 키우겠다는 공격적인 전략을 의미합니다.</li>
</ul>
</section>
<section id="summary-3" class="level3">
<h3 class="anchored" data-anchor-id="summary-3">Summary</h3>
<ul>
<li>DAG-GNN의 학습 과정은 단순히 손실 함수를 미분하여 역전파하는 것을 넘어섭니다.
<ul>
<li><ol type="1">
<li><strong>Augmented Lagrangian</strong>을 통해 연속적인 제약 조건(Acyclicity)을 목적 함수에 부드럽게 통합했습니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Primal-Dual Update</strong> 방식을 통해 모델 파라미터(<img src="https://latex.codecogs.com/png.latex?A,%20%5Ctheta">)와 제약 파라미터(<img src="https://latex.codecogs.com/png.latex?%5Clambda,%20c">)를 번갈아 최적화하며, 점진적으로 DAG 구조를 만족하는 해로 수렴해 나갑니다.</li>
</ol></li>
</ul></li>
<li>이로써 우리는 복잡한 조합 최적화 문제였던 구조 학습을, 딥러닝 프레임워크 위에서 수행 가능한 연속 최적화 문제로 완벽하게 변환하였습니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="experiments" class="level1">
<h1>4. Experiments</h1>
<ul>
<li>본 섹션에서는 제안된 <strong>DAG-GNN</strong> 모델의 성능을 다양한 합성 데이터셋(Synthetic Data)과 벤치마크 데이터셋(Benchmark Data)을 통해 검증합니다.</li>
<li><ul>
<li>비교 대상으로는 당시 SOTA였던 연속 최적화 기반의 <strong>DAG-NOTEARS (Zheng et al., 2018)</strong>를 주로 사용합니다.</li>
</ul></li>
<li>실험의 목표는 DAG-GNN이 <strong>선형성(Linearity)</strong> 가정에 국한되지 않고, <strong>비선형(Nonlinear)</strong> 관계나 <strong>이산형(Discrete)</strong> 변수, 그리고 <strong>벡터 값 노드(Vector-valued node)</strong>까지 얼마나 유연하게 처리할 수 있는지 보여주는 데 있습니다.</li>
</ul>
<hr>
<section id="synthetic-data-sets" class="level2">
<h2 class="anchored" data-anchor-id="synthetic-data-sets">4.1. Synthetic Data Sets</h2>
<ul>
<li><p>저자들은 Erdős-Rényi 모델을 사용하여 임의의 DAG 구조를 생성하고, 노드 수 <img src="https://latex.codecogs.com/png.latex?m%20%5Cin%20%5C%7B10,%2020,%2050,%20100%5C%7D">에 대해 데이터를 생성하여 실험을 진행했습니다. (샘플 수 <img src="https://latex.codecogs.com/png.latex?n=5000">)</p></li>
<li><p>평가 지표로는 다음 두 가지를 사용합니다:</p>
<ul>
<li><ol type="1">
<li><strong>SHD (Structural Hamming Distance):</strong> 예측된 그래프와 정답 그래프 간의 엣지 불일치 개수 (낮을수록 좋음).</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>FDR (False Discovery Rate):</strong> 잘못 예측된 엣지의 비율 (낮을수록 좋음).</li>
</ol></li>
</ul></li>
</ul>
<section id="linear-case" class="level3">
<h3 class="anchored" data-anchor-id="linear-case">4.1.1. Linear Case</h3>
<ul>
<li>먼저, DAG-GNN이 기존 Linear SEM 환경에서도 잘 작동하는지 확인합니다. 데이터 생성 과정은 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20A%5ET%20x%20+%20z%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?g">는 항등 함수(Identity mapping)입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure2_linear_case.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Linear Case에서의 성능 비교. 그래프 크기(m)가 커짐에 따라 DAG-NOTEARS(파란색)와 DAG-GNN(빨간색)의 SHD와 FDR 변화를 보여준다. DAG-GNN이 선형 모델에서도 NOTEARS와 대등하거나 더 우수한 성능을 보임을 알 수 있다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결과:</strong> 선형 데이터임에도 불구하고, DAG-GNN은 Linear 전용 모델인 NOTEARS보다 더 정확한 구조를 학습했습니다. 특히 그래프 크기가 커질수록(<img src="https://latex.codecogs.com/png.latex?m=100">) 격차가 벌어지는 경향을 보입니다.</li>
</ul>
</section>
<section id="nonlinear-case-core-contribution" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-case-core-contribution">4.1.2. Nonlinear Case (Core Contribution)</h3>
<ul>
<li>DAG-GNN의 진가는 선형 모델을 넘어선 비선형 데이터 처리 능력에서 드러납니다. 저자들은 비선형성의 적용 시점에 따라 두 가지 시나리오를 실험했습니다.</li>
</ul>
<section id="변수에-비선형성이-먼저-적용되는-경우-element-wise-nonlinearity" class="level4">
<h4 class="anchored" data-anchor-id="변수에-비선형성이-먼저-적용되는-경우-element-wise-nonlinearity">1) 변수에 비선형성이 먼저 적용되는 경우 (Element-wise Nonlinearity)</h4>
<ul>
<li>첫 번째는 변수들에 비선형 함수 <img src="https://latex.codecogs.com/png.latex?h">가 적용된 후 선형 결합되는 모델입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20A%5ET%20h(x)%20+%20z,%20%5Cquad%20%5Ctext%7Bwhere%20%7D%20h(x)%20=%20%5Ccos(x%20+%201)%0A"></p>
<ul>
<li>이 경우 <img src="https://latex.codecogs.com/png.latex?h(x)">를 1차 테일러 근사하면 <img src="https://latex.codecogs.com/png.latex?h(x)%20%5Capprox%20h(0)%5Cmathbf%7B1%7D%20+%20h'(0)x">가 되며, 이는 <img src="https://latex.codecogs.com/png.latex?h'(0)A">를 인접 행렬로 갖는 선형 모델로 근사하여 해석할 수 있습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure3_nonlinear_case.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Nonlinear Case (<img src="https://latex.codecogs.com/png.latex?h(x)=%5Ccos(x+1)">)에서의 성능 비교. 비선형성이 도입되자 NOTEARS(파란색)의 SHD와 FDR이 급격히 증가하는 반면, DAG-GNN(빨간색)은 안정적인 성능을 유지한다.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure4_heatmap_nonlinear.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: 파라미터 추정 히트맵 (Heatmap). (왼쪽) True Graph, (중간) DAG-GNN 추정, (오른쪽) NOTEARS 추정. DAG-GNN은 정답의 희소(Sparse)한 구조를 잘 복원하고 “False Alarm”이 적은 반면, NOTEARS는 노이즈가 많이 낀 결과를 보여준다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결과:</strong> SHD 측면에서 약간의 개선이 있었으나, 특히 <strong>FDR(가짜 발견율)이 약 3배 가량 대폭 개선</strong>되었습니다. 이는 DAG-GNN이 비선형 관계 속에서도 가짜 엣지(False Alarm)를 효과적으로 걸러냄을 보여줍니다.</li>
</ul>
</section>
<section id="선형-결합-후-비선형성이-발생하는-경우-complex-nonlinearity" class="level4">
<h4 class="anchored" data-anchor-id="선형-결합-후-비선형성이-발생하는-경우-complex-nonlinearity">2) 선형 결합 후 비선형성이 발생하는 경우 (Complex Nonlinearity)</h4>
<ul>
<li>저자들은 더 나아가 비선형성이 변수들의 선형 결합 <strong>이후(after)</strong>에 발생하는, 더 높은 수준의 비선형 모델에 대해서도 실험을 수행했습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20=%202%5Csin(A%5ET(x%20+%200.5%20%5Ccdot%20%5Cmathbf%7B1%7D))%20+%20A%5ET(x%20+%200.5%20%5Ccdot%20%5Cmathbf%7B1%7D)%20+%20z%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure5_complex_nonlinear.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: 더 복잡한 비선형 모델에서의 구조 학습 성능 (SHD, FDR). 비선형성이 심화될수록 DAG-NOTEARS(파란색)와의 성능 격차가 더욱 벌어지며, DAG-GNN(빨간색)이 월등히 낮은 에러율을 보인다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결과:</strong> Figure 5에서 볼 수 있듯이, 비선형성이 더 강한 이 모델에서는 DAG-GNN이 DAG-NOTEARS에 비해 <strong>SHD와 FDR 모든 지표에서 압도적으로 우수한 성능</strong>을 보였습니다. 이는 DAG-GNN이 단순한 근사를 넘어 복잡한 비선형 인과 구조를 포착하는 데 매우 효과적임을 시사합니다.</li>
</ul>
</section>
</section>
<section id="vector-valued-case" class="level3">
<h3 class="anchored" data-anchor-id="vector-valued-case">4.1.3. Vector-Valued Case</h3>
<ul>
<li><p>기존 방법론들은 변수를 스칼라(Scalar)로만 취급했습니다. 하지만 DAG-GNN은 GNN 구조 덕분에 각 노드가 벡터(<img src="https://latex.codecogs.com/png.latex?d%20%3E%201">)인 경우도 자연스럽게 처리합니다.</p></li>
<li><p>실험 설정은 다음과 같습니다:</p>
<ul>
<li>노드 차원 <img src="https://latex.codecogs.com/png.latex?d=5">, 잠재 차원 <img src="https://latex.codecogs.com/png.latex?d_Z=1">.</li>
<li>데이터는 더욱 복잡한 비선형 식 <img src="https://latex.codecogs.com/png.latex?x%20=%202%5Csin(A%5ET(x%20+%200.5%20%5Ccdot%201))%20+%20A%5ET(x%20+%200.5%20%5Ccdot%201)%20+%20z"> 로 생성됩니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure6_vector_case.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6: Vector-valued Case의 성능 비교. 벡터 노드와 복잡한 비선형성이 결합된 환경에서 DAG-GNN(빨간색)이 NOTEARS(파란색)를 압도하는 성능 차이를 보여준다.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure7_heatmap_vector.png" class="img-fluid figure-img"></p>
<figcaption>Figure 7: Vector-valued 데이터에 대한 파라미터 추정 비교. DAG-GNN은 Ground Truth의 구조를 거의 완벽하게 복원한 반면, NOTEARS는 구조를 거의 학습하지 못했다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결과:</strong> NOTEARS는 이러한 설정(벡터 입력)을 처리할 수 없어, 데이터를 강제로 스칼라로 변환하거나 가정을 단순화해야 했습니다. 반면 DAG-GNN은 구조적 정보를 잠재 공간(Latent Space)에서 효과적으로 포착하여 압도적인 성능 차이를 보여줍니다.</li>
</ul>
<hr>
</section>
</section>
<section id="benchmark-data-sets-discrete-variables" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-data-sets-discrete-variables">4.2. Benchmark Data Sets (Discrete Variables)</h2>
<ul>
<li>다음으로, 이산형 변수(Discrete Variables)로 구성된 유명한 베이지안 네트워크 벤치마크 데이터셋(Child, Alarm, Pigs)에 대한 실험입니다.</li>
<li>여기서는 <strong>GOPNILP</strong> (Integer Programming을 이용한 Exact Solver)와 비교하여, <strong>BIC Score</strong>를 평가 지표로 사용했습니다.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?m"> (Nodes)</th>
<th style="text-align: left;">Ground Truth BIC</th>
<th style="text-align: left;">GOPNILP BIC</th>
<th style="text-align: left;">DAG-GNN BIC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Child</strong></td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">-1.27e+4</td>
<td style="text-align: left;">-1.27e+4</td>
<td style="text-align: left;">-1.38e+4</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Alarm</strong></td>
<td style="text-align: left;">37</td>
<td style="text-align: left;">-1.07e+4</td>
<td style="text-align: left;">-1.12e+4</td>
<td style="text-align: left;">-1.28e+4</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Pigs</strong></td>
<td style="text-align: left;">441</td>
<td style="text-align: left;">-3.48e+5</td>
<td style="text-align: left;">-3.50e+5</td>
<td style="text-align: left;">-3.69e+5</td>
</tr>
</tbody>
</table>
<p><em>(Table 1의 내용을 재구성)</em></p>
<ul>
<li><strong>해석:</strong> GOPNILP는 전역 최적해(Global Optimum)를 찾는 알고리즘이므로 가장 좋은 점수를 보입니다. DAG-GNN은 근사 해법임에도 불구하고, 전역 최적해에 <strong>합리적으로 근접한(reasonably close)</strong> 결과를 보여줍니다.</li>
<li>BIC 차이가 나는 이유는 VAE 구조가 다항 분포(Multinomial distribution)를 근사하는 과정에서 발생하는 한계로 보이지만, 통합된 프레임워크로 이산형 데이터까지 처리할 수 있다는 점은 큰 강점입니다.</li>
</ul>
<hr>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">4.3. Applications</h2>
<section id="protein-signaling-network" class="level3">
<h3 class="anchored" data-anchor-id="protein-signaling-network">Protein Signaling Network</h3>
<ul>
<li>Sachs et al.&nbsp;(2005)의 단백질 신호 전달 네트워크 데이터(<img src="https://latex.codecogs.com/png.latex?n=7466,%20m=11">)를 사용하여 실제 인과 구조 복원 능력을 테스트했습니다.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Method</th>
<th style="text-align: left;">SHD</th>
<th style="text-align: left;">Predicted Edges</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">FGS (Fast Greedy Search)</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">17</td>
</tr>
<tr class="even">
<td style="text-align: left;">NOTEARS</td>
<td style="text-align: left;">22</td>
<td style="text-align: left;">16</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DAG-GNN</strong></td>
<td style="text-align: left;"><strong>19</strong></td>
<td style="text-align: left;"><strong>18</strong></td>
</tr>
</tbody>
</table>
<p><em>(Table 2의 내용을 재구성)</em></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/images/figure8_protein_network.png" class="img-fluid figure-img"></p>
<figcaption>Figure 8: DAG-GNN이 추정한 단백질 신호 전달 네트워크. 붉은색 화살표는 Ground Truth와 일치하는 엣지, 파란색 점선은 간접 연결, 노란색은 역방향 연결을 나타낸다.</figcaption>
</figure>
</div>
<ul>
<li><strong>결과:</strong> DAG-GNN은 SHD 19를 기록하여, NOTEARS(22) 및 FGS(22)보다 <strong>더 정확하게 실제 생물학적 인과관계(Ground Truth)를 복원</strong>했습니다.</li>
</ul>
</section>
<section id="knowledge-base-construction" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-base-construction">Knowledge Base Construction</h3>
<ul>
<li><p>FB15K-237 데이터셋을 사용하여, 지식 베이스(Knowledge Base) 내의 관계(Relation)들 사이의 인과성을 추론하는 새로운 태스크를 제안했습니다.</p></li>
<li><p>예를 들어, <code>Person/Nationality</code>라는 관계가 있다면, 이것이 <code>Person/Language</code> 관계의 원인이 될 수 있다는 식의 메타 관계를 학습합니다.</p></li>
<li><p>Table 3 결과에 따르면, <code>film/ProducedBy</code> <img src="https://latex.codecogs.com/png.latex?%5CRightarrow"> <code>film/Country</code>와 같이 직관적으로 타당한 인과 관계들을 성공적으로 추출했습니다.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>5. Conclusion</h1>
<ul>
<li><p>본 논문은 그래프 구조 학습(Structure Learning)이라는 난제를 해결하기 위해, <strong>딥러닝(Deep Generative Model)과 연속 최적화(Continuous Optimization)</strong>를 결합한 <strong>DAG-GNN</strong> 프레임워크를 제안했습니다.</p></li>
<li><p>이 연구의 핵심 기여와 의의는 다음과 같이 요약할 수 있습니다.</p>
<ul>
<li><ol type="1">
<li><strong>Generalized SEM:</strong> 기존의 선형 구조 방정식 모델(Linear SEM)을 일반화하여, 비선형 관계와 복잡한 데이터 분포를 포착할 수 있는 VAE 기반 생성 모델을 설계했습니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Novel GNN Architecture:</strong> 인코더와 디코더를 구조 학습에 특화된 새로운 Graph Neural Network로 파라미터화했습니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Polynomial Acyclicity Constraint:</strong> 기존의 Matrix Exponential 제약 조건의 수치적 불안정성을 개선하고 구현 용이성을 높인 다항식 형태의 제약 조건을 제안했습니다.</li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>Versatility:</strong> 실험을 통해 선형/비선형, 연속형/이산형, 스칼라/벡터 노드 등 다양한 데이터 형태에 대해 일관되게 우수한 성능을 입증했습니다.</li>
</ol></li>
</ul></li>
<li><p>DAG-GNN은 인과추론 분야에서 딥러닝의 표현력을 구조 학습에 성공적으로 이식한 중요한 이정표가 되는 연구라 할 수 있습니다.</p></li>
</ul>



</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/DAG-GNN: DAG Structure Learning with Graph Neural Networks/</guid>
  <pubDate>Fri, 30 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Paper Review] Conformal Meta-learners for Predictive Inference of Individual Treatment Effects</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<ul>
<li>최근 의료, 정치, 사회과학 등 다양한 분야에서 <strong>개별 대상에 대한 처치 효과의 이질성(Heterogeneity in Treatment Effects)</strong>을 식별하는 문제는 핵심적인 과제로 떠올랐습니다.</li>
<li>단순히 집단 전체의 평균적인 효과를 아는 것을 넘어, “이 약이 <strong>특정 환자</strong>에게 얼마나 효과가 있을까?”와 같은 질문에 답하기 위해 Machine Learning(ML) 모델을 활용하려는 시도가 늘어나고 있습니다.</li>
<li>하지만 기존의 ML 기반 인과추론 모델들은 대부분 <strong>조건부 평균 처치 효과(CATE, Conditional Average Treatment Effect)</strong>의 <strong>점 추정(Point Estimate)</strong>에만 집중해 왔습니다. 즉, 주어진 공변량 <img src="https://latex.codecogs.com/png.latex?X">를 가진 개인의 기대 처치 효과를 하나의 숫자로만 예측할 뿐, 그 예측이 얼마나 불확실한지에 대한 정보는 제공하지 못했습니다.</li>
<li>이 논문은 이러한 한계를 극복하기 위해, <strong>개별 처치 효과(ITE, Individual Treatment Effect)</strong>에 대한 <strong>예측 구간(Predictive Intervals)</strong>을 생성하는 새로운 프레임워크를 제안합니다.</li>
</ul>
<section id="기존-방법론bayesian의-한계" class="level2">
<h2 class="anchored" data-anchor-id="기존-방법론bayesian의-한계">기존 방법론(Bayesian)의 한계</h2>
<ul>
<li><p>기존에 ITE의 불확실성을 다루기 위한 시도들은 주로 <strong>Bayesian 방법론</strong>에 의존해 왔습니다.</p></li>
<li><p>대표적으로 BART(Bayesian Additive Regression Trees)나 Gaussian Processes(GPs)가 있습니다.</p></li>
<li><p>이들은 사후 분포(Posterior Distribution)를 통해 신용 구간(Credible Interval)을 제공할 수 있다는 장점이 있습니다.</p></li>
<li><p>그러나 이러한 Bayesian 접근법은 다음과 같은 명확한 한계가 존재합니다:</p>
<ul>
<li><ol type="1">
<li><strong>Model-Specific:</strong> 특정 모델 구조에 종속적입니다. 최근 Vision이나 NLP 분야에서 두각을 나타내는 Transformer와 같은 현대적인 딥러닝 아키텍처에 유연하게 적용하기 어렵습니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Lack of Frequentist Guarantee:</strong> Bayesian 신용 구간의 커버리지(Coverage)는 사전 분포(Prior)에 의존하며, 유한한 샘플(Finite-sample)에서 빈도주의적 커버리지(Frequentist Coverage)를 보장하지 못합니다.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="conformal-prediction-cp의-도입" class="level2">
<h2 class="anchored" data-anchor-id="conformal-prediction-cp의-도입">Conformal Prediction (CP)의 도입</h2>
<ul>
<li>이러한 배경에서 저자들은 <strong>Conformal Prediction (CP)</strong>에 주목합니다.</li>
<li>CP는 어떤 ML 모델이든 상관없이(Model-agnostic), 데이터 분포에 대한 가정 없이(Distribution-free), 유효한 예측 구간을 생성할 수 있는 빈도주의적 대안입니다.</li>
<li>하지만 일반적인 회귀(Regression) 문제와 달리, 인과추론 문제에 CP를 적용하는 것은 <strong>“인과추론의 근본적인 문제(Fundamental Problem of Causal Inference)”</strong> 때문에 훨씬 까다롭습니다.</li>
</ul>
</section>
<section id="core-challenges-in-causal-conformal-prediction" class="level2">
<h2 class="anchored" data-anchor-id="core-challenges-in-causal-conformal-prediction">Core Challenges in Causal Conformal Prediction</h2>
<ul>
<li>일반적인 지도 학습(Supervised Learning)에서 우리는 입력 <img src="https://latex.codecogs.com/png.latex?X">와 정답 라벨 <img src="https://latex.codecogs.com/png.latex?Y">를 모두 관측할 수 있습니다.</li>
<li>하지만 인과추론에서의 “라벨”은 <strong>ITE(Individual Treatment Effect)</strong>이며, 이는 관측이 불가능합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BITE%7D_i%20=%20Y_i(1)%20-%20Y_i(0)%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?Y_i(1)">은 처치를 받았을 때의 잠재적 결과, <img src="https://latex.codecogs.com/png.latex?Y_i(0)">는 받지 않았을 때의 결과입니다.</li>
<li>우리는 현실에서 둘 중 하나만 관측할 수 있습니다(Factual Outcome). 나머지 하나는 영원히 알 수 없는 반사실적 결과(Counterfactual Outcome)입니다.</li>
<li>이로 인해 ITE에 대한 예측 구간을 생성할 때 두 가지 주요한 도전 과제가 발생합니다.</li>
</ul>
<section id="challenge-1-covariate-shift-공변량-이동" class="level3">
<h3 class="anchored" data-anchor-id="challenge-1-covariate-shift-공변량-이동">Challenge 1: Covariate Shift (공변량 이동)</h3>
<ul>
<li>처치 집단과 통제 집단은 무작위로 배정되지 않는 경우가 많습니다.</li>
<li>개인의 특성(Covariate)에 따라 처치를 받을 확률이 달라지기 때문에, 처치군과 대조군의 공변량 분포가 서로 다릅니다.</li>
<li>결과적으로 모델을 학습시키는 데이터의 분포와 우리가 예측하고자 하는 목표 모집단의 분포가 달라지는 <strong>Covariate Shift</strong> 문제가 발생합니다.</li>
</ul>
</section>
<section id="challenge-2-inductive-biases-귀납적-편향" class="level3">
<h3 class="anchored" data-anchor-id="challenge-2-inductive-biases-귀납적-편향">Challenge 2: Inductive Biases (귀납적 편향)</h3>
<ul>
<li>ITE는 직접 관측되지 않기 때문에 모델을 ITE에 직접 피팅(Fit)할 수 없습니다.</li>
<li>대신 우리는 <img src="https://latex.codecogs.com/png.latex?Y(1)">과 <img src="https://latex.codecogs.com/png.latex?Y(0)">라는 <strong>Nuisance Parameters(관심 없는 모수)</strong>를 각각 추정하고 이를 조합해야 합니다.</li>
<li>이 과정에서 어떤 방식으로 잠재적 결과를 추정하고 결합하느냐에 따라 모델의 성능이 달라지며, 이를 해결하기 위해 다양한 <strong>Meta-learner</strong>들이 개발되었습니다.</li>
</ul>
</section>
</section>
<section id="proposed-framework-conformal-meta-learners" class="level2">
<h2 class="anchored" data-anchor-id="proposed-framework-conformal-meta-learners">Proposed Framework: Conformal Meta-learners</h2>
<ul>
<li>이 논문의 핵심 기여는 위의 두 가지 문제(Covariate Shift, Inductive Biases)를 동시에 해결하는 <strong>Conformal Meta-learners</strong> 프레임워크를 제안한 것입니다.</li>
</ul>
<section id="two-stage-pseudo-outcome-regression" class="level3">
<h3 class="anchored" data-anchor-id="two-stage-pseudo-outcome-regression">Two-stage Pseudo-outcome Regression</h3>
<ul>
<li>저자들은 <strong>Two-stage pseudo-outcome regression</strong>에 기반한 광범위한 Meta-learner 클래스에 집중합니다.</li>
<li>이 방법론은 다음과 같은 2단계로 진행됩니다:
<ul>
<li><ol type="1">
<li><strong>Pseudo-outcome Estimation:</strong> 관측 가능한 변수들만을 사용하여 ITE의 대리 변수(Proxy) 역할을 하는 <strong>Pseudo-outcome</strong>을 생성합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Regression:</strong> 이 Pseudo-outcome을 공변량 <img src="https://latex.codecogs.com/png.latex?X">에 대해 회귀 분석하여 CATE의 점 추정치를 얻습니다.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="conformal-inference-procedure" class="level3">
<h3 class="anchored" data-anchor-id="conformal-inference-procedure">Conformal Inference Procedure</h3>
<ul>
<li><p>Conformal Meta-learner는 이 과정 위에 CP를 적용합니다.</p></li>
<li><p>핵심 아이디어는 <strong>보류된 보정 데이터셋(Held-out Calibration Set)에서 Pseudo-outcome에 대한 적합성 점수(Conformity Scores)를 계산하고, 이 점수의 경험적 분위수(Empirical Quantile)를 사용하여 구간을 구성</strong>하는 것입니다.</p></li>
<li><p>이 접근법이 갖는 장점은 다음과 같습니다:</p>
<ul>
<li><strong>Covariate Shift 해결:</strong> Pseudo-outcome과 연관된 공변량의 분포는 학습 데이터와 테스트 데이터에서 동일하게 취급될 수 있습니다.</li>
<li><strong>Inductive Biases 해결:</strong> 보정 단계(Calibration step)가 모델 아키텍처와 분리되어 있습니다. 따라서 CATE 추정에 효과적이라고 알려진 기존의 다양한 Meta-learner(예: T-learner, X-learner 등)나 딥러닝 아키텍처를 그대로 가져와서 사용할 수 있습니다.</li>
</ul></li>
</ul>
</section>
<section id="theoretical-guarantee-stochastic-ordering" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-guarantee-stochastic-ordering">Theoretical Guarantee: Stochastic Ordering</h3>
<ul>
<li><p>하지만 Pseudo-outcome에 대해 CP를 적용한다고 해서, 그것이 곧바로 관측되지 않은 <strong>ITE</strong>에 대한 커버리지를 보장하는 것은 아닙니다.</p></li>
<li><p>저자들은 이를 증명하기 위해 <strong>확률적 순서(Stochastic Ordering)</strong> 프레임워크를 도입했습니다.</p></li>
<li><p>만약 우리가 사용하는 Pseudo-outcome 기반의 적합성 점수(Conformity Score)가, 실제 ITE를 알았을 때 계산할 수 있는 “Oracle” 적합성 점수보다 <strong>확률적으로 우월(Stochastically Dominate)</strong>하다면, 결과적으로 생성된 구간은 유효(Valid)합니다.</p></li>
<li><p>특히, 널리 사용되는 <strong>Doubly-Robust Learner</strong>와 같은 Meta-learner들이 이러한 확률적 지배 조건(또는 볼록 지배 조건)을 만족함을 증명했습니다.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="predictive-inference-of-individual-treatment-effects-ites" class="level1">
<h1>2. Predictive Inference of Individual Treatment Effects (ITEs)</h1>
<section id="problem-setup" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup">2.1. Problem Setup</h2>
<ul>
<li>논문은 인과추론의 표준인 <strong>잠재적 결과 프레임워크(Potential Outcomes Framework)</strong>, 혹은 <strong>Rubin Causal Model</strong>을 따릅니다.</li>
</ul>
<section id="notation-definition" class="level3">
<h3 class="anchored" data-anchor-id="notation-definition">Notation &amp; Definition</h3>
<ul>
<li>데이터는 <img src="https://latex.codecogs.com/png.latex?n">명의 대상(subject)에 대해 관측되며, 각 대상 <img src="https://latex.codecogs.com/png.latex?i">는 다음과 같은 변수들을 가집니다.
<ul>
<li><strong>Covariates (공변량)</strong> <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20%5Cmathcal%7BX%7D">: 개인의 특징.</li>
<li><strong>Treatment Indicator (처치 여부)</strong> <img src="https://latex.codecogs.com/png.latex?W%20%5Cin%20%5C%7B0,%201%5C%7D">: <img src="https://latex.codecogs.com/png.latex?1">이면 처치군, <img src="https://latex.codecogs.com/png.latex?0">이면 대조군.</li>
<li><strong>Outcome (결과)</strong> <img src="https://latex.codecogs.com/png.latex?Y%20%5Cin%20%5Cmathbb%7BR%7D">: 우리가 관심 있는 결과 변수.</li>
</ul></li>
<li>각 대상 <img src="https://latex.codecogs.com/png.latex?i">에 대해 두 가지 <strong>잠재적 결과(Potential Outcomes)</strong>가 존재합니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y_i(1)">: 처치를 받았을 때의 결과 (<img src="https://latex.codecogs.com/png.latex?W=1">)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_i(0)">: 처치를 받지 않았을 때의 결과 (<img src="https://latex.codecogs.com/png.latex?W=0">)</li>
</ul></li>
<li>하지만 현실에서는 <strong>인과추론의 근본적인 문제(Fundamental Problem of Causal Inference)</strong>로 인해, 우리는 오직 실제 발생한 결과(Factual Outcome)만을 관측할 수 있습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_i%20=%20W_i%20Y_i(1)%20+%20(1%20-%20W_i)%20Y_i(0)%0A"></p>
<ul>
<li>반면, 관측되지 않은 쪽인 <img src="https://latex.codecogs.com/png.latex?Y_i(1-W_i)">는 <strong>반사실적 결과(Counterfactual Outcome)</strong>가 됩니다.</li>
<li>우리는 <img src="https://latex.codecogs.com/png.latex?n">명의 대상에 대한 데이터 생성 과정(Data Generation Process)이 결합 분포 <img src="https://latex.codecogs.com/png.latex?P">로부터 <strong>독립적이고 동일하게 분포(i.i.d.)</strong>한다고 가정합니다.</li>
</ul>
<p><span id="eq-(1)"><img src="https://latex.codecogs.com/png.latex?%0A(X_i,%20W_i,%20Y_i(0),%20Y_i(1))%20%5Coverset%7Biid%7D%7B%5Csim%7D%20P(X,%20W,%20Y(0),%20Y(1)),%20%5Cquad%20i%20=%201,%20%5Cdots,%20n%0A%5Ctag%7B1%7D"></span></p>
</section>
<section id="assumptions" class="level3">
<h3 class="anchored" data-anchor-id="assumptions">Assumptions</h3>
<ul>
<li><p>관측된 데이터 <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_i%20=%20(X_i,%20W_i,%20Y_i)%5C%7D_%7Bi=1%7D%5En">로부터 인과 효과를 식별(Identification)하기 위해 다음 세 가지 가정을 도입합니다.</p></li>
<li><ol type="1">
<li><strong>Unconfoundedness (Ignorability):</strong> <img src="https://latex.codecogs.com/png.latex?(Y(0),%20Y(1))%20%5Cperp%20W%20%5Cmid%20X"></li>
</ol>
<ul>
<li>공변량 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때, 처치 할당 <img src="https://latex.codecogs.com/png.latex?W">는 잠재적 결과들과 독립입니다.</li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?X"> 외에 처치와 결과 모두에 영향을 주는 숨겨진 교란 변수(Confounder)는 없습니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Consistency:</strong> <img src="https://latex.codecogs.com/png.latex?Y%20=%20Y(W)"></li>
</ol>
<ul>
<li>관측된 결과 <img src="https://latex.codecogs.com/png.latex?Y">는 실제로 받은 처치 <img src="https://latex.codecogs.com/png.latex?W">에 해당하는 잠재적 결과와 일치합니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>Positivity (Overlap):</strong> <img src="https://latex.codecogs.com/png.latex?0%20%3C%20P(W=1%20%5Cmid%20X=x)%20%3C%201,%20%5Cquad%20%5Cforall%20x%20%5Cin%20%5Cmathcal%7BX%7D"></li>
</ol>
<ul>
<li>모든 공변량 영역에서 처치군과 대조군이 될 확률이 0이 아니어야 합니다.</li>
</ul></li>
</ul>
</section>
<section id="cate-vs.-ite-무엇이-다른가" class="level3">
<h3 class="anchored" data-anchor-id="cate-vs.-ite-무엇이-다른가">CATE vs.&nbsp;ITE: 무엇이 다른가?</h3>
<ul>
<li><p>이 논문에서 가장 중요하게 강조하는 구분이 바로 <strong>CATE</strong>와 <strong>ITE</strong>의 차이입니다.</p></li>
<li><p><strong>CATE (Conditional Average Treatment Effect):</strong></p>
<ul>
<li>기존 연구들이 주로 추정해온 <strong>결정론적(Deterministic) 함수</strong>로, 조건부 기대값의 차이입니다. <img src="https://latex.codecogs.com/png.latex?%5Ctau(x)%20%5Ctriangleq%20%5Cmathbb%7BE%7D%5BY(1)%20-%20Y(0)%20%5Cmid%20X=x%5D"></li>
</ul></li>
<li><p><strong>ITE (Individual Treatment Effect):</strong></p>
<ul>
<li>이 논문의 관심 대상인 <strong>확률 변수(Random Variable)</strong>입니다. <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BITE%7D_i%20=%20Y_i(1)%20-%20Y_i(0)"></li>
</ul></li>
<li><p>CATE는 “당신과 같은 특성을 가진 사람들의 평균적인 효과”를 말해주지만, ITE는 “당신이 겪을 실제 효과”를 의미합니다.</p></li>
<li><p>ITE는 모델의 오차뿐만 아니라, 같은 <img src="https://latex.codecogs.com/png.latex?X">를 가진 사람들 사이에서도 존재하는 <strong>내재적 변동성(Intrinsic Variability)</strong>을 포함합니다.</p></li>
<li><p>관측된 변수 <img src="https://latex.codecogs.com/png.latex?Z=(X,%20W,%20Y)">의 분포를 기술하기 위해, 우리는 다음과 같은 두 가지 <strong>Nuisance Functions(방해 모수 함수)</strong>를 정의합니다.</p>
<ul>
<li><strong>Propensity Score <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">:</strong> 처치 할당 메커니즘을 나타냅니다.</li>
<li><strong>Outcome Mean Functions <img src="https://latex.codecogs.com/png.latex?%5Cmu_w(x)">:</strong> 각 처치 그룹 내에서의 조건부 기대 결과를 나타냅니다.</li>
</ul></li>
</ul>
<p><span id="eq-(2)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cpi(x)%20&amp;=%20%5Cmathbb%7BP%7D(W=1%20%5Cmid%20X=x),%20%5C%5C%0A%5Cmu_w(x)%20&amp;=%20%5Cmathbb%7BE%7D%5BY%20%5Cmid%20X=x,%20W=w%5D,%20%5Cquad%20w%20%5Cin%20%5C%7B0,%201%5C%7D.%0A%5Cend%7Baligned%7D%0A%5Ctag%7B2%7D"></span></p>
<ul>
<li>특히 본 논문에서는 데이터가 실험 연구(Experimental Study)에서 얻어졌거나 처치 할당 메커니즘이 알려져 있어, <strong>Propensity Score <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">를 알고 있다(Known)</strong>고 가정합니다.</li>
</ul>
</section>
<section id="목표-marginally-valid-predictive-interval" class="level3">
<h3 class="anchored" data-anchor-id="목표-marginally-valid-predictive-interval">목표: Marginally Valid Predictive Interval</h3>
<ul>
<li>논문의 목표는 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D">이 주어졌을 때, 실제 ITE가 포함될 확률이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 이상인 <strong>예측 구간(Predictive Band)</strong> <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D(x)">를 구성하는 것입니다.</li>
</ul>
<p><span id="eq-(3)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(Y_%7Bn+1%7D(1)%20-%20Y_%7Bn+1%7D(0)%20%5Cin%20%5Chat%7BC%7D(X_%7Bn+1%7D))%20%5Cge%201%20-%20%5Calpha%0A%5Ctag%7B3%7D"></span></p>
<ul>
<li>이 확률은 훈련 데이터 <img src="https://latex.codecogs.com/png.latex?%5C%7BZ_i%5C%7D">와 새로운 테스트 포인트의 랜덤성을 모두 고려한 <strong>Marginal Validity</strong>를 의미합니다.</li>
</ul>
<hr>
</section>
</section>
<section id="conformal-prediction" class="level2">
<h2 class="anchored" data-anchor-id="conformal-prediction">2.2. Conformal Prediction</h2>
<ul>
<li>이 목표를 달성하기 위한 도구로 <strong>Conformal Prediction (CP)</strong>을 사용합니다.</li>
<li>CP는 모델이나 분포에 대한 가정 없이(Model-free, Distribution-free) 유효한 예측 구간을 생성하는 프레임워크입니다.</li>
</ul>
<section id="split-conformal-prediction-inductive-cp" class="level3">
<h3 class="anchored" data-anchor-id="split-conformal-prediction-inductive-cp">Split Conformal Prediction (Inductive CP)</h3>
<ul>
<li><p>가장 기본적인 형태인 Split CP의 절차는 다음과 같습니다.</p></li>
<li><ol type="1">
<li><strong>Data Splitting:</strong> 전체 데이터 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D">를 훈련 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bt%7D">와 보정 집합(Calibration set) <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc%7D">로 나눕니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Model Fitting:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bt%7D">를 사용하여 예측 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D(x)">를 학습합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Conformity Scores Calculation:</strong> 보정 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc%7D">의 모든 샘플에 대해, 실제 값과 예측 값의 괴리를 나타내는 점수(Conformity Score)를 계산합니다. <span id="eq-(4)"><img src="https://latex.codecogs.com/png.latex?V_k(%5Chat%7B%5Cmu%7D)%20%5Ctriangleq%20V(X_k,%20Y_k;%20%5Chat%7B%5Cmu%7D),%20%5Cquad%20%5Cforall%20k%20%5Cin%20%5Cmathcal%7BD%7D_c%20%5Ctag%7B4%7D"></span><br>
</li>
</ol>
<ul>
<li>일반적인 회귀 문제에서는 절대 잔차(Absolute Residual)를 사용합니다. <img src="https://latex.codecogs.com/png.latex?V_k(%5Chat%7B%5Cmu%7D)%20%5Ctriangleq%20%7C%20%5Chat%7B%5Cmu%7D(X_k)%20-%20Y_k%20%7C,%20%5Cquad%20%5Cforall%20k%20%5Cin%20%5Cmathcal%7BD%7D_c"></li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>Quantile Computation:</strong> 계산된 모든 점수들의 집합을 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BV%7D(%5Chat%7B%5Cmu%7D)%20=%20%5C%7BV_k(%5Chat%7B%5Cmu%7D)%20:%20k%20%5Cin%20%5Cmathcal%7BD%7D_c%5C%7D">라고 할 때, 목표 커버리지 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">에 해당하는 경험적 분위수(Empirical Quantile)를 구합니다. <span id="eq-(5)"><img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cmathcal%7BV%7D%7D(1-%5Calpha)%20%5Ctriangleq%20(1-%5Calpha)(1%20+%201/%7C%5Cmathcal%7BD%7D_c%7C)%5Ctext%7B-th%20quantile%20of%20%7D%20%5Cmathcal%7BV%7D(%5Chat%7B%5Cmu%7D)%20%5Ctag%7B5%7D"></span></li>
</ol></li>
<li><ol start="5" type="1">
<li><strong>Interval Construction:</strong> 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D=x">에 대한 예측 구간 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D(x)">는 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D(x)">를 중심으로 해당 분위수만큼 벌린 구간으로 정의됩니다. <span id="eq-(6)"><img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D(x)%20=%20%5B%5Chat%7B%5Cmu%7D(x)%20-%20Q_%7B%5Cmathcal%7BV%7D%7D(1-%5Calpha),%20%5C;%5C;%20%5Chat%7B%5Cmu%7D(x)%20+%20Q_%7B%5Cmathcal%7BV%7D%7D(1-%5Calpha)%5D%20%5Ctag%7B6%7D"></span></li>
</ol></li>
<li><p>이 구간은 훈련 데이터와 테스트 데이터 간의 <strong>Exchangeability (교환 가능성)</strong> 가정 하에서, 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?Y_%7Bn+1%7D">을 포함할 확률이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 이상임이 수학적으로 보장됩니다 (Marginal Coverage Guarantee).</p></li>
</ul>
<hr>
</section>
</section>
<section id="oracle-conformal-prediction-of-ites" class="level2">
<h2 class="anchored" data-anchor-id="oracle-conformal-prediction-of-ites">2.3. Oracle Conformal Prediction of ITEs</h2>
<ul>
<li><p>이제 CP를 ITE 추정에 적용해봅시다. 만약 우리가 <strong>신(Oracle)</strong>이어서 반사실적 결과까지 모두 볼 수 있다면, 문제는 매우 간단해집니다.</p></li>
<li><p>가상의 “Oracle 데이터셋” <img src="https://latex.codecogs.com/png.latex?%7B%5Cmathcal%7BD%7D%7D%5E*%20=%20%5C%7B(X_i,%20%5Cunderbrace%7BY_i(1)%20-%20Y_i(0)%7D_%7B%5Ctext%7BTrue%20ITE%7D%7D)%5C%7D_%7Bi%7D"> 가 있다고 가정해봅시다.</p></li>
<li><p>이 경우 <strong>Oracle Conformity Score</strong>를 다음과 같이 정의할 수 있습니다. <span id="eq-(7)"><img src="https://latex.codecogs.com/png.latex?V%5E*_k(%5Chat%7B%5Ctau%7D)%20%5Ctriangleq%20V(X_k,%20%5Cunderbrace%7BY_k(1)%20-%20Y_k(0)%7D_%7B%5Ctext%7BLabel%7D%7D,%20%5Chat%7B%5Ctau%7D)%20%5Ctag%7B7%7D"></span></p></li>
<li><p>이 점수를 사용해 CP를 수행하면 식 (3)의 coverage 조건을 완벽하게 만족하는 구간 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D%5E*(X)">를 얻을 수 있습니다.</p></li>
<li><p>하지만 현실에서는 <img src="https://latex.codecogs.com/png.latex?Y_i(1)">과 <img src="https://latex.codecogs.com/png.latex?Y_i(0)"> 중 하나만 관측되므로, <strong>이 Oracle 절차는 불가능(Infeasible)</strong>합니다.</p></li>
</ul>
<hr>
</section>
<section id="the-two-challenges-of-predictive-inference-on-ites" class="level2">
<h2 class="anchored" data-anchor-id="the-two-challenges-of-predictive-inference-on-ites">2.4. The Two Challenges of Predictive Inference on ITEs</h2>
<ul>
<li><p>현실적인 대안으로, 관측된 데이터를 처치군(<img src="https://latex.codecogs.com/png.latex?W=1">)과 대조군(<img src="https://latex.codecogs.com/png.latex?W=0">)으로 나누어 각각의 잠재적 결과 <img src="https://latex.codecogs.com/png.latex?Y(1)">과 <img src="https://latex.codecogs.com/png.latex?Y(0)">에 대해 별도로 CP를 적용하는 <strong>“Naïve Approach”</strong>를 생각할 수 있습니다.</p></li>
<li><p><strong>데이터 분할(Data Splitting)</strong></p>
<ul>
<li>이 방식은 먼저 데이터를 처치 그룹(<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_1">)과 대조 그룹(<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_0">)으로 분할합니다.</li>
</ul></li>
<li><p><strong>적합성 점수 계산(Conformity Score Calculation)</strong></p>
<ul>
<li>각각의 Nuisance Estimate (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D_1,%20%5Chat%7B%5Cmu%7D_0">)에 대해 다음과 같이 별도의 적합성 점수를 계산합니다. <span id="eq-(8)"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AV_k%5E%7B(0)%7D(%5Chat%7B%5Cmu%7D_0)%20&amp;%5Ctriangleq%20%7CY_k(0)%20-%20%5Chat%7B%5Cmu%7D_0(X_k)%7C,%20%5Cquad%20%5Cforall%20k%20%5Cin%20%5Cmathcal%7BD%7D_%7Bc,0%7D%20%5C%5C%0AV_k%5E%7B(1)%7D(%5Chat%7B%5Cmu%7D_1)%20&amp;%5Ctriangleq%20%7CY_k(1)%20-%20%5Chat%7B%5Cmu%7D_1(X_k)%7C,%20%5Cquad%20%5Cforall%20k%20%5Cin%20%5Cmathcal%7BD%7D_%7Bc,1%7D%0A%5Cend%7Baligned%7D%0A%5Ctag%7B8%7D"></span></li>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc,0%7D">와 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc,1%7D">은 각각 대조군과 처치군의 보정(Calibration) 데이터셋을 의미합니다.</li>
</ul></li>
<li><p><strong>개별 구간 생성 (Individual Calibration):</strong></p>
<ul>
<li>그 다음, 각 그룹에서 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha/2"> 수준의 분위수(Quantile) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D_0,%20%5Chat%7Bq%7D_1">를 계산하여 개별 예측 구간 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D_0(X),%20%5Chat%7BC%7D_1(X)">를 생성합니다.</li>
<li>이때 ITE의 동시 포함 확률(Joint Coverage) <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 보장하기 위해, 본페로니 보정(Bonferroni Correction)을 적용하여 각 구간의 신뢰 수준을 높여야 합니다.</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BC%7D_w(X)%20=%20%5Cleft%5B%20%5Chat%7B%5Cmu%7D_w(X)%20-%20%5Chat%7Bq%7D_w,%20%5C;%5C;%20%5Chat%7B%5Cmu%7D_w(X)%20+%20%5Chat%7Bq%7D_w%20%5Cright%5D,%20%5Cquad%20w%20%5Cin%20%5C%7B0,%201%5C%7D%0A"></p>
<ul>
<li><strong>구간 결합 (Interval Combination):</strong>
<ul>
<li>최종적으로 ITE <img src="https://latex.codecogs.com/png.latex?%5Ctau(X)%20=%20Y(1)%20-%20Y(0)">에 대한 예측 구간 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BC%7D_%7B%5Ctext%7BITE%7D%7D(X)">는 두 구간의 차이(Minkowski difference)로 정의됩니다.</li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?Y(1)">이 가질 수 있는 구간에서 <img src="https://latex.codecogs.com/png.latex?Y(0)">가 가질 수 있는 구간을 뺐을 때 나올 수 있는 <strong>최소값과 최대값의 범위</strong>를 구합니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Chat%7BC%7D_%7B%5Ctext%7BITE%7D%7D(X)%20&amp;=%20%5Chat%7BC%7D_1(X)%20%5Cominus%20%5Chat%7BC%7D_0(X)%20%5C%5C%0A&amp;=%20%5Cleft%5B%20%5Cmin(%5Chat%7BC%7D_1)%20-%20%5Cmax(%5Chat%7BC%7D_0),%20%5C;%5C;%20%5Cmax(%5Chat%7BC%7D_1)%20-%20%5Cmin(%5Chat%7BC%7D_0)%20%5Cright%5D%20%5C%5C%0A&amp;=%20%5Cleft%5B%20(%5Chat%7B%5Cmu%7D_1(X)%20-%20%5Chat%7B%5Cmu%7D_0(X))%20-%20(%5Chat%7Bq%7D_1%20+%20%5Chat%7Bq%7D_0),%20%5C;%5C;%20(%5Chat%7B%5Cmu%7D_1(X)%20-%20%5Chat%7B%5Cmu%7D_0(X))%20+%20(%5Chat%7Bq%7D_1%20+%20%5Chat%7Bq%7D_0)%20%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></li>
<li>이 식을 통해 알 수 있듯, Naïve Approach의 ITE 구간 폭(Width)은 <strong>두 개별 구간 폭의 합(<img src="https://latex.codecogs.com/png.latex?2%5Chat%7Bq%7D_1%20+%202%5Chat%7Bq%7D_0">)</strong>이 되어 불확실성이 단순 합산됩니다.</li>
</ul></li>
<li>이러한 접근법은 직관적이지만, 실제로는 다음과 같은 두 가지 심각한 문제에 직면합니다.</li>
</ul>
<section id="challenge-1-covariate-shift-공변량-이동-1" class="level3">
<h3 class="anchored" data-anchor-id="challenge-1-covariate-shift-공변량-이동-1">Challenge 1: Covariate Shift (공변량 이동)</h3>
<ul>
<li><p>CP의 핵심 가정은 <strong>Exchangeability</strong>입니다. 즉, 보정 데이터(Calibration data)와 테스트 데이터(Test data)가 같은 분포에서 와야 합니다.</p></li>
<li><p>그러나 인과추론 상황에서는 필연적으로 <strong>Covariate Shift</strong>가 발생합니다.처치를 받을 확률(Propensity Score, <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">)이 개인마다 다르기 때문입니다.</p></li>
<li><p>처치군(<img src="https://latex.codecogs.com/png.latex?W=1">)의 공변량 분포 <img src="https://latex.codecogs.com/png.latex?P_%7BX%7CW=1%7D">와 대조군(<img src="https://latex.codecogs.com/png.latex?W=0">)의 분포 <img src="https://latex.codecogs.com/png.latex?P_%7BX%7CW=0%7D">는 서로 다릅니다. 무엇보다, 이 둘은 우리가 예측하고자 하는 전체 모집단의 분포 <img src="https://latex.codecogs.com/png.latex?P_X">와도 다릅니다. <img src="https://latex.codecogs.com/png.latex?P_%7BX%7CW=0%7D%20%5Cneq%20P_%7BX%7CW=1%7D%20%5Cneq%20P_X"></p></li>
<li><p>이러한 공변량의 이동은 결과적으로 우리가 계산하는 <strong>Conformity Score (<img src="https://latex.codecogs.com/png.latex?V">)의 결합 분포</strong>에도 영향을 미칩니다. 즉, 특정 처치 그룹에서 계산한 점수의 분포가 전체 모집단에서의 분포와 일치하지 않게 됩니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP_%7BX,V%5E%7B(0)%7D%7CW=0%7D%20%5Cneq%20P_%7BX,V%5E%7B(0)%7D%7D,%20%5Cquad%20P_%7BX,V%5E%7B(1)%7D%7CW=1%7D%20%5Cneq%20P_%7BX,V%5E%7B(1)%7D%7D%0A"></p>
<ul>
<li>따라서 단순히 처치군 데이터로 학습하고 보정한 모델을 전체 모집단에 적용하면 <strong>Exchangeability 가정이 깨지므로, CP의 커버리지 보장(Validity)이 성립하지 않습니다.</strong></li>
</ul>
</section>
<section id="challenge-2-inductive-biases-귀납적-편향-1" class="level3">
<h3 class="anchored" data-anchor-id="challenge-2-inductive-biases-귀납적-편향-1">Challenge 2: Inductive Biases (귀납적 편향)</h3>
<ul>
<li>지도 학습에서는 <img src="https://latex.codecogs.com/png.latex?(X,%20Y)"> 쌍을 통해 하나의 함수를 학습하지만, ITE 추정은 관측되지 않는 효과를 추정해야 하므로 모델링의 자유도가 훨씬 높습니다. 이를 위해 다양한 <strong>Meta-learner</strong>들이 존재합니다.
<ul>
<li><strong>T-learner:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmu_0(x)">와 <img src="https://latex.codecogs.com/png.latex?%5Cmu_1(x)">를 별도로 학습하여 <img src="https://latex.codecogs.com/png.latex?Y(0),%20Y(1)"> 각각에 서로 다른 규제(Regularization)를 가함.</li>
<li><strong>S-learner:</strong> 처치 변수 <img src="https://latex.codecogs.com/png.latex?W">를 공변량에 포함시켜 하나의 모델 <img src="https://latex.codecogs.com/png.latex?%5Cmu(X,%20W)"> 학습.</li>
</ul></li>
<li>각 Meta-learner는 <img src="https://latex.codecogs.com/png.latex?Y(0)">와 <img src="https://latex.codecogs.com/png.latex?Y(1)">이라는 <strong>Nuisance Parameters(방해 모수)</strong>를 추정하고 결합하는 방식에 있어 서로 다른 <strong>Inductive Bias</strong>를 가집니다.</li>
</ul>
</section>
<section id="limitations-of-existing-approaches-기존-방법의-한계" class="level3">
<h3 class="anchored" data-anchor-id="limitations-of-existing-approaches-기존-방법의-한계">Limitations of Existing Approaches (기존 방법의 한계)</h3>
<ul>
<li>기존 연구들은 이러한 Covariate Shift를 해결하기 위해 Conformity Score에 가중치를 부여(Reweighting)하거나, <img src="https://latex.codecogs.com/png.latex?Y(0)">와 <img src="https://latex.codecogs.com/png.latex?Y(1)"> 각각에 대한 구간을 구한 뒤 이를 결합하는 방식을 사용했습니다. 하지만 이는 다음과 같은 한계를 가집니다.
<ul>
<li><ol type="1">
<li><strong>Loss of Post-hoc Nature (사후적 특성 상실):</strong> CP 절차가 모델 아키텍처(Nuisance Parameter 추정 방식)에 종속되게 만들어, CP 고유의 장점인 ‘모델에 구애받지 않는(Model-agnostic)’ 성격을 잃게 합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Conservativeness (보수성):</strong> 두 개의 잠재적 결과(PO) 구간을 결합하여 ITE 구간을 생성할 경우, 구간이 불필요하게 넓어져 효율성이 떨어집니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Limited Applicability (적용의 한계):</strong> 모든 CATE 모델이 <img src="https://latex.codecogs.com/png.latex?Y(0)">와 <img src="https://latex.codecogs.com/png.latex?Y(1)">을 명시적으로 추정하는 것은 아니므로, 다양한 Inductive Prior를 유연하게 적용하기 어렵습니다.</li>
</ol></li>
</ul></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="conformal-meta-learners" class="level1">
<h1>3. Conformal Meta-learners</h1>
<ul>
<li>저자들이 제안한 핵심 솔루션인 <strong>Conformal Meta-learners</strong> 프레임워크를 자세히 살펴봅니다.</li>
<li>이 방법론은 <strong>Pseudo-outcome</strong>이라는 개념을 도입하여 기존의 CATE 추정 모델들을 Conformal Prediction(CP) 파이프라인에 유연하게 결합합니다.</li>
<li>또한, 우리가 만든 구간이 실제 ITE를 포함한다는 것을 보장하기 위해 필요한 수학적 조건인 <strong>Stochastic Dominance(확률적 지배)</strong> 개념까지 확장해 보겠습니다.</li>
</ul>
<section id="pseudo-outcome-regression-for-cate-estimation" class="level2">
<h2 class="anchored" data-anchor-id="pseudo-outcome-regression-for-cate-estimation">3.1. Pseudo-outcome Regression for CATE Estimation</h2>
<ul>
<li>Conformal Meta-learner의 핵심 아이디어는 관측되지 않는 ITE(<img src="https://latex.codecogs.com/png.latex?Y(1)-Y(0)">)를 직접 예측하려 드는 대신, 관측 가능한 변수들로 구성된 대리 변수, 즉 <strong>Pseudo-outcome (<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">)</strong>을 정의하는 것입니다.</li>
</ul>
<section id="two-stage-regression-procedure" class="level3">
<h3 class="anchored" data-anchor-id="two-stage-regression-procedure">Two-stage Regression Procedure</h3>
<ul>
<li><p>이 프레임워크는 크게 두 단계로 구성됩니다.</p></li>
<li><ol type="1">
<li><strong>Stage 1 (Nuisance Estimation):</strong></li>
</ol>
<ul>
<li>데이터의 일부(<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Cvarphi%7D">)를 사용하여 Nuisance parameter인 <img src="https://latex.codecogs.com/png.latex?%5Cvarphi%20=%20(%5Cpi,%20%5Cmu_0,%20%5Cmu_1)">를 추정합니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cpi(x)%20=%20P(W=1%7CX=x)">: Propensity score (실험 데이터라면 알려져 있음)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu_w(x)%20=%20%5Cmathbb%7BE%7D%5BY%7CX=x,%20W=w%5D">: Response surfaces</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Stage 2 (CATE Estimation):</strong></li>
</ol>
<ul>
<li>추정된 Nuisance parameter <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cvarphi%7D">를 사용하여 각 관측치에 대해 Pseudo-outcome <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">를 생성합니다.</li>
<li>그 후, 공변량 <img src="https://latex.codecogs.com/png.latex?X">를 입력으로 하고 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">를 타겟으로 하는 회귀 모델을 학습하여 CATE(<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">)를 추정합니다.</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctau%7D(x)%20=%20%5Cmathbb%7BE%7D%5B%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%20%5Cmid%20X=x%5D%0A"></p>
</section>
<section id="meta-learners-as-pseudo-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="meta-learners-as-pseudo-outcomes">Meta-learners as Pseudo-outcomes</h3>
<ul>
<li>흥미로운 점은, 기존에 제안된 유명한 Meta-learner들이 사실 이 <strong>Pseudo-outcome Regression</strong>의 특수한 형태(Instantiation)로 해석될 수 있다는 것입니다.</li>
<li>논문에서는 다음 세 가지 대표적인 Learner들을 재정의합니다.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Meta-learner</th>
<th style="text-align: left;">Pseudo-outcome Definition (<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">)</th>
<th style="text-align: left;">설명</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>IPW-learner</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cfrac%7BW-%5Cpi(X)%7D%7B%5Cpi(X)(1-%5Cpi(X))%7DY"></td>
<td style="text-align: left;">Propensity Score로 가중치를 부여하여 Factual Outcome을 재조정함. 편향은 적으나 분산이 큼.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>X-learner</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20W(Y-%5Chat%7B%5Cmu%7D_0(X))%20+%20(1-W)(%5Chat%7B%5Cmu%7D_1(X)-Y)"></td>
<td style="text-align: left;">처치군에서는 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D_0">를 빼고, 대조군에서는 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D_1">을 빼서 반사실적 결과를 보정(Imputation)함.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DR-learner</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?%5Cdisplaystyle%20%5Cfrac%7BW-%5Cpi(X)%7D%7B%5Cpi(X)(1-%5Cpi(X))%7D(Y-%5Chat%7B%5Cmu%7D_W(X))%20+%20%5Chat%7B%5Cmu%7D_1(X)%20-%20%5Chat%7B%5Cmu%7D_0(X)"></td>
<td style="text-align: left;">IPW와 Regression Adjustment를 결합한 Doubly Robust 형태. 모델 설정이 하나라도 맞으면 일치성(Consistency)을 가짐.</td>
</tr>
</tbody>
</table>
<ul>
<li>이러한 통합된 관점은 우리가 어떤 Meta-learner를 사용하든, 공통된 Conformal Prediction 절차를 적용할 수 있게 해줍니다.</li>
</ul>
</section>
</section>
<section id="conformal-pseudo-intervals-for-ites" class="level2">
<h2 class="anchored" data-anchor-id="conformal-pseudo-intervals-for-ites">3.2. Conformal Pseudo-Intervals for ITEs</h2>
<ul>
<li>이제 이 Pseudo-outcome 위에 CP를 얹어 <strong>예측 구간(Pseudo-intervals)</strong>을 생성하는 전체 알고리즘을 살펴보겠습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/conformal_meta_learner_process.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Conformal Meta-learners의 전체 구조. (Left) Nuisance Estimator를 통해 생성된 Pseudo-outcome을 사용하여 CATE Estimator를 학습한다. (Right) Calibration Set에서 계산된 Pseudo-outcome의 잔차(Conformity Score) 분포를 이용해 불확실성 구간을 계산한다.</figcaption>
</figure>
</div>
<section id="data-splitting-strategy" class="level3">
<h3 class="anchored" data-anchor-id="data-splitting-strategy">Data Splitting Strategy</h3>
<ul>
<li><p>데이터의 독립성을 보장하기 위해 전체 데이터셋 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D">를 세 개의 상호 배타적인(Mutually Exclusive) 부분집합으로 나눕니다.</p></li>
<li><ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Cvarphi%7D">: Nuisance parameter (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cpi%7D,%20%5Chat%7B%5Cmu%7D_0,%20%5Chat%7B%5Cmu%7D_1">) 학습용</li>
</ol></li>
<li><ol start="2" type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bt%7D">: CATE 모델(<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">) 학습용 (Pseudo-outcome 타겟 회귀)</li>
</ol></li>
<li><ol start="3" type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc%7D">: Calibration(보정) 및 Conformity score 계산용</li>
</ol></li>
</ul>
</section>
<section id="algorithm-steps-algorithm-1" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-steps-algorithm-1">Algorithm Steps (Algorithm 1)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/algorithm1.png" class="img-fluid figure-img"></p>
<figcaption>Algorithm 1: Conformal Meta-Learner</figcaption>
</figure>
</div>
<ul>
<li><ol type="1">
<li><strong>Estimation:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7B%5Cvarphi%7D">를 이용해 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cvarphi%7D%20=%20(%5Cpi,%20%5Chat%7B%5Cmu%7D_0,%20%5Chat%7B%5Cmu%7D_1)">를 추정합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Transformation:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bt%7D">의 각 데이터에 대해 Pseudo-outcome <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">를 계산하고, 이를 타겟으로 하는 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D(x)">를 학습합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Calibration:</strong> 보정 데이터셋 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_%7Bc%7D">에 대해 <strong>Pseudo-outcome 기반의 Conformity Score</strong>를 계산합니다. <span id="eq-(9)"><img src="https://latex.codecogs.com/png.latex?%0A%20%20V_%7B%5Cvarphi,%20k%7D(%5Chat%7B%5Ctau%7D)%20%5Ctriangleq%20V(X_k,%20%5Ctilde%7BY%7D_%7B%5Cvarphi,%20k%7D;%20%5Chat%7B%5Ctau%7D),%20%5Cquad%20%5Cforall%20k%20%5Cin%20%5Cmathcal%7BD%7D_%7Bc%7D%0A%20%20%20%5Ctag%7B9%7D"></span></li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>Interval Construction:</strong> 점수들의 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수(Quantile)인 <img src="https://latex.codecogs.com/png.latex?Q_%7B%5Cmathcal%7BV%7D_%7B%5Cvarphi%7D%7D(1-%5Calpha)">를 구하여, 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D">에 대한 구간을 반환합니다. <span id="eq-(10)"><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Chat%7BC%7D_%7B%5Cvarphi%7D(X_%7Bn+1%7D)%20=%20%5B%5Chat%7B%5Ctau%7D(X_%7Bn+1%7D)%20-%20Q_%7B%5Cmathcal%7BV%7D_%7B%5Cvarphi%7D%7D(1-%5Calpha),%20%5C;%5C;%20%5Chat%7B%5Ctau%7D(X_%7Bn+1%7D)%20+%20Q_%7B%5Cmathcal%7BV%7D_%7B%5Cvarphi%7D%7D(1-%5Calpha)%5D%0A%20%20%20%5Ctag%7B10%7D"></span></li>
</ol></li>
</ul>
</section>
<section id="why-this-solves-the-challenges" class="level3">
<h3 class="anchored" data-anchor-id="why-this-solves-the-challenges">Why this solves the challenges?</h3>
<ul>
<li><strong>Covariate Shift 해결:</strong> Pseudo-outcome <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">는 관측 데이터 <img src="https://latex.codecogs.com/png.latex?(X,%20W,%20Y)">의 함수이므로, Calibration set과 Test set의 공변량 분포가 동일합니다(Exchangeability 성립).</li>
<li><strong>Inductive Bias 분리:</strong> 어떤 Meta-learner를 쓰든 Pseudo-outcome으로 변환만 하면 되므로, CP 절차는 모델 구조와 무관(Model-agnostic)하게 적용 가능합니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="validity-of-conformal-meta-learners-a-stochastic-ordering-framework" class="level1">
<h1>4. Validity of Conformal Meta-learners: A Stochastic Ordering Framework</h1>
<ul>
<li>정리하면 <strong>Conformal Meta-learners</strong>의 알고리즘과 Pseudo-outcome(<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">)을 이용해 CATE를 추정하고 예측 구간을 생성하는 과정을 살펴보았습니다.</li>
<li>하지만 여기서 우리는 아주 본질적이고 불편한 질문을 마주해야 합니다.</li>
<li>우리가 만든 구간은 <strong>Pseudo-outcome</strong>을 잘 맞추도록 설계되었습니다. 그런데 이 구간이 과연 <strong>실제 ITE (Individual Treatment Effect)</strong>를 포함한다고 보장할 수 있을까요?</li>
<li>아래에서 이 질문에 답하기 위해 저자들이 도입한 <strong>Stochastic Ordering (확률적 지배)</strong> 프레임워크와, 이를 통해 증명된 <strong>Theorem 1 &amp; 2</strong>를 검토해 보겠습니다.</li>
</ul>
<section id="the-validity-gap-exchangeability-breakdown" class="level2">
<h2 class="anchored" data-anchor-id="the-validity-gap-exchangeability-breakdown">The Validity Gap: Exchangeability Breakdown</h2>
<ul>
<li><p>Conformal Prediction(CP)의 강력함은 <strong>Exchangeability(교환 가능성)</strong> 가정에서 나옵니다. 하지만 ITE 추정 문제에서는 이 가정이 미묘하게 깨집니다.</p></li>
<li><p>우리가 비교해야 할 두 가지 대상은 다음과 같습니다.</p></li>
<li><ol type="1">
<li><strong>Pseudo-outcome Conformity Score (<img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">):</strong> 우리가 실제로 계산하는 값. <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D(%5Chat%7B%5Ctau%7D)%20=%20%7C%5Chat%7B%5Ctau%7D(X)%20-%20%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%7C"></li>
</ol>
<ul>
<li>이는 Nuisance parameter <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cvarphi%7D">를 통해 변환된 Pseudo-outcome을 사용합니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Oracle Conformity Score (<img src="https://latex.codecogs.com/png.latex?V%5E*">):</strong> 우리가 알고 싶은 이상적인 값. <img src="https://latex.codecogs.com/png.latex?V%5E*(%5Chat%7B%5Ctau%7D)%20=%20%7C%5Chat%7B%5Ctau%7D(X)%20-%20(Y(1)%20-%20Y(0))%7C"></li>
</ol>
<ul>
<li>이는 실제 ITE를 사용합니다.</li>
</ul></li>
<li><p><strong>문제점:</strong></p>
<ul>
<li>비록 Pseudo-outcome이 동일한 공변량 분포에서 나왔다 하더라도, <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">와 <img src="https://latex.codecogs.com/png.latex?V%5E*">는 서로 다른 변수입니다.</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">에 대해 CP를 적용해 얻은 커버리지 보장(<img src="https://latex.codecogs.com/png.latex?1-%5Calpha">)이 <img src="https://latex.codecogs.com/png.latex?V%5E*">에 그대로 적용된다는 보장이 없습니다.</li>
</ul></li>
<li><p>저자들은 이 간극을 메우기 위해 <strong>“우리의 점수(<img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">)가 Oracle 점수(<img src="https://latex.codecogs.com/png.latex?V%5E*">)보다 확률적으로 더 크거나 넓게 퍼져 있다면, 구간도 더 넓어질 것이므로 안전하다”</strong>라는 논리를 펼칩니다.</p></li>
</ul>
</section>
<section id="stochastic-ordering-framework" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-ordering-framework">Stochastic Ordering Framework</h2>
<ul>
<li>두 확률 변수의 크기나 변동성을 비교하기 위해 <strong>Stochastic Dominance(확률적 지배)</strong> 개념을 도입합니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?F">와 <img src="https://latex.codecogs.com/png.latex?G">를 각각 두 확률 변수의 누적 분포 함수(CDF)라고 합시다.</li>
</ul>
<section id="definition-1.1-first-order-stochastic-dominance-fosd" class="level3">
<h3 class="anchored" data-anchor-id="definition-1.1-first-order-stochastic-dominance-fosd">Definition 1.1: First-order Stochastic Dominance (FOSD)</h3>
<p><img src="https://latex.codecogs.com/png.latex?F%20%5Cge_%7B(1)%7D%20G%20%5Ciff%20F(x)%20%5Cle%20G(x),%20%5Cquad%20%5Cforall%20x"></p>
<ul>
<li><strong>의미:</strong> <img src="https://latex.codecogs.com/png.latex?F">의 CDF가 <img src="https://latex.codecogs.com/png.latex?G">보다 항상 아래에 있습니다. 이는 <img src="https://latex.codecogs.com/png.latex?F">에서 추출한 샘플이 <img src="https://latex.codecogs.com/png.latex?G">보다 <strong>확률적으로 더 큼</strong>을 의미합니다.</li>
<li><strong>직관:</strong> 모든 의사결정자가 <img src="https://latex.codecogs.com/png.latex?G">보다 <img src="https://latex.codecogs.com/png.latex?F">를 선호하는 상황입니다.</li>
</ul>
</section>
<section id="definition-1.2-second-order-stochastic-dominance-sosd" class="level3">
<h3 class="anchored" data-anchor-id="definition-1.2-second-order-stochastic-dominance-sosd">Definition 1.2: Second-order Stochastic Dominance (SOSD)</h3>
<p><img src="https://latex.codecogs.com/png.latex?F%20%5Cge_%7B(2)%7D%20G%20%5Ciff%20%5Cint_%7B-%5Cinfty%7D%5E%7Bx%7D%20%5BG(t)%20-%20F(t)%5D%20dt%20%5Cge%200,%20%5Cquad%20%5Cforall%20x"></p>
<ul>
<li><strong>의미:</strong> 위험 회피적(Risk-averse)인 관점에서 <img src="https://latex.codecogs.com/png.latex?F">가 <img src="https://latex.codecogs.com/png.latex?G">보다 선호되지 않는 상황입니다. 통계적으로는 <img src="https://latex.codecogs.com/png.latex?F">가 <img src="https://latex.codecogs.com/png.latex?G">보다 <strong>평균은 같더라도 분산(Spread)이 더 큼</strong>을 의미할 수 있습니다.</li>
</ul>
</section>
<section id="definition-2-monotone-convex-dominance-mcx" class="level3">
<h3 class="anchored" data-anchor-id="definition-2-monotone-convex-dominance-mcx">Definition 2: Monotone Convex Dominance (MCX)</h3>
<ul>
<li>두 확률분포 <img src="https://latex.codecogs.com/png.latex?F">와 <img src="https://latex.codecogs.com/png.latex?G"> 사이의 <strong>Monotone Convex Dominance (MCX)</strong> 관계는 다음과 같이 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?F%20%5Cge_%7Bmcx%7D%20G%20%5Ciff%20%5Cmathbb%7BE%7D_%7BX%20%5Csim%20F%7D%5Bu(X)%5D%20%5Cge%20%5Cmathbb%7BE%7D_%7BX%20%5Csim%20G%7D%5Bu(X)%5D"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?u:%20%5Cmathbb%7BR%7D%20%5Cto%20%5Cmathbb%7BR%7D">는 <strong>모든 비감소 볼록 함수(non-decreasing convex function)</strong>를 의미합니다.</li>
</ul>
<section id="ux의-역할과-직관적-의미" class="level4">
<h4 class="anchored" data-anchor-id="ux의-역할과-직관적-의미"><img src="https://latex.codecogs.com/png.latex?u(X)">의 역할과 직관적 의미</h4>
<ul>
<li><p>이 정의에서 <img src="https://latex.codecogs.com/png.latex?u(X)">는 단순한 함수가 아니라, 분포의 성질을 테스트하는 <strong>‘극단값 감지기(Extreme Value Detector)’</strong>로 이해해야 합니다.</p></li>
<li><ol type="1">
<li><strong>“비감소 (Non-decreasing)”의 의미:</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?u(x)">는 <img src="https://latex.codecogs.com/png.latex?x">가 커질수록 값도 커집니다.</li>
<li>이는 분포가 전체적으로 <strong>오른쪽(큰 값)</strong>으로 치우쳐 있는지를 검사합니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>“볼록 (Convex)”의 의미 (핵심):</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?u(x)">는 기울기가 점점 가파라지는 형태(예: <img src="https://latex.codecogs.com/png.latex?x%5E2,%20e%5Ex">)를 띱니다.</li>
<li>이는 평범한 값보다 <strong>꼬리 부분의 극단적인 값(Extreme outliers)</strong>에 훨씬 더 큰 가중치(Penalty)를 부여함을 뜻합니다.</li>
</ul></li>
<li><p>따라서, <strong>모든</strong> <img src="https://latex.codecogs.com/png.latex?u">에 대해 위 부등식이 성립한다는 것은 다음을 시사합니다:</p>
<ul>
<li>“<img src="https://latex.codecogs.com/png.latex?F">는 <img src="https://latex.codecogs.com/png.latex?G">에 비해 <strong>’엄청나게 큰 값’이 튀어나올 확률(Risk)</strong>이 더 높다.”</li>
</ul></li>
</ul>
</section>
<section id="cp에서의-함의-heavier-tails-safer-intervals" class="level4">
<h4 class="anchored" data-anchor-id="cp에서의-함의-heavier-tails-safer-intervals">CP에서의 함의: Heavier Tails &amp; Safer Intervals</h4>
<ul>
<li>이 수학적 정의는 Conformal Prediction에서 <strong>안전 장치</strong>가 됩니다.
<ul>
<li><strong>꼬리가 두껍다 (Heavier Tails):</strong> <img src="https://latex.codecogs.com/png.latex?F">(Pseudo-outcome 점수)는 <img src="https://latex.codecogs.com/png.latex?G">(실제 ITE 점수)보다 극단적인 오차가 발생할 빈도가 높습니다.</li>
<li><strong>분위수가 더 크다:</strong> 따라서 <img src="https://latex.codecogs.com/png.latex?F">의 상위 10% 지점(Quantile)은 <img src="https://latex.codecogs.com/png.latex?G">의 상위 10% 지점보다 <strong>더 멀리(크게)</strong> 형성됩니다.</li>
<li><strong>결론:</strong> <img src="https://latex.codecogs.com/png.latex?F">를 기준으로 설정한 예측 구간은 실제 분포 <img src="https://latex.codecogs.com/png.latex?G">를 커버하기에 <strong>충분히 넓고 보수적(Conservative)</strong>임이 보장됩니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/stochastic_dominance_illustration.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: 확률적 지배의 개념적 도식. (a) FOSD는 분포 자체가 오른쪽(큰 값)으로 이동한 형태이고, (b) SOSD/MCX는 분포가 더 넓게 퍼져 불확실성이 큰 형태를 띤다. [cite: 279]</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="theorem-1-general-validity-condition" class="level2">
<h2 class="anchored" data-anchor-id="theorem-1-general-validity-condition">Theorem 1: General Validity Condition</h2>
<ul>
<li>이 정의들을 바탕으로 저자들은 Conformal Meta-learner가 유효하기 위한 충분 조건을 제시합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem 1.</strong> 만약 Pseudo-outcome Conformity Score <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">가 Oracle Score <img src="https://latex.codecogs.com/png.latex?V%5E*">에 대해 다음 중 하나를 만족한다면:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cge_%7B(1)%7D%20V%5E*"> (FOSD)</li>
<li><img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cle_%7B(2)%7D%20V%5E*"> (SOSD의 역방향 조건)</li>
<li><img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cge_%7Bmcx%7D%20V%5E*"> (MCX)</li>
</ol>
<p>특정 범위의 <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cin%20(0,%20%5Calpha%5E*)">에 대하여, 다음 커버리지 보장이 성립한다. <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5Ctext%7BITE%7D%20%5Cin%20%5Chat%7BC%7D_%7B%5Cvarphi%7D(X))%20%5Cge%201%20-%20%5Calpha"></p>
</blockquote>
<ul>
<li><strong>해석:</strong>
<ul>
<li>우리가 사용하는 점수 <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">가 Oracle 점수 <img src="https://latex.codecogs.com/png.latex?V%5E*">보다 <strong>더 크거나(FOSD), 더 변동성이 크다면(MCX)</strong>, 우리가 설정한 분위수(Quantile) <img src="https://latex.codecogs.com/png.latex?Q_%7BV_%7B%5Cvarphi%7D%7D">는 Oracle 분위수 <img src="https://latex.codecogs.com/png.latex?Q_%7BV%5E*%7D">보다 크게 됩니다.</li>
<li>결과적으로 <strong>생성된 구간의 폭이 실제 필요한 폭보다 넓어지므로</strong>, 보수적인 관점에서 실제 ITE를 안전하게 포함하게 됩니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/validity_region_theorem1.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Theorem 1의 시각적 설명. Pseudo-score(점선)의 CDF가 Oracle-score(실선)보다 아래에 위치하거나 꼬리가 두꺼우면, 같은 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 레벨에서 더 큰 Threshold 값을 가지게 되어 Validity Region에 들어간다.</figcaption>
</figure>
</div>
</section>
<section id="theorem-2-which-meta-learners-are-valid" class="level2">
<h2 class="anchored" data-anchor-id="theorem-2-which-meta-learners-are-valid">Theorem 2: Which Meta-learners are Valid?</h2>
<ul>
<li>그렇다면 우리가 앞서 살펴본 3가지 Meta-learner(IPW, X, DR) 중 어떤 것이 이 조건을 만족할까요? 이것이 논문의 핵심 발견입니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem 2.</strong> Propensity score <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">가 정확히 알려져 있다고 가정할 때:</p>
<ol type="1">
<li><strong>X-learner:</strong> <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">와 <img src="https://latex.codecogs.com/png.latex?V%5E*"> 사이에 모델이나 분포와 무관한(Model-free distribution-free) 확률적 순서가 존재하지 않는다.</li>
<li><strong>IPW-learner &amp; DR-learner:</strong> 모든 데이터 분포와 Nuisance estimate에 대해 다음을 만족한다. <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cge_%7Bmcx%7D%20V%5E*"> 즉, <strong>Monotone Convex Dominance</strong>를 만족하여 유효성을 보장한다.</li>
</ol>
</blockquote>
<section id="why-ipw-dr-satisfy-mcx" class="level3">
<h3 class="anchored" data-anchor-id="why-ipw-dr-satisfy-mcx">Why IPW &amp; DR satisfy MCX?</h3>
<ul>
<li>IPW와 DR-learner가 <strong>MCX(<img src="https://latex.codecogs.com/png.latex?%5Cge_%7Bmcx%7D">)</strong> 조건을 만족하여 유효한 예측 구간을 생성할 수 있는 핵심 이유는 이들이 생성하는 Pseudo-outcome의 통계적 성질에 있습니다.</li>
</ul>
<section id="구조적-비편향성-structural-unbiasedness" class="level4">
<h4 class="anchored" data-anchor-id="구조적-비편향성-structural-unbiasedness">1. 구조적 비편향성 (Structural Unbiasedness)</h4>
<ul>
<li>이 두 모델의 가장 강력한 특징은 Nuisance parameter(<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D">)의 추정이 불안정하더라도, Propensity Score가 정확하다면 Pseudo-outcome <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">가 CATE에 대해 <strong>비편향(Unbiased)</strong>이라는 점입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%20%5Cmid%20X=x%5D%20=%20%5Ctau(x)"></p>
<ul>
<li>즉, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">는 실제 효과 <img src="https://latex.codecogs.com/png.latex?%5Ctau(x)">를 중심으로 하는 분포를 가집니다. 이를 수식으로 분해하면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%20=%20%5Ctau(x)%20+%20%5Cvarepsilon,%20%5Cquad%20%5Ctext%7Bwhere%20%7D%20%5Cmathbb%7BE%7D%5B%5Cvarepsilon%20%5Cmid%20X%5D%20=%200%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon">은 Propensity weighting(<img src="https://latex.codecogs.com/png.latex?1/%5Cpi">)에 의해 증폭된 노이즈입니다.</li>
</ul>
</section>
<section id="젠센-부등식을-통한-mcx-유도-derivation-via-jensens-inequality" class="level4">
<h4 class="anchored" data-anchor-id="젠센-부등식을-통한-mcx-유도-derivation-via-jensens-inequality">2. 젠센 부등식을 통한 MCX 유도 (Derivation via Jensen’s Inequality)</h4>
<ul>
<li><p>이 비편향성 덕분에 우리는 <strong>젠센 부등식(Jensen’s Inequality)</strong>을 적용하여 MCX를 증명할 수 있습니다.</p></li>
<li><p><strong>가정:</strong> <img src="https://latex.codecogs.com/png.latex?u(%5Ccdot)">를 임의의 <strong>비감소 볼록 함수(Non-decreasing Convex Function)</strong>라고 합시다.</p></li>
<li><p><strong>비교:</strong> 우리의 목표는 가짜 점수(<img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D">)의 기대 손실이 진짜 점수(<img src="https://latex.codecogs.com/png.latex?Y(1)-Y(0)">)보다 큼을 보이는 것입니다.</p></li>
<li><p>Pseudo-outcome은 구조적으로 <strong>“평균 보존 확산(Mean-Preserving Spread)”</strong>의 형태를 띱니다. 즉, 중심(Mean)은 진짜 효과 <img src="https://latex.codecogs.com/png.latex?%5Ctau(x)">에 고정된 채로, 인위적인 노이즈 <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon">이 더해져 분산만 커진 상태입니다.</p></li>
<li><p>볼록 함수 <img src="https://latex.codecogs.com/png.latex?u">의 성질에 의해, <strong>변동성(Spread)이 커질수록 기댓값은 증가</strong>합니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D%5Bu(%5Ctilde%7BY%7D_%7B%5Cvarphi%7D)%20%5Cmid%20X%5D%20&amp;=%20%5Cmathbb%7BE%7D%5Bu(%5Ctau(x)%20+%20%5Cvarepsilon)%20%5Cmid%20X%5D%20%5C%5C%0A&amp;%5Cge%20u(%5Cmathbb%7BE%7D%5B%5Ctau(x)%20+%20%5Cvarepsilon%20%5Cmid%20X%5D)%20%5Cquad%20(%5Cbecause%20%5Ctext%7BJensen's%20Inequality%7D)%20%5C%5C%0A&amp;=%20u(%5Ctau(x))%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>논문의 증명(Theorem 2 Proof)에 따르면, IPW/DR의 구조적 노이즈 분산은 실제 ITE의 분산보다 크거나 같습니다. 따라서 다음이 성립합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D_%7BX%20%5Csim%20%5Ctext%7BPseudo%7D%7D%5Bu(%5Ctext%7BScore%7D)%5D%20%5Cge%20%5Cmathbb%7BE%7D_%7BX%20%5Csim%20%5Ctext%7BTrue%7D%7D%5Bu(%5Ctext%7BScore%7D)%5D%0A"></p>
</section>
</section>
<section id="why-x-learner-fails" class="level3">
<h3 class="anchored" data-anchor-id="why-x-learner-fails">Why X-learner fails?</h3>
<ul>
<li>반면 X-learner의 Pseudo-outcome은 다음과 같습니다. <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%20=%20W(Y-%5Chat%7B%5Cmu%7D_0)%20+%20(1-W)(%5Chat%7B%5Cmu%7D_1-Y)"></li>
<li>이 식은 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D_0,%20%5Chat%7B%5Cmu%7D_1"> 추정이 완벽하지 않다면 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Ctilde%7BY%7D_%7B%5Cvarphi%7D%7CX%5D%20%5Cneq%20%5Ctau(x)">일 수 있습니다.</li>
<li>또한 Propensity score를 이용해 이 오차를 보정하는 구조가 아니기 때문에, 분포에 따라 <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">가 <img src="https://latex.codecogs.com/png.latex?V%5E*">보다 작아질 위험(Under-coverage)이 존재합니다.</li>
</ul>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Meta-learner</th>
<th style="text-align: left;">Conformity Score Order</th>
<th style="text-align: left;">Validity Guarantee</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>X-learner</strong></td>
<td style="text-align: left;">No stochastic order</td>
<td style="text-align: left;">❌ Not Guaranteed</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>IPW-learner</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cge_%7Bmcx%7D%20V%5E*"></td>
<td style="text-align: left;">✅ Valid</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>DR-learner</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D%20%5Cge_%7Bmcx%7D%20V%5E*"></td>
<td style="text-align: left;">✅ Valid</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="limitations-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-discussion">Limitations and Discussion</h2>
<ul>
<li>이 이론은 강력하지만, 저자들은 두 가지 현실적인 한계를 솔직하게 인정합니다.</li>
</ul>
<section id="limitation-1-known-propensity-score-assumption" class="level3">
<h3 class="anchored" data-anchor-id="limitation-1-known-propensity-score-assumption">Limitation 1: Known Propensity Score Assumption</h3>
<ul>
<li>Theorem 2의 보장은 Propensity score <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">를 정확히 알고 있다는 가정하에 성립합니다.</li>
<li>RCT(무작위 대조 실험) 데이터라면 <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">가 설계에 의해 주어지므로 문제가 없습니다.</li>
<li>하지만 관측 데이터(Observational Data)라면 <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">를 추정해야 하며, 추정 오차가 발생할 경우 이론적 보장이 약화될 수 있습니다. (다만 이는 Weighted CP 등 다른 방법론들도 공유하는 문제입니다)</li>
</ul>
</section>
<section id="limitation-2-unknown-alpha" class="level3">
<h3 class="anchored" data-anchor-id="limitation-2-unknown-alpha">Limitation 2: Unknown <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E*"></h3>
<ul>
<li>Theorem 1은 “특정 <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E*"> 이하의 <img src="https://latex.codecogs.com/png.latex?%5Calpha">에 대해 유효하다”라고 말합니다.</li>
<li>하지만 이 임계값 <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E*">가 정확히 얼마인지는 데이터 분포에 따라 다르며 사전에 알기 어렵습니다.</li>
<li>저자들은 이를 이론적으로 유도하는 것을 Future Work로 남겨두고, 실험적으로 이를 검증하는 방식을 택했습니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="experiments" class="level1">
<h1>5. Experiments</h1>
<ul>
<li><p>지난 포스트들에서는 <strong>Conformal Meta-learners</strong>의 이론적 배경과 <strong>Stochastic Ordering</strong>을 통한 유효성 증명을 살펴보았습니다.</p></li>
<li><p>이론적으로 IPW-learner와 DR-learner는 유효함이 증명되었고, X-learner는 보장할 수 없다는 결론을 얻었습니다.</p></li>
<li><p>이번 마지막 포스트에서는 이러한 이론적 결과가 <strong>실제 데이터(Synthetic &amp; Semi-synthetic)</strong> 실험에서도 나타나는지 확인하고, 경쟁 모델(Baselines)과 비교하여 어떤 성능 차이를 보이는지 분석합니다.</p></li>
</ul>
<section id="experimental-setup" class="level2">
<h2 class="anchored" data-anchor-id="experimental-setup">5.1. Experimental Setup</h2>
<ul>
<li>인과추론 실험의 가장 큰 어려움은 <strong>Ground Truth (실제 ITE)</strong>를 관측할 수 없다는 점입니다.</li>
<li>따라서 저자들은 실제 공변량을 사용하되 결과값은 시뮬레이션하는 방식 등을 도입했습니다.</li>
</ul>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<ul>
<li><p>실험은 크게 두 가지 환경에서 진행되었습니다.</p></li>
<li><ol type="1">
<li><strong>Synthetic Datasets (완전 합성 데이터):</strong></li>
</ol>
<ul>
<li>공변량 <img src="https://latex.codecogs.com/png.latex?X">와 처치 <img src="https://latex.codecogs.com/png.latex?W">를 모두 생성.</li>
<li><strong>Heteroscedastic Noise Model:</strong> 오차의 분산이 공변량에 따라 달라지는 <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2(x)%20=%20-%5Clog(x_1)"> 모델을 사용하여 불확실성 추정의 난이도를 높였습니다.</li>
<li><strong>Setup A:</strong> 처치 효과가 없는 경우 (<img src="https://latex.codecogs.com/png.latex?%5Czeta=1">).</li>
<li><strong>Setup B:</strong> 이질적 처치 효과(Heterogeneous effects)가 존재하는 경우 (<img src="https://latex.codecogs.com/png.latex?%5Czeta=0">).</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Semi-synthetic Datasets (준합성 데이터):</strong></li>
</ol>
<ul>
<li><strong>IHDP:</strong> 영유아 건강 발달 프로그램 데이터.</li>
<li><strong>NLSM:</strong> 국립 학습 사고 방식 연구 데이터.</li>
<li>실제 공변량을 사용하되, 결과 변수(Outcome)는 시뮬레이션하여 <img src="https://latex.codecogs.com/png.latex?Y(1)">과 <img src="https://latex.codecogs.com/png.latex?Y(0)">를 모두 알 수 있게 설정했습니다.</li>
</ul></li>
</ul>
</section>
<section id="baselines-비교-모델" class="level3">
<h3 class="anchored" data-anchor-id="baselines-비교-모델">Baselines (비교 모델)</h3>
<ul>
<li><p>Conformal Meta-learner(CM)와 비교할 대상은 <strong>Weighted Conformal Prediction (WCP)</strong> 계열의 방법론들입니다.</p></li>
<li><p><strong>Naïve WCP:</strong> <img src="https://latex.codecogs.com/png.latex?Y(0)">와 <img src="https://latex.codecogs.com/png.latex?Y(1)"> 각각에 대해 구간을 구한 뒤, Bonferroni correction을 이용해 결합합니다. (보수적임)</p></li>
<li><p><strong>Exact Nested WCP:</strong> ITE의 Plug-in 추정치에 대해 WCP를 적용하고, 2차 CP 절차를 수행합니다. (유효성 보장됨)</p></li>
<li><p><strong>Inexact Nested WCP:</strong> Exact 방식에서 2차 CP 대신 Quantile Regression을 사용합니다. (유효성 보장 안 됨)</p></li>
<li><p><strong>CM Variants:</strong> 본 논문의 제안 방법론 (CM-IPW, CM-DR, CM-X).</p></li>
<li><p>모든 모델은 기본 예측기(Base Learner)로 <strong>Gradient Boosting</strong>을 사용했습니다.</p></li>
</ul>
<hr>
</section>
</section>
<section id="results-and-discussion" class="level2">
<h2 class="anchored" data-anchor-id="results-and-discussion">5.2. Results and Discussion</h2>
<section id="key-result-1-empirical-stochastic-orders" class="level3">
<h3 class="anchored" data-anchor-id="key-result-1-empirical-stochastic-orders">Key Result 1: Empirical Stochastic Orders</h3>
<ul>
<li><p>가장 먼저 확인해야 할 것은 <strong>Theorem 1 &amp; 2</strong>의 이론적 예측이 실제 데이터 분포에서 성립하는지 여부입니다.</p></li>
<li><p>우리는 <strong>Pseudo-outcome Conformity Score (<img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">)</strong>의 누적 분포 함수(CDF)가 <strong>Oracle Score (<img src="https://latex.codecogs.com/png.latex?V%5E*">)</strong>의 CDF보다 아래에 위치(FOSD)하거나, 더 완만하게 증가(MCX)하기를 기대합니다.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/figure4_synthetic_results.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: 합성 데이터 실험 결과. (a) Conformity Score들의 CDF 비교. 파란색 실선(<img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">)이 빨간색 점선(<img src="https://latex.codecogs.com/png.latex?V%5E*">)보다 우측에 위치하면 확률적으로 더 큰 값(FOSD)임을 의미한다. DR과 IPW는 이를 만족하지만, X-learner는 반대 경향을 보인다.</figcaption>
</figure>
</div>
<section id="분석-결과" class="level4">
<h4 class="anchored" data-anchor-id="분석-결과">분석 결과</h4>
<ul>
<li><strong>IPW &amp; DR-learner:</strong>
<ul>
<li>그래프 (a)에서 파란색 선(<img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">)이 빨간색 선(<img src="https://latex.codecogs.com/png.latex?V%5E*">)보다 <strong>항상 우측 하단</strong>에 위치합니다.</li>
<li>이는 <strong>First-Order Stochastic Dominance (FOSD)</strong>를 만족함을 보여줍니다.</li>
<li>FOSD는 이론적으로 요구되었던 MCX보다 훨씬 강력한 조건입니다. 즉, 이 모델들은 매우 안정적으로 유효한 구간을 생성합니다.</li>
</ul></li>
<li><strong>X-learner:</strong>
<ul>
<li>반대로 파란색 선이 빨간색 선보다 좌측 상단에 위치합니다.</li>
<li>이는 Pseudo-score가 실제 오차보다 과소평가됨을 의미하며, <strong>Under-coverage(커버리지 미달)</strong>로 이어질 것임을 예고합니다.</li>
</ul></li>
</ul>
</section>
</section>
<section id="key-result-2-performance-comparison" class="level3">
<h3 class="anchored" data-anchor-id="key-result-2-performance-comparison">Key Result 2: Performance Comparison</h3>
<ul>
<li>이제 실제 <strong>커버리지(Coverage)</strong>, <strong>구간 길이(Efficiency)</strong>, <strong>정확도(RMSE)</strong> 측면에서 성능을 비교해봅시다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/images/figure5_semisynthetic_results.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: 준합성 데이터(IHDP, NLSM)에서의 확률적 지배 양상. DR-learner와 IPW-learner는 Oracle Score를 지배(Dominance)하는 패턴을 유지한다.</figcaption>
</figure>
</div>
<section id="coverage-유효성" class="level4">
<h4 class="anchored" data-anchor-id="coverage-유효성">Coverage (유효성)</h4>
<ul>
<li><strong>CM-DR, CM-IPW:</strong> 목표 수준인 90% (<img src="https://latex.codecogs.com/png.latex?1-%5Calpha=0.9">)를 안정적으로 달성하거나 상회했습니다.</li>
<li><strong>CM-X:</strong> 예상대로 목표 커버리지에 도달하지 못했습니다 (Under-coverage).</li>
<li><strong>Baselines:</strong> Naïve와 Exact WCP는 유효했으나, Inexact WCP는 커버리지를 보장하지 못했습니다.</li>
</ul>
</section>
<section id="efficiency-구간-길이-rmse" class="level4">
<h4 class="anchored" data-anchor-id="efficiency-구간-길이-rmse">Efficiency (구간 길이) &amp; RMSE</h4>
<ul>
<li><strong>CM-DR (Winner):</strong> DR-learner는 유효한 모델들 중에서 <strong>가장 짧은 구간 길이(Interval Length)</strong>와 <strong>가장 낮은 RMSE</strong>를 기록했습니다. 즉, <strong>“정답을 포함하면서도 가장 타이트한 구간”</strong>을 제공했습니다.</li>
<li><strong>Naïve WCP:</strong> 유효하지만 구간이 너무 넓어 실용성이 떨어집니다.</li>
<li><strong>CM-X:</strong> RMSE는 가장 낮았으나(점 추정은 정확함), 구간 추정이 틀렸기 때문에 신뢰할 수 없습니다.</li>
</ul>
</section>
<section id="why-is-dr-better-than-ipw" class="level4">
<h4 class="anchored" data-anchor-id="why-is-dr-better-than-ipw">Why is DR better than IPW?</h4>
<ul>
<li>CM-IPW와 CM-DR 모두 유효하지만, CM-DR의 구간이 더 좁고 효율적입니다. 그 이유는 <strong>CDF의 차이(Gap)</strong>에 있습니다.</li>
<li>IPW의 Pseudo-outcome은 분산이 매우 큽니다. 이로 인해 Conformity score가 지나치게 커져, Oracle score와의 격차(Gap)가 벌어집니다.</li>
<li>반면, DR-learner는 Regression adjustment 항 덕분에 Pseudo-outcome의 분산이 상대적으로 작습니다. 결과적으로 <img src="https://latex.codecogs.com/png.latex?V_%7B%5Cvarphi%7D">의 분포가 <img src="https://latex.codecogs.com/png.latex?V%5E*">의 분포에 더 가깝게 밀착(Tight bound)되어, 불필요하게 넓은 구간을 만들지 않습니다.</li>
</ul>
<hr>
</section>
</section>
</section>
</section>
<section id="conclusions" class="level1">
<h1>6. Conclusions</h1>
<ul>
<li>이 논문(<strong>Conformal Meta-learners for Predictive Inference of Individual Treatment Effects</strong>)은 인과추론의 난제인 ITE 구간 추정 문제를 해결하기 위해, Conformal Prediction과 Meta-learner를 결합한 새로운 프레임워크를 제안했습니다.</li>
</ul>
<section id="핵심-요약" class="level2">
<h2 class="anchored" data-anchor-id="핵심-요약">핵심 요약</h2>
<ul>
<li><ol type="1">
<li><strong>Framework:</strong> Pseudo-outcome을 정의하여 CATE 추정 문제를 회귀 문제로 변환하고, 그 위에 CP를 적용하여 모델에 구애받지 않는(Model-agnostic) 구간 추정 방법을 제시했습니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Theory:</strong> Pseudo-score와 Oracle-score 간의 <strong>Stochastic Ordering (확률적 지배)</strong> 조건을 정립하여, 어떤 상황에서 구간이 유효한지 수학적으로 증명했습니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Findings:</strong></li>
</ol>
<ul>
<li><strong>DR-learner</strong>와 <strong>IPW-learner</strong>는 이론적/실험적으로 유효성(Validity)이 보장됩니다.</li>
<li>특히 <strong>DR-learner</strong>는 가장 효율적인(좁은) 구간을 제공하여 실용적으로 가장 우수합니다.</li>
<li><strong>X-learner</strong>는 점 추정 성능은 좋으나, 불확실성 추정에는 적합하지 않을 수 있습니다.</li>
</ul></li>
</ul>
</section>
<section id="future-work" class="level2">
<h2 class="anchored" data-anchor-id="future-work">Future Work</h2>
<ul>
<li>Propensity score를 모를 때의 불확실성 반영.</li>
<li>이론적인 최적 <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E*"> 값의 유도.</li>
<li>Stochastic order를 유지하면서도 효율성을 극대화하는 새로운 Pseudo-outcome 변환법 연구.</li>
</ul>



</section>
</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/Conformal Meta-learners for Predictive Inference of Individual Treatment Effects/</guid>
  <pubDate>Thu, 29 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 15A. SDiD (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-01/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<ul>
<li><p>인과 추론(Causal Inference)에서 패널 데이터(Panel Data)를 분석할 때 가장 널리 쓰이는 두 가지 방법론은 <strong>이중차분법(Difference-in-Differences, DiD)</strong>과 <strong>통제집단합성법(Synthetic Control Method, SCM)</strong>입니다.</p></li>
<li><p>하지만 이 두 방법론은 각각의 한계점을 가지고 있습니다.</p>
<ul>
<li>DiD는 엄격한 ’평행 추세 가정(Parallel Trends Assumption)’에 의존합니다.</li>
<li>SCM은 처치 유닛(treated unit)과 대조 유닛(control unit)의 레벨(level)을 강제로 맞추려고 합니다.</li>
</ul></li>
<li><p>이번 포스트에서는 Arkhangelsky et al.&nbsp;(2021)이 제안한 <strong>Synthetic Difference-in-Differences (SDiD)</strong>를 다룹니다.</p></li>
<li><p>SDiD는 DiD와 SCM의 장점을 결합하여, <strong>가중치(weights)</strong>를 통해 평행 추세를 보정하고 이원 고정 효과(Two-way Fixed Effects)를 통해 강건성(robustness)을 확보하는 방법론입니다.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-01/images/did_sc_sdid_comparison.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: DiD, SC, SDiD의 추정 방식 비교. (좌) DiD는 전체 대조군의 평균 추세를 평행 이동하여 반사실(counterfactual)을 추정함. (중) SC는 처치 이전 기간의 outcome 레벨까지 정확히 일치시키는 가중치를 찾음. (우) SDiD는 추세(trend)만 평행하게 맞추면 되며(절편 허용), 가중치를 통해 평행 추세 가정을 만족시키는 대조군을 합성함.</figcaption>
</figure>
</div>
<hr>
</section>
<section id="recap-fixed-effects" class="level1">
<h1>2. Recap: Fixed Effects</h1>
<ul>
<li>SDiD는 기존 방법론들의 한계를 극복하기 위해 등장했습니다.</li>
<li>이를 이해하기 위해서는 먼저 <strong>고정 효과(Fixed Effects)</strong>의 개념을 이해해야 합니다.</li>
<li>패널 데이터 분석에서 관측되지 않는 이질성(Unobserved Heterogeneity)을 통제하는 것은 인과 추론의 핵심입니다.</li>
<li>이를 위해 우리는 주로 <strong>이원 고정 효과(Two-Way Fixed Effects, TWFE)</strong> 모델을 사용합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20%5Cmu%20+%20%5Calpha_i%20+%20%5Cbeta_t%20+%20%5Ctau%20X_%7Bit%7D%20+%20%5Cvarepsilon_%7Bit%7D%0A"></p>
<ul>
<li><strong>Unit Fixed Effect (<img src="https://latex.codecogs.com/png.latex?%5Calpha_i">)</strong>:
<ul>
<li>개별 유닛 <img src="https://latex.codecogs.com/png.latex?i">가 고유하게 가지는 특성으로, <strong>시간이 지나도 변하지 않는(Time-invariant)</strong> 요인입니다.</li>
<li>e.g.&nbsp;국가의 지리적 위치, 회사의 기업 문화</li>
</ul></li>
<li><strong>Time Fixed Effect (<img src="https://latex.codecogs.com/png.latex?%5Cbeta_t">)</strong>:
<ul>
<li>특정 시점 <img src="https://latex.codecogs.com/png.latex?t">에 모든 유닛에게 공통적으로 영향을 미치는 충격으로, <strong>유닛 간에 차이가 없는(Unit-invariant)</strong> 요인입니다.</li>
<li>e.g.&nbsp;거시경제적 쇼크, 팬데믹</li>
</ul></li>
<li>SDiD의 핵심 동기는 기존의 방법론들이 이 <img src="https://latex.codecogs.com/png.latex?%5Calpha_i">와 <img src="https://latex.codecogs.com/png.latex?%5Cbeta_t">를 다루는 방식에서 각각 장단점이 뚜렷하다는 점에 착안합니다.</li>
</ul>
<hr>
</section>
<section id="synthetic-difference-in-differences-sdid" class="level1">
<h1>3. Synthetic Difference-in-Differences (SDiD)</h1>
<ul>
<li>SDiD는 SCM처럼 처치 이전(pre-treatment) 추세를 맞추기 위해 재가중(reweighting)을 수행하면서도, DiD처럼 유닛 수준의 레벨 차이(additive unit-level shifts)에 불변(invariant)하도록 설계되었습니다.</li>
</ul>
<section id="setting-notation" class="level2">
<h2 class="anchored" data-anchor-id="setting-notation">3.1. Setting &amp; Notation</h2>
<ul>
<li>SDiD 모델을 이해하기 위한 기본적인 데이터 구조와 파라미터 정의는 다음과 같습니다.</li>
</ul>
<section id="data-structure" class="level3">
<h3 class="anchored" data-anchor-id="data-structure">Data Structure</h3>
<ul>
<li><strong>Balanced Panel:</strong> <img src="https://latex.codecogs.com/png.latex?N">개의 유닛과 <img src="https://latex.codecogs.com/png.latex?T">개의 시간(periods)으로 구성된 균형 패널 데이터를 가정합니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D">: 유닛 <img src="https://latex.codecogs.com/png.latex?i">, 시간 <img src="https://latex.codecogs.com/png.latex?t">에서의 결과 변수(Outcome).</li>
<li><img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D%20%5Cin%20%5C%7B0,%201%5C%7D">: 이진 처치 여부(Binary treatment indicator).</li>
<li><strong>Units Breakdown:</strong>
<ul>
<li><strong>Control Units (<img src="https://latex.codecogs.com/png.latex?N_%7Bco%7D">):</strong> <img src="https://latex.codecogs.com/png.latex?i%20=%201,%20%5Cdots,%20N_%7Bco%7D"> (처치를 전혀 받지 않음).</li>
<li><strong>Treated Units (<img src="https://latex.codecogs.com/png.latex?N_%7Btr%7D">):</strong> <img src="https://latex.codecogs.com/png.latex?i%20=%20N_%7Bco%7D%20+%201,%20%5Cdots,%20N"> (시간 <img src="https://latex.codecogs.com/png.latex?T_%7Bpre%7D"> 이후 처치를 받음).</li>
<li><img src="https://latex.codecogs.com/png.latex?N_%7Btr%7D%20=%20N%20-%20N_%7Bco%7D"></li>
</ul></li>
</ul>
</section>
<section id="model-parameters-fixed-effects" class="level3">
<h3 class="anchored" data-anchor-id="model-parameters-fixed-effects">Model Parameters (Fixed Effects)</h3>
<ul>
<li>기존 DiD와 마찬가지로 Two-Way Fixed Effects(TWFE) 구조를 기반으로 합니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu">: <strong>Global Intercept (전체 절편)</strong>.
<ul>
<li>모든 유닛과 시점에 공통적으로 적용되는 기저 수준(Base level)입니다.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha_i">: <strong>Unit Fixed Effects (유닛 고정 효과)</strong>.
<ul>
<li>유닛 <img src="https://latex.codecogs.com/png.latex?i">가 가진 고유한 특성으로, <strong>시간에 따라 변하지 않는(Time-invariant)</strong> 이질성을 포착합니다.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta_t">: <strong>Time Fixed Effects (시간 고정 효과)</strong>.
<ul>
<li>특정 시점 <img src="https://latex.codecogs.com/png.latex?t">에 모든 유닛에게 공통적으로 영향을 미치는 충격으로, <strong>유닛 간에 차이가 없는(Unit-invariant)</strong> 요인입니다.</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="sdid-weights" class="level3">
<h3 class="anchored" data-anchor-id="sdid-weights">SDiD Weights</h3>
<ul>
<li>SDiD는 위 파라미터를 추정하기 전에, 데이터의 균형을 맞추기 위한 두 가지 가중치를 먼저 계산합니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Comega_i">: <strong>Unit Weights (유닛 가중치)</strong>.
<ul>
<li>처치군의 <strong>추세(Trend)</strong>와 유사한 대조군을 합성하기 위해 대조 유닛들에 부여하는 가중치입니다.</li>
</ul></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda_t">: <strong>Time Weights (시간 가중치)</strong>.
<ul>
<li>처치 이전 기간(Pre-treatment) 중, 처치 이후 기간(Post-treatment)과 유사한 시점을 강조하기 위해 부여하는 가중치입니다.</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="formulations-comparison" class="level2">
<h2 class="anchored" data-anchor-id="formulations-comparison">3.2. Formulations Comparison</h2>
<ul>
<li>세 가지 추정량(Estimator)은 <strong>최적화 문제(Minimization Problem)</strong>로 정식화하여 비교할 때 그 차이가 명확해집니다.</li>
<li>우리는 평균 인과 효과 <img src="https://latex.codecogs.com/png.latex?%5Ctau">를 추정하고자 합니다.</li>
</ul>
<section id="did-estimator" class="level3">
<h3 class="anchored" data-anchor-id="did-estimator">(1) DiD Estimator</h3>
<ul>
<li><p>DiD는 <strong>평행 추세 가정(Parallel Trends Assumption)</strong>에 기반하여 인과 효과를 추정하며, 수식적으로 <strong>Unit Fixed Effect (<img src="https://latex.codecogs.com/png.latex?%5Calpha_i">)를 허용</strong>합니다.</p></li>
<li><p><strong>Mechanism</strong></p>
<ul>
<li>처치군과 대조군 사이에 레벨(Level) 차이가 있더라도, 그 차이가 시불변(<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Btr%7D%20%5Cneq%20%5Calpha_%7Bco%7D">)한다면, 변화량의 차이(Difference-in-Differences)를 구하는 과정에서 <img src="https://latex.codecogs.com/png.latex?%5Calpha_i">가 상쇄되어 사라집니다.</li>
</ul></li>
<li><p><strong>Formulation</strong></p>
<ul>
<li>회귀분석 관점에서 DiD는 <strong>가중치 없이(unweighted)</strong> TWFE 문제를 푸는 것과 같습니다. <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Chat%7B%5Ctau%7D%5E%7BDiD%7D%20=%20%5Cunderset%7B%5Calpha,%20%5Cbeta,%20%5Cmu,%20%5Ctau%7D%7B%5Carg%20%5Cmin%7D%20%5Cleft%5C%7B%20%5Csum_%7Bi=1%7D%5E%7BN%7D%5Csum_%7Bt=1%7D%5E%7BT%7D%20(Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Calpha_i%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D)%5E2%20%5Cright%5C%7D%0A%20%20"></li>
</ul></li>
<li><p><strong>Limitation</strong></p>
<ul>
<li>모든 대조군 유닛에 동일한 가중치(<img src="https://latex.codecogs.com/png.latex?1/N_%7Bco%7D">)를 부여합니다.</li>
<li>따라서 평행 추세 가정이 위배되는(추세가 다른) 유닛들이 대조군에 섞여 있을 경우 편향(Bias)이 발생할 수 있습니다.</li>
</ul></li>
</ul>
</section>
<section id="scm-estimator" class="level3">
<h3 class="anchored" data-anchor-id="scm-estimator">(2) SCM Estimator</h3>
<ul>
<li><p>SCM은 처치 유닛과 가장 유사한 가상의 대조군(Synthetic Control)을 만들기 위해 대조군 유닛들에 <strong>가중치(Unit weights, <img src="https://latex.codecogs.com/png.latex?%5Comega_i">)</strong>를 부여합니다.</p></li>
<li><p><strong>Mechanism</strong></p>
<ul>
<li>SCM은 처치 이전 기간의 결과 변수(<img src="https://latex.codecogs.com/png.latex?Y">)의 <strong>경로(Path)와 레벨(Level)</strong>을 모두 맞추려고 시도합니다.</li>
<li>즉, 추세뿐만 아니라 절대적인 수치까지 일치시키려 합니다.</li>
</ul></li>
<li><p><strong>Formulation</strong></p>
<ul>
<li>전통적인 SCM 추정식은 시간 고정 효과(<img src="https://latex.codecogs.com/png.latex?%5Cbeta_t">)는 포함하지만, <strong>Unit Fixed Effect (<img src="https://latex.codecogs.com/png.latex?%5Calpha_i">)와 전체 절편(Intercept)을 제외</strong>합니다. <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Chat%7B%5Ctau%7D%5E%7BSCM%7D%20=%20%5Cunderset%7B%5Cbeta,%20%5Cmu,%20%5Ctau%7D%7B%5Carg%20%5Cmin%7D%20%5Cleft%5C%7B%20%5Csum_%7Bi=1%7D%5E%7BN%7D%5Csum_%7Bt=1%7D%5E%7BT%7D%20(Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D)%5E2%20%5Ccdot%20%5Chat%7B%5Comega%7D_i%20%5Cright%5C%7D%0A%20%20"></li>
</ul></li>
<li><p><strong>Limitation</strong></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha_i">를 모델에 포함하지 않기 때문에, SCM은 처치군과 대조군 간의 레벨 차이(Intercept shift)를 허용하지 않습니다.</li>
<li>즉, <strong>“평행 이동”을 허용하지 않고 절대적인 수치까지 맞춰야 하므로</strong>, 완벽하게 일치하는 대조군을 찾지 못하면(Interpolation bias) 추정 성능이 떨어질 수 있습니다.</li>
</ul></li>
</ul>
</section>
<section id="sdid-estimator" class="level3">
<h3 class="anchored" data-anchor-id="sdid-estimator">(3) SDiD Estimator</h3>
<ul>
<li><p>SDiD는 <strong>유닛 가중치 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Comega%7D_i"></strong>와 <strong>시간 가중치 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D_t"></strong>를 모두 사용하며, 동시에 <strong>유닛 및 시간 고정 효과(<img src="https://latex.codecogs.com/png.latex?%5Calpha_i,%20%5Cbeta_t">)</strong>를 모두 포함합니다.</p></li>
<li><p><strong>Mechanism</strong></p>
<ul>
<li><strong>Local Regression:</strong> 가중치(<img src="https://latex.codecogs.com/png.latex?%5Comega,%20%5Clambda">)를 통해 처치군과 유사한 대조군, 처치 시점과 유사한 시점을 강조합니다.</li>
<li><strong>Robustness:</strong> 고정 효과(<img src="https://latex.codecogs.com/png.latex?%5Calpha,%20%5Cbeta">)를 통해 가중치로 설명되지 않는 시스템적인 차이(Systematic differences)를 제거합니다.</li>
</ul></li>
<li><p><strong>Formulation</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Chat%7B%5Ctau%7D%5E%7BSDiD%7D%20=%20%5Cunderset%7B%5Calpha,%20%5Cbeta,%20%5Cmu,%20%5Ctau%7D%7B%5Carg%20%5Cmin%7D%20%5Cleft%5C%7B%20%5Csum_%7Bi=1%7D%5E%7BN%7D%5Csum_%7Bt=1%7D%5E%7BT%7D%20(Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Calpha_i%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D)%5E2%20%5Ccdot%20%5Chat%7B%5Comega%7D_i%20%5Ccdot%20%5Chat%7B%5Clambda%7D_t%20%5Cright%5C%7D%0A%20%20"></p></li>
<li><p><strong>Key Advantage</strong></p>
<ul>
<li>SCM의 유연성(Flexibility)과 DiD의 강건성(Robustness)을 결합하여, 레벨이 달라도 추세가 유사한 유닛들을 효과적으로 매칭하고 편향을 줄입니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="summary-comparison-of-methodologies" class="level2">
<h2 class="anchored" data-anchor-id="summary-comparison-of-methodologies">Summary: Comparison of Methodologies</h2>
<ul>
<li>아래 표는 DiD, SCM, 그리고 SDiD가 고정 효과(Fixed Effect)와 가중치(Weights)를 다루는 방식의 핵심적인 차이를 요약합니다.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">구분</th>
<th style="text-align: left;"><strong>DiD</strong></th>
<th style="text-align: left;"><strong>SCM</strong></th>
<th style="text-align: left;"><strong>SDiD</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Unit Fixed Effect (<img src="https://latex.codecogs.com/png.latex?%5Calpha_i">)</strong></td>
<td style="text-align: left;"><strong>포함 (Included)</strong><br>레벨 차이(Intercept) 허용</td>
<td style="text-align: left;"><strong>미포함 (Excluded)</strong><br>절편 허용 안 함</td>
<td style="text-align: left;"><strong>포함 (Included)</strong><br>레벨 차이(Intercept) 허용</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>가중치 (Weights)</strong></td>
<td style="text-align: left;"><strong>없음 (Unweighted)</strong><br>모든 대조군 동일 가중치</td>
<td style="text-align: left;"><strong>Unit Weights (<img src="https://latex.codecogs.com/png.latex?%5Comega_i">)</strong><br>유사한 유닛에 가중치 부여</td>
<td style="text-align: left;"><strong>Unit(<img src="https://latex.codecogs.com/png.latex?%5Comega_i">) + Time(<img src="https://latex.codecogs.com/png.latex?%5Clambda_t">)</strong><br>유사한 유닛 및 시점 강조</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>매칭 조건 (Matching)</strong></td>
<td style="text-align: left;"><strong>추세 (Trend)</strong>만 평행하면 됨</td>
<td style="text-align: left;"><strong>레벨 (Level)</strong>까지 정확히 일치해야 함</td>
<td style="text-align: left;"><strong>추세 (Trend)</strong>만 평행하면 됨</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>장점과 한계</strong></td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?%5Calpha_i">를 허용하지만,<br>가중치가 없어 유연성 부족</td>
<td style="text-align: left;">가중치로 유연하게 맞추지만,<br>레벨까지 맞춰야 하는 제약 존재</td>
<td style="text-align: left;"><strong>SCM의 유연성(가중치)</strong>과<br><strong>DiD의 강건성(FE)</strong>을 결합</td>
</tr>
</tbody>
</table>
<ul>
<li>결론적으로 <strong>SDiD</strong>는 SCM처럼 가중치를 사용하여 데이터에 유연하게 적합(Fit)시키면서도, DiD처럼 유닛 고정 효과를 도입하여 레벨이 아닌 <strong>추세(Trend)만 맞추면 되도록</strong> 설계된, 두 방법론의 장점을 결합한 접근법입니다.</li>
</ul>
<hr>
</section>
</section>
<section id="sdid-algorithm-description" class="level1">
<h1>4. SDiD Algorithm Description</h1>
<ul>
<li>SDiD 알고리즘은 크게 세 단계로 구성됩니다: <strong>(1) 유닛 가중치 계산</strong>, <strong>(2) 시간 가중치 계산</strong>, <strong>(3) 가중 회귀 분석</strong>.</li>
</ul>
<section id="step-1-compute-unit-weights-hatomega_i" class="level2">
<h2 class="anchored" data-anchor-id="step-1-compute-unit-weights-hatomega_i">Step 1: Compute Unit Weights (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Comega%7D_i">)</h2>
<ul>
<li>처치 이전 기간(<img src="https://latex.codecogs.com/png.latex?T_%7Bpre%7D">) 동안, 대조군들의 가중 합이 처치군의 평균 추세를 따르도록 만듭니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Comega%7D%20=%20%5Cunderset%7B%5Comega_0,%20%5Comega_%7Bco%7D%7D%7B%5Carg%20%5Cmin%7D%20%5Cleft(%20%7C%7C%20%5Coverline%7By%7D_%7Bpre,%20tr%7D%20-%20(%5Comega_0%20+%20%5Comega_%7Bco%7D%20Y_%7Bpre,%20co%7D)%20%7C%7C_2%5E2%20+%20%5Czeta%5E2%20T_%7Bpre%7D%20%7C%7C%5Comega_%7Bco%7D%7C%7C_2%5E2%20%5Cright)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%7Bwhere%20%7D%20%5Cquad%20%5Cbar%7B%5Cmathbf%7By%7D%7D_%7Bpre,tr%7D%20&amp;=%20%5Cfrac%7B1%7D%7BN_%7Btr%7D%7D%5Csum_%7Bi=N_%7Bco%7D+1%7D%5E%7BN%7D%20Y_%7Bit%7D%20%5C%5C%0A%5Comega_%7Bco%7D%5Cmathbf%7BY%7D_%7Bpre,co%7D%20&amp;=%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%5Comega_i%20Y_%7Bit%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>여기서:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7By%7D_%7Bpre,%20tr%7D">: 처치군들의 처치 이전 평균 추세 (벡터)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bpre,%20co%7D">: 대조군들의 처치 이전 데이터 행렬 (<img src="https://latex.codecogs.com/png.latex?T_%7Bpre%7D%20%5Ctimes%20N_%7Bco%7D">)</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Comega_0"> (Intercept):</strong> SDiD의 결정적 차이점입니다. SCM과 달리 절편을 허용합니다. 즉, <strong>레벨(Level)을 맞출 필요 없이 추세(Trend)만 평행하게 맞추면 됩니다.</strong></li>
<li><strong><img src="https://latex.codecogs.com/png.latex?L_2"> Regularization (<img src="https://latex.codecogs.com/png.latex?%5Czeta">):</strong> Ridge penalty를 사용하여 가중치가 특정 유닛에 쏠리는 것을 방지하고(dispersed weights), 대조군 전체에 고르게 분포되도록 합니다.</li>
</ul></li>
</ul>
<section id="regularization-parameter-zeta" class="level3">
<h3 class="anchored" data-anchor-id="regularization-parameter-zeta">Regularization Parameter <img src="https://latex.codecogs.com/png.latex?%5Czeta"></h3>
<ul>
<li>정규화 파라미터 <img src="https://latex.codecogs.com/png.latex?%5Czeta">는 데이터의 변동성에 기반하여 다음과 같이 결정됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Czeta%20=%20(N_%7Btr%7D%20%5Ccdot%20T_%7Bpost%7D)%5E%7B1/4%7D%20%5Chat%7B%5Csigma%7D(%5CDelta_%7Bit%7D)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%7Bwhere%20%7D%20%5Cquad%20%5CDelta_%7Bit%7D%20&amp;=%20Y_%7Bi(t+1)%7D%20-%20Y_%7Bit%7D%20%5C%5C%0A%5Cbar%7B%5CDelta%7D%20&amp;=%20%5Cfrac%7B1%7D%7BN_%7Bco%7D(T_%7Bpre%7D-1)%7D%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D-1%7D%20%5CDelta_%7Bit%7D%20%5C%5C%0A%5Chat%7B%5Csigma%7D%5E2(%5CDelta_%7Bit%7D)%20&amp;=%20%5Cfrac%7B1%7D%7BN_%7Bco%7D(T_%7Bpre%7D-1)%7D%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D-1%7D%20(%5CDelta_%7Bit%7D%20-%20%5Cbar%7B%5CDelta%7D)%5E2%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>이때 <img src="https://latex.codecogs.com/png.latex?%5CDelta_%7Bit%7D">는 결과 변수의 1차 차분(first difference, <img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D%20-%20Y_%7Bi(t-1)%7D">)이며, <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D">는 이 차분 값들의 표준편차입니다.</li>
<li>이는 시계열적 변동성이 클수록 페널티를 강하게 주어 과적합을 막겠다는 의도입니다.</li>
</ul>
</section>
</section>
<section id="step-2-compute-time-weights-hatlambda_t" class="level2">
<h2 class="anchored" data-anchor-id="step-2-compute-time-weights-hatlambda_t">Step 2: Compute Time Weights (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D_t">)</h2>
<ul>
<li>SDiD는 <strong>시간 가중치</strong>도 계산합니다.</li>
<li><ul>
<li>이는 처치 이전 시점들(<img src="https://latex.codecogs.com/png.latex?1%20%5Cdots%20T_%7Bpre%7D">) 중, 처치 이후 시점(<img src="https://latex.codecogs.com/png.latex?T_%7Bpost%7D">)과 유사한 시점에 더 큰 가중치를 부여하기 위함입니다.</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Clambda%7D%20=%20%5Cunderset%7B%5Clambda_0,%20%5Clambda_%7Bpre%7D%7D%7B%5Carg%20%5Cmin%7D%20%7C%7C%20%5Coverline%7By%7D_%7Bpost,%20co%7D%20-%20(%5Clambda_0%20+%20%5Clambda_%7Bpre%7D%20Y_%7Bpre,%20co%7D)%20%7C%7C_2%5E2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%7Bwhere%20%7D%20%5Cquad%20%5Cbar%7B%5Cmathbf%7By%7D%7D_%7Bpost,co%7D%20&amp;=%20%5Cfrac%7B1%7D%7BT_%7Bpost%7D%7D%20%5Csum_%7Bt=T_%7Bpre%7D+1%7D%5E%7BT%7D%20Y_%7Bit%7D%20%5C%5C%0A%5Cboldsymbol%7B%5Clambda%7D_%7Bpre%7D%5Cmathbf%7BY%7D_%7Bpre,co%7D%20&amp;=%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Clambda_t%20Y_%7Bit%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Coverline%7By%7D_%7Bpost,%20co%7D">: 대조군의 처치 이후 평균 (스칼라 혹은 벡터)</li>
<li>이 과정은 대조군 내에서 “처치 이전 기간의 가중 합”이 “처치 이후 기간”과 유사해지도록 만듭니다. 이를 통해 시간적 편향(bias)을 제거합니다.</li>
</ul>
</section>
<section id="step-3-weighted-did-regression" class="level2">
<h2 class="anchored" data-anchor-id="step-3-weighted-did-regression">Step 3: Weighted DiD Regression</h2>
<ul>
<li>구해진 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Comega%7D_i">와 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D_t">를 사용하여 최종적으로 가중 이원 고정 효과(Weighted TWFE) 회귀를 수행합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Chat%7B%5Ctau%7D%5E%7BSDiD%7D,%20%5Chat%7B%5Cmu%7D,%20%5Chat%7B%5Calpha%7D,%20%5Chat%7B%5Cbeta%7D)%20=%20%5Cunderset%7B%5Ctau,%20%5Cmu,%20%5Calpha,%20%5Cbeta%7D%7B%5Carg%20%5Cmin%7D%20%5Cleft%5C%7B%20%5Csum_%7Bi=1%7D%5E%7BN%7D%5Csum_%7Bt=1%7D%5E%7BT%7D%20(Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Calpha_i%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D)%5E2%20%5Chat%7B%5Comega%7D_i%20%5Chat%7B%5Clambda%7D_t%20%5Cright%5C%7D%0A"></p>
<ul>
<li>이 회귀분석의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> 값이 바로 SDiD가 추정한 인과 효과입니다.</li>
</ul>
<hr>
</section>
<section id="appendix-closed-form-solution" class="level2">
<h2 class="anchored" data-anchor-id="appendix-closed-form-solution">Appendix: Closed-Form Solution</h2>
<section id="weighted-twfe-objective-function" class="level3">
<h3 class="anchored" data-anchor-id="weighted-twfe-objective-function">1. Weighted TWFE Objective Function</h3>
<ul>
<li>SDiD는 다음의 <strong>가중 잔차 제곱합(Weighted Sum of Squared Residuals)</strong>을 최소화하는 파라미터 <img src="https://latex.codecogs.com/png.latex?%5C%7B%5Ctau,%20%5Cmu,%20%5Calpha,%20%5Cbeta%5C%7D">를 찾습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmin_%7B%5Ctau,%20%5Cmu,%20%5Calpha,%20%5Cbeta%7D%20%5Csum_%7Bi=1%7D%5EN%20%5Csum_%7Bt=1%7D%5ET%20%5Cleft(%20Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Calpha_i%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D%20%5Cright)%5E2%20%5Chat%7B%5Comega%7D_i%20%5Chat%7B%5Clambda%7D_t%0A"> * <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D">: 처치군(<img src="https://latex.codecogs.com/png.latex?tr">)이면서 처치 후(<img src="https://latex.codecogs.com/png.latex?post">)일 때 <img src="https://latex.codecogs.com/png.latex?1">, 그 외 <img src="https://latex.codecogs.com/png.latex?0"> (Treatment Indicator) * <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Comega%7D_i,%20%5Chat%7B%5Clambda%7D_t">: Step 1, 2에서 미리 구해둔 유닛 및 시간 가중치</p>
</section>
<section id="first-order-condition-foc-w.r.t-tau" class="level3">
<h3 class="anchored" data-anchor-id="first-order-condition-foc-w.r.t-tau">2. First Order Condition (FOC) w.r.t <img src="https://latex.codecogs.com/png.latex?%5Ctau"></h3>
<ul>
<li>목적함수를 <img src="https://latex.codecogs.com/png.latex?%5Ctau">에 대해 편미분하고 <img src="https://latex.codecogs.com/png.latex?0">으로 둡니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20%5Ctau%7D%20=%20-2%20%5Csum_%7Bi=1%7D%5EN%20%5Csum_%7Bt=1%7D%5ET%20%5Chat%7B%5Comega%7D_i%20%5Chat%7B%5Clambda%7D_t%20%5Cleft(%20Y_%7Bit%7D%20-%20%5Cmu%20-%20%5Calpha_i%20-%20%5Cbeta_t%20-%20%5Ctau%20X_%7Bit%7D%20%5Cright)%20X_%7Bit%7D%20=%200%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D=1">인 경우(즉, 처치군의 사후 기간)만 항이 살아남으므로, 식을 정리하면 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D">는 <strong>“관측된 값”</strong>과 <strong>“반사실적(Counterfactual) 추정치”</strong>의 차이가 됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctau%7D%5E%7BSDiD%7D%20=%20%5Cunderbrace%7B%5Cbar%7BY%7D_%7Btr,%20post%7D%7D_%7B%5Ctext%7BObserved%7D%7D%20-%20%5Cunderbrace%7B(%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D)%7D_%7B%5Ctext%7BCounterfactual%20%7D%20%5Chat%7BY%7D(0)%7D%0A"></p>
<ul>
<li>이제 우리의 목표는 미지의 파라미터 조합 <strong><img src="https://latex.codecogs.com/png.latex?(%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D)"></strong>를 우리가 아는 <strong>데이터의 가중 평균(3개의 항)</strong>으로 표현하는 것입니다.</li>
</ul>
</section>
<section id="parameter-decomposition-via-normal-equations" class="level3">
<h3 class="anchored" data-anchor-id="parameter-decomposition-via-normal-equations">3. Parameter Decomposition via Normal Equations</h3>
<ul>
<li>최소자승법의 1계 조건(FOC)은 <strong>“잔차의 합을 0으로 만든다”</strong>는 성질이 있습니다.</li>
<li>이를 이용해 관측 가능한 세 가지 항을 유도합니다.</li>
<li>단, 가중치의 합은 1로 정규화되어 있다고 가정합니다: <img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Chat%7B%5Comega%7D_i%20=%201,%20%5Csum%20%5Chat%7B%5Clambda%7D_t%20=%201"></li>
</ul>
<section id="처치군의-사전-기간-bary_tr-prelambda" class="level4">
<h4 class="anchored" data-anchor-id="처치군의-사전-기간-bary_tr-prelambda">3-1. 처치군의 사전 기간 (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D">)</h4>
<ul>
<li>처치 유닛(<img src="https://latex.codecogs.com/png.latex?i=tr">)과 사전 기간(<img src="https://latex.codecogs.com/png.latex?t%20%5Cin%20Pre">)에 해당하는 목적함수를 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Calpha%7D_%7Btr%7D">에 대해 편미분합니다.</li>
<li>이 기간에는 <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D=0">이므로 <img src="https://latex.codecogs.com/png.latex?%5Ctau"> 항은 사라집니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L_%7Btr%7D%7D%7B%5Cpartial%20%5Chat%7B%5Calpha%7D_%7Btr%7D%7D%20=%20-2%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Chat%7B%5Clambda%7D_t%20(Y_%7Btr,t%7D%20-%20%5Chat%7B%5Cmu%7D%20-%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20-%20%5Chat%7B%5Cbeta%7D_t)%20=%200%0A"></p>
<ul>
<li>시그마를 분배하고 정리하면 다음과 같습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Chat%7B%5Clambda%7D_t%20=%201"> 적용</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Clambda%7D_t%20Y_%7Btr,t%7D%7D_%7B%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D%7D%20=%20%5Chat%7B%5Cmu%7D%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Clambda%7D_t%7D_%7B1%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Clambda%7D_t%7D_%7B1%7D%20+%20%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Clambda%7D_t%20%5Chat%7B%5Cbeta%7D_t%7D_%7B%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctherefore%20%5Cquad%20%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D%20=%20%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D%20%5Cquad%20%5Ccdots%20%5Ctext%7B(%EC%8B%9D%201)%7D%0A"></p>
</section>
<section id="통제군의-사후-기간-bary_co-postomega" class="level4">
<h4 class="anchored" data-anchor-id="통제군의-사후-기간-bary_co-postomega">3-2. 통제군의 사후 기간 (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D">)</h4>
<ul>
<li>통제 유닛들(<img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20Co">)과 사후 기간(<img src="https://latex.codecogs.com/png.latex?t=post">)에 해당하는 목적함수를 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7Bpost%7D">에 대해 편미분합니다.</li>
<li>통제군이므로 <img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D=0">, 따라서 <img src="https://latex.codecogs.com/png.latex?%5Ctau"> 항은 사라집니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7B%5Cpartial%20L_%7Bpost%7D%7D%7B%5Cpartial%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D%7D%20=%20-2%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Chat%7B%5Comega%7D_i%20(Y_%7Bi,%20post%7D%20-%20%5Chat%7B%5Cmu%7D%20-%20%5Chat%7B%5Calpha%7D_i%20-%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D)%20=%200%0A"></p>
<ul>
<li>마찬가지로 정리합니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Chat%7B%5Comega%7D_i%20=%201"> 적용</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Comega%7D_i%20Y_%7Bi,%20post%7D%7D_%7B%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D%7D%20=%20%5Chat%7B%5Cmu%7D%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Comega%7D_i%7D_%7B1%7D%20+%20%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Comega%7D_i%20%5Chat%7B%5Calpha%7D_i%7D_%7B%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D%5Cunderbrace%7B%5Csum%20%5Chat%7B%5Comega%7D_i%7D_%7B1%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctherefore%20%5Cquad%20%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D%20=%20%5Chat%7B%5Cmu%7D%20+%20%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D%20%5Cquad%20%5Ccdots%20%5Ctext%7B(%EC%8B%9D%202)%7D%0A"></p>
</section>
<section id="통제군의-사전-기간-bary_co-preomega-lambda" class="level4">
<h4 class="anchored" data-anchor-id="통제군의-사전-기간-bary_co-preomega-lambda">3-3. 통제군의 사전 기간 (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D">)</h4>
<ul>
<li>통제 유닛(<img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20Co">)과 사전 기간(<img src="https://latex.codecogs.com/png.latex?t%20%5Cin%20Pre">) 전체에 대해 편미분(혹은 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmu%7D">에 대한 FOC)을 적용합니다. (<img src="https://latex.codecogs.com/png.latex?X_%7Bit%7D=0">)</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Chat%7B%5Comega%7D_i%20%5Chat%7B%5Clambda%7D_t%20(Y_%7Bit%7D%20-%20%5Chat%7B%5Cmu%7D%20-%20%5Chat%7B%5Calpha%7D_i%20-%20%5Chat%7B%5Cbeta%7D_t)%20=%200%0A"></p>
<ul>
<li>이중 시그마를 풀면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D%20=%20%5Chat%7B%5Cmu%7D(%5Csum%20%5Comega%20%5Csum%20%5Clambda)%20+%20(%5Csum%20%5Comega%20%5Chat%7B%5Calpha%7D_i)(%5Csum%20%5Clambda)%20+%20(%5Csum%20%5Clambda%20%5Chat%7B%5Cbeta%7D_t)(%5Csum%20%5Comega)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctherefore%20%5Cquad%20%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D%20=%20%5Chat%7B%5Cmu%7D%20+%20%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%20+%20%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D%20%5Cquad%20%5Ccdots%20%5Ctext%7B(%EC%8B%9D%203)%7D%0A"></p>
</section>
</section>
<section id="cancellation-derivation" class="level3">
<h3 class="anchored" data-anchor-id="cancellation-derivation">4. Cancellation &amp; Derivation</h3>
<ul>
<li>위에서 유도한 세 식을 조합하여 <strong>반사실적 추정치</strong>를 만들어냅니다.</li>
<li><strong>(식 1) + (식 2) - (식 3)</strong>을 계산하면, 우리가 원하지 않는 파라미터들이 소거됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A&amp;%20%5Cquad%20%5Cunderbrace%7B(%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D)%7D_%7B%5Ctext%7B%E2%91%A0%20Treated,%20Pre%7D%7D%20+%20%5Cunderbrace%7B(%5Chat%7B%5Cmu%7D%20+%20%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D)%7D_%7B%5Ctext%7B%E2%91%A1%20Control,%20Post%7D%7D%20-%20%5Cunderbrace%7B(%5Chat%7B%5Cmu%7D%20+%20%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%20+%20%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D)%7D_%7B%5Ctext%7B%E2%91%A2%20Control,%20Pre%7D%7D%20%5C%5C%0A%5C%5C%0A&amp;=%20(%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Cmu%7D%20-%20%5Chat%7B%5Cmu%7D)%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D%20+%20%5Cunderbrace%7B(%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D%20-%20%5Cbar%7B%5Calpha%7D_%7Bco%7D%5E%7B%5Comega%7D)%7D_%7B%5Ctext%7BUnit%20FE%20Cancel%7D%7D%20+%20%5Cunderbrace%7B(%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D%20-%20%5Cbar%7B%5Cbeta%7D_%7Bpre%7D%5E%7B%5Clambda%7D)%7D_%7B%5Ctext%7BTime%20FE%20Cancel%7D%7D%20%5C%5C%0A%5C%5C%0A&amp;=%20%5Cmathbf%7B%5Chat%7B%5Cmu%7D%20+%20%5Chat%7B%5Calpha%7D_%7Btr%7D%20+%20%5Chat%7B%5Cbeta%7D_%7Bpost%7D%7D%20%5Cquad%20(=%20%5Ctext%7BCounterfactual%7D)%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="final-result" class="level3">
<h3 class="anchored" data-anchor-id="final-result">5. Final Result</h3>
<ul>
<li>따라서 FOC에서 도출된 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Ctau%7D"> 식에 대입하면, 최종적으로 <strong>2x2 DID</strong> 형태가 완성됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Chat%7B%5Ctau%7D%5E%7BSDiD%7D%20&amp;=%20%5Cbar%7BY%7D_%7Btr,%20post%7D%20-%20%5Ctext%7BCounterfactual%7D%20%5C%5C%0A&amp;=%20%5Cbar%7BY%7D_%7Btr,%20post%7D%20-%20%5Cleft(%20%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D%20+%20%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D%20-%20%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D%20%5Cright)%20%5C%5C%0A&amp;=%20(%5Cbar%7BY%7D_%7Btr,%20post%7D%20-%20%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D)%20-%20(%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D%20-%20%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<hr>
</section>
</section>
</section>
<section id="interpretation-why-sdid" class="level1">
<h1>5. Interpretation: Why SDiD?</h1>
<section id="vs.-did" class="level2">
<h2 class="anchored" data-anchor-id="vs.-did">5.1. vs.&nbsp;DiD</h2>
<ul>
<li><strong>Local Fitting:</strong> DiD는 모든 대조군과 모든 시점을 동일하게 취급하지만, SDiD는 처치군과 유사한 과거를 가진 유닛, 처치 시기와 유사한 특성을 가진 시점을 강조(weighting)하여 “Local”한 회귀를 수행합니다.</li>
<li><strong>Precision:</strong> 가중치를 통해 결과 변수의 시스템적이고 예측 가능한 부분을 제거함으로써 추정의 정밀도(precision)를 높입니다.</li>
</ul>
</section>
<section id="vs.-scm" class="level2">
<h2 class="anchored" data-anchor-id="vs.-scm">5.2. vs.&nbsp;SCM</h2>
<ul>
<li><strong>Parallelism over Levels:</strong> SCM은 Outcome의 절대적인 수치(Level)까지 맞춰야 하지만, SDiD는 유닛 고정 효과(<img src="https://latex.codecogs.com/png.latex?%5Calpha_i">)와 절편(<img src="https://latex.codecogs.com/png.latex?%5Comega_0">)을 포함하므로 <strong>평행 추세만 만족하면 됩니다.</strong> 이는 더 유연한 매칭을 가능하게 합니다.</li>
<li><strong>Bias Removal:</strong> 시간 가중치(<img src="https://latex.codecogs.com/png.latex?%5Clambda_t">)를 도입하여, 처치 이후 기간과 성격이 매우 다른 처치 이전 기간의 영향력을 배제하여 편향을 줄입니다.</li>
<li><strong>Robustness:</strong> 유닛 고정 효과는 결과 변수의 변동 중 상당 부분을 설명하므로, 모델의 강건성을 높여줍니다.</li>
</ul>
<hr>
</section>
</section>
<section id="key-takeaways" class="level1">
<h1>6. Key Takeaways</h1>
<ol type="1">
<li><strong>Trend Matching:</strong> SDiD는 SCM처럼 처치 이전 추세를 맞추지만, 레벨(Level)이 아닌 <strong>변화(Trend)</strong>가 평행하도록 가중치를 학습합니다 (Intercept <img src="https://latex.codecogs.com/png.latex?%5Comega_0"> 허용).</li>
<li><strong>Regularization:</strong> 유닛 가중치 계산 시 <img src="https://latex.codecogs.com/png.latex?L_2"> Norm을 사용하여 가중치가 특정 소수 유닛에 집중되는 SCM의 문제를 완화하고, 대조군 전반에 퍼지도록 합니다.</li>
<li><strong>Time Weights:</strong> DiD나 SCM에는 없는 <strong>시간 가중치(<img src="https://latex.codecogs.com/png.latex?%5Clambda_t">)</strong>를 도입하여, 처치 전후 기간의 구조적 차이에서 오는 편향을 보정합니다.</li>
<li><strong>Efficiency:</strong> 유닛/시간 고정 효과와 가중치를 동시에 활용함으로써 추정량의 분산을 줄이고 인과 효과 추정의 신뢰도를 높입니다.</li>
</ol>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-01/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 15A. SDiD (Part 2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/</link>
  <description><![CDATA[ 





<section id="개요-introduction" class="level1">
<h1>개요 (Introduction)</h1>
<ul>
<li><p>인과 추론(Causal Inference)에서 가장 널리 쓰이는 <strong>이중차분법(DiD)</strong>은 ’평행 추세 가정(Parallel Trends Assumption)’에 크게 의존합니다.</p></li>
<li><p>하지만 현실 데이터에서는 처치군과 대조군의 추세가 평행하지 않은 경우가 많습니다.</p></li>
<li><p>이를 보완하기 위해 <strong>Synthetic Control Method (SCM)</strong>이 등장했지만, SCM 역시 제약이 있습니다.</p></li>
<li><p><strong>Synthetic Difference in Differences (SDiD)</strong>는 DiD와 SCM의 장점을 결합하여, 평행 추세가 위배되는 상황에서도 더욱 강건한(robust) 추정치를 제공합니다.</p></li>
<li><p>이번 포스트에서는 파이썬을 사용하여 SDiD를 직접 구현해보고, 평행 추세가 위배되는 시뮬레이션 데이터를 통해 DiD, SCM과 성능을 비교해 봅니다.</p></li>
</ul>
</section>
<section id="import-library" class="level1">
<h1>0. Import Library</h1>
<div id="8d6fc200" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> cvxpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> cp</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sm</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> smf</span></code></pre></div></div>
</details>
</div>
</section>
<section id="define-syntheticdid-class" class="level1">
<h1>1. Define SyntheticDiD Class</h1>
<ul>
<li>Python의 <code>SyntheticDiD</code> 클래스는 처치군(Treated)과 통제군(Control) 데이터를 입력받아, SDiD 추정량을 산출하는 핵심 로직을 담고 있습니다.</li>
<li>주요 단계는 초기화, 유닛 가중치 계산, 시간 가중치 계산, 그리고 최종 효과 추정으로 나뉩니다.</li>
</ul>
<section id="초기화-initialization" class="level2">
<h2 class="anchored" data-anchor-id="초기화-initialization">초기화 (Initialization)</h2>
<ul>
<li>입력된 데이터를 처치 시점(<img src="https://latex.codecogs.com/png.latex?T_%7Bpre%7D">)을 기준으로 <strong>Pre-period</strong>와 <strong>Post-period</strong>로 분리하고, 행렬 연산에 적합한 형태로 변환합니다.
<ul>
<li><strong>Inputs</strong>: <img src="https://latex.codecogs.com/png.latex?Y_%7Bco%7D"> (Control Data), <img src="https://latex.codecogs.com/png.latex?Y_%7Btr%7D"> (Treated Data), <img src="https://latex.codecogs.com/png.latex?T_%7Bpre%7D"> (Intervention Time)</li>
<li><strong>Split</strong>: <img src="https://latex.codecogs.com/png.latex?Y_%7Bpre,%20co%7D,%20Y_%7Bpre,%20tr%7D"> 등으로 데이터를 슬라이싱합니다.</li>
</ul></li>
</ul>
</section>
<section id="단위-가중치-unit-weights-계산" class="level2">
<h2 class="anchored" data-anchor-id="단위-가중치-unit-weights-계산">단위 가중치 (Unit Weights) 계산</h2>
<ul>
<li>처치 유닛의 처치 전 추세(Pre-trend)를 가장 잘 모방하는 합성 통제군을 만들기 위해 유닛 가중치 <img src="https://latex.codecogs.com/png.latex?%5Comega">를 계산합니다.</li>
</ul>
<section id="핵심-구현-사항" class="level3">
<h3 class="anchored" data-anchor-id="핵심-구현-사항">핵심 구현 사항</h3>
<ul>
<li><strong>Intercept (<img src="https://latex.codecogs.com/png.latex?%5Comega_0">) 허용</strong>:
<ul>
<li><code>get_unit_weights(intercept=True)</code> 옵션을 통해 절편을 포함합니다.</li>
<li>이는 기존 SCM과 달리 처치 유닛이 통제 유닛들의 볼록 껍질(Convex Hull) 밖에 있어도 평행 이동을 통해 맞출 수 있게 합니다.</li>
</ul></li>
<li><strong>Regularization (<img src="https://latex.codecogs.com/png.latex?%5Czeta">)</strong>:
<ul>
<li>가중치가 특정 유닛에 쏠리는 것을 방지하고 분산시키기 위해 Ridge Penalty(<img src="https://latex.codecogs.com/png.latex?%5Czeta%5E2%20%7C%7C%5Comega%7C%7C%5E2">)를 적용합니다.</li>
</ul></li>
</ul>
</section>
<section id="최적화-수식" class="level3">
<h3 class="anchored" data-anchor-id="최적화-수식">최적화 수식</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Chat%7B%5Comega%7D_0,%20%5Chat%7B%5Comega%7D)%20=%20%5Cunderset%7B%5Comega_0,%20%5Comega%7D%7B%5Carg%5Cmin%7D%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Cleft(%20Y_%7Btr,t%7D%20-%20%5Comega_0%20-%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Comega_i%20Y_%7Bit%7D%20%5Cright)%5E2%20+%20%5Czeta%5E2%20%7C%7C%5Comega%7C%7C_2%5E2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsubject%20to%20%7D%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Comega_i%20=%201,%20%5Cquad%20%5Comega_i%20%5Cge%200%0A"></p>
</section>
</section>
<section id="시간-가중치-time-weights-계산" class="level2">
<h2 class="anchored" data-anchor-id="시간-가중치-time-weights-계산">시간 가중치 (Time Weights) 계산</h2>
<ul>
<li>처치 전 기간(Pre-period) 중, 처치 후 기간(Post-period)과 가장 유사한 패턴을 보이는 시점들에 더 큰 비중을 두기 위해 시간 가중치 <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 계산합니다.</li>
</ul>
<section id="핵심-구현-사항-1" class="level3">
<h3 class="anchored" data-anchor-id="핵심-구현-사항-1">핵심 구현 사항</h3>
<ul>
<li><strong>Target</strong>: 각 통제 유닛(Control Unit)의 <strong>처치 후 평균값</strong>(<img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Bpost,%20co%7D">)을 타겟으로 설정합니다.</li>
<li><strong>Intercept (<img src="https://latex.codecogs.com/png.latex?%5Clambda_0">) 허용</strong>: 시점 간의 레벨 차이를 보정하기 위해 절편을 포함합니다.</li>
</ul>
</section>
<section id="최적화-수식-1" class="level3">
<h3 class="anchored" data-anchor-id="최적화-수식-1">최적화 수식</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A(%5Chat%7B%5Clambda%7D_0,%20%5Chat%7B%5Clambda%7D)%20=%20%5Cunderset%7B%5Clambda_0,%20%5Clambda%7D%7B%5Carg%5Cmin%7D%20%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Cleft(%20%5Cbar%7BY%7D_%7Bi,%20post%7D%20-%20%5Clambda_0%20-%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Clambda_t%20Y_%7Bit%7D%20%5Cright)%5E2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsubject%20to%20%7D%20%5Csum_%7Bt=1%7D%5E%7BT_%7Bpre%7D%7D%20%5Clambda_t%20=%201,%20%5Cquad%20%5Clambda_t%20%5Cge%200%0A"></p>
</section>
</section>
<section id="추정-estimation-closed-form-solution" class="level2">
<h2 class="anchored" data-anchor-id="추정-estimation-closed-form-solution">추정 (Estimation: Closed-form Solution)</h2>
<ul>
<li>복잡한 가중 회귀분석(Weighted TWFE)을 수행하는 대신, 구해진 가중치(<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Comega%7D,%20%5Chat%7B%5Clambda%7D">)를 사용하여 <strong>가중 이중차분(Weighted DID)</strong> 형태로 직접 <img src="https://latex.codecogs.com/png.latex?%5Ctau_%7Bsdid%7D">를 계산합니다.</li>
</ul>
<section id="구현-로직-estimate-메서드" class="level3">
<h3 class="anchored" data-anchor-id="구현-로직-estimate-메서드">구현 로직 (<code>estimate</code> 메서드)</h3>
<ul>
<li>다음의 4가지 항을 계산하여 최종 효과를 도출합니다.</li>
</ul>
<ol type="1">
<li><strong>Treated Post</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Btr,%20post%7D"> (단순 평균)</li>
<li><strong>Treated Pre (Time-weighted)</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D%20=%20%5Csum%20%5Chat%7B%5Clambda%7D_t%20Y_%7Btr,t%7D"></li>
<li><strong>Control Post (Unit-weighted)</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Bco,%20post%7D%5E%7B%5Comega%7D%20=%20%5Csum%20%5Chat%7B%5Comega%7D_i%20%5Cbar%7BY%7D_%7Bi,%20post%7D"></li>
<li><strong>Control Pre (Double-weighted)</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_%7Bco,%20pre%7D%5E%7B%5Comega,%20%5Clambda%7D%20=%20%5Csum%20%5Chat%7B%5Comega%7D_i%20(%5Csum%20%5Chat%7B%5Clambda%7D_t%20Y_%7Bit%7D)"></li>
</ol>
</section>
<section id="최종-추정식" class="level3">
<h3 class="anchored" data-anchor-id="최종-추정식">최종 추정식</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Ctau%7D%5E%7BSDiD%7D%20=%20%5Cunderbrace%7B(%5Cbar%7BY%7D_%7Btr,%20post%7D%20-%20%5Cbar%7BY%7D_%7Btr,%20pre%7D%5E%7B%5Clambda%7D)%7D_%7B%5Ctext%7BTreated%20Change%7D%7D%20-%20%5Cunderbrace%7B%5Csum_%7Bi=1%7D%5E%7BN_%7Bco%7D%7D%20%5Chat%7B%5Comega%7D_i%20(%5Cbar%7BY%7D_%7Bi,%20post%7D%20-%20%5Cbar%7BY%7D_%7Bi,%20pre%7D%5E%7B%5Clambda%7D)%7D_%7B%5Ctext%7BSynthetic%20Control%20Change%7D%7D%0A"></p>
<div id="e6dadcf3" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> SyntheticDiD:</span>
<span id="cb2-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, Y_co, Y_tr, T_pre):</span>
<span id="cb2-3">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_co  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Control Data (N_co x T)</span></span>
<span id="cb2-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_tr  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Treated Data (1 x T)</span></span>
<span id="cb2-5">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> T_pre</span>
<span id="cb2-6">        </span>
<span id="cb2-7">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Split Pre/Post</span></span>
<span id="cb2-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_co[:, :T_pre]</span>
<span id="cb2-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_tr[:, :T_pre]</span>
<span id="cb2-10">        </span>
<span id="cb2-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_co.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-12">        </span>
<span id="cb2-13">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_unit_weights(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>):</span>
<span id="cb2-14">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Step 1: Compute Unit Weights (omega)</span></span>
<span id="cb2-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        SDiD Eq: argmin || mean(Y_tr) - (w0 + w @ Y_co) ||^2 + zeta^2 ||w||^2</span></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb2-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Variables</span></span>
<span id="cb2-19">        w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Variable(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N_co)</span>
<span id="cb2-20">        w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Variable(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> intercept <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SCM은 intercept=False</span></span>
<span id="cb2-21">        </span>
<span id="cb2-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Target (Treated Pre-trend)</span></span>
<span id="cb2-23">        target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_tr.flatten()</span>
<span id="cb2-24">        </span>
<span id="cb2-25">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prediction (Weighted Control Pre-trend)</span></span>
<span id="cb2-26">        prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_co</span>
<span id="cb2-27">        </span>
<span id="cb2-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Regularization (Zeta)</span></span>
<span id="cb2-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 논문의 zeta 계산식 간소화 (표준편차 * N_tr 등)</span></span>
<span id="cb2-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> zeta <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-31">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simple heuristic for tutorial: std of first differences</span></span>
<span id="cb2-32">            diffs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.diff(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_co, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-33">            sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.std(diffs)</span>
<span id="cb2-34">            zeta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_co.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre))<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> sigma</span>
<span id="cb2-35">        </span>
<span id="cb2-36">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Objective Function</span></span>
<span id="cb2-37">        error_term <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.sum_squares(target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction)</span>
<span id="cb2-38">        reg_term <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.sum_squares(w) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb2-39">        objective <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Minimize(error_term <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> reg_term)</span>
<span id="cb2-40">        </span>
<span id="cb2-41">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Constraints: Sum to 1, Non-negative (Simplex)</span></span>
<span id="cb2-42">        constraints <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [cp.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(w) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, w <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-43">        </span>
<span id="cb2-44">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Solve</span></span>
<span id="cb2-45">        prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Problem(objective, constraints)</span>
<span id="cb2-46">        prob.solve()</span>
<span id="cb2-47">        </span>
<span id="cb2-48">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> w.value, (w0.value <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> intercept <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-49"></span>
<span id="cb2-50">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_time_weights(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, lambda_reg<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>):</span>
<span id="cb2-51">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        Step 2: Compute Time Weights (lambda)</span></span>
<span id="cb2-53"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        SDiD Eq: argmin || mean(Y_post_co) - (lam0 + Y_pre_co @ lam) ||^2</span></span>
<span id="cb2-54"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">        """</span></span>
<span id="cb2-55">        T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre</span>
<span id="cb2-56">        lam <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Variable(T_pre)</span>
<span id="cb2-57">        lam0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Variable(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-58">        </span>
<span id="cb2-59">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Target: Control Units' Post-treatment Average (Vector of size N_co)</span></span>
<span id="cb2-60">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 각 Control Unit의 처치 후 평균값</span></span>
<span id="cb2-61">        target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_co[:, T_pre:], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-62">        </span>
<span id="cb2-63">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prediction: Weighted sum of Pre-treatment values for each unit</span></span>
<span id="cb2-64">        prediction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> lam0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_pre_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> lam</span>
<span id="cb2-65">        </span>
<span id="cb2-66">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Objective</span></span>
<span id="cb2-67">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Ridge penalty slightly added for numerical stability</span></span>
<span id="cb2-68">        objective <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Minimize(cp.sum_squares(target <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> prediction) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> lambda_reg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> cp.sum_squares(lam))</span>
<span id="cb2-69">        </span>
<span id="cb2-70">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Constraints</span></span>
<span id="cb2-71">        constraints <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [cp.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(lam) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, lam <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb2-72">        </span>
<span id="cb2-73">        prob <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cp.Problem(objective, constraints)</span>
<span id="cb2-74">        prob.solve()</span>
<span id="cb2-75">        </span>
<span id="cb2-76">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> lam.value, lam0.value</span>
<span id="cb2-77"></span>
<span id="cb2-78">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> estimate(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb2-79">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Unit Weights (SDiD)</span></span>
<span id="cb2-80">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.omega, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.omega0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-81">        </span>
<span id="cb2-82">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Unit Weights (SCM - for comparison)</span></span>
<span id="cb2-83">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SCM: No intercept, No regularization (or small) in classic form, but usually L2 used.</span></span>
<span id="cb2-84">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Here we mimic SCM by forcing intercept=0</span></span>
<span id="cb2-85">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.omega_sc, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) </span>
<span id="cb2-86"></span>
<span id="cb2-87">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Time Weights (SDiD)</span></span>
<span id="cb2-88">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.lam, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.lam0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.get_time_weights()</span>
<span id="cb2-89">        </span>
<span id="cb2-90">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. SDiD Estimator (Weighted TWFE)</span></span>
<span id="cb2-91">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 간단한 구현을 위해 Weighted Diff-in-Diff 공식을 직접 적용합니다.</span></span>
<span id="cb2-92">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># tau_sdid = (Y_tr_post - Y_tr_pre_weighted) - (Y_co_post_weighted - Y_co_pre_weighted)</span></span>
<span id="cb2-93">        </span>
<span id="cb2-94">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply Time Weights to Pre-periods</span></span>
<span id="cb2-95">        y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_tr[:, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre:])</span>
<span id="cb2-96">        y_tr_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_tr[:, :<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre].flatten(), <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.lam)</span>
<span id="cb2-97">        </span>
<span id="cb2-98">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Apply Unit Weights to Control Units</span></span>
<span id="cb2-99">        y_co_post_series <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_co[:, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre:], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-100">        y_co_pre_matrix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.Y_co[:, :<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.T_pre]</span>
<span id="cb2-101">        </span>
<span id="cb2-102">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Double Weighted Control</span></span>
<span id="cb2-103">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Control Term = Sum( omega_i * ( Mean_Post_i - Sum(lambda_t * Y_it_pre) ) )</span></span>
<span id="cb2-104">        control_term <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb2-105">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.N_co):</span>
<span id="cb2-106">            y_i_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_co_post_series[i]</span>
<span id="cb2-107">            y_i_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.dot(y_co_pre_matrix[i], <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.lam)</span>
<span id="cb2-108">            control_term <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.omega[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (y_i_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_i_pre)</span>
<span id="cb2-109">            </span>
<span id="cb2-110">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tau_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_tr_pre) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> control_term</span>
<span id="cb2-111">        </span>
<span id="cb2-112">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.tau_sdid</span></code></pre></div></div>
</details>
</div>
<hr>
</section>
</section>
</section>
<section id="시뮬레이션1-평행-추세-가정의-위배-selection-on-trends" class="level1">
<h1>시뮬레이션1: 평행 추세 가정의 위배 (Selection on Trends)</h1>
<ul>
<li>SDiD가 기존의 이중차분법(DID)보다 우수한 점을 검증하기 위해, <strong>평행 추세 가정(Parallel Trends Assumption)이 성립하지 않는 데이터</strong>를 생성합니다.</li>
<li>이를 위해 단순한 이원 고정 효과(TWFE) 모델이 아닌, <strong>잠재 요인(Latent Factor) 모형</strong>을 사용합니다.</li>
</ul>
<section id="data-generation" class="level2">
<h2 class="anchored" data-anchor-id="data-generation">1. Data Generation</h2>
<section id="데이터-생성-모형-dgp" class="level3">
<h3 class="anchored" data-anchor-id="데이터-생성-모형-dgp">데이터 생성 모형 (DGP)</h3>
<ul>
<li>데이터는 다음과 같은 수식을 따릅니다. <img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20%5Cunderbrace%7B%5Calpha_i%7D_%7B%5Ctext%7BUnit%20FE%7D%7D%20+%20%5Cunderbrace%7B%5Cbeta_t%7D_%7B%5Ctext%7BTime%20FE%7D%7D%20+%20%5Cunderbrace%7B%5Cgamma_i%20f_t%7D_%7B%5Ctext%7BLatent%20Structure%7D%7D%20+%20%5Ctau%20D_%7Bit%7D%20+%20%5Cvarepsilon_%7Bit%7D%0A">
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Calpha_i,%20%5Cbeta_t"></strong>: 일반적인 개체 및 시간 고정 효과입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?f_t"></strong>: 시간에 따라 선형적으로 증가하는 공통의 잠재 요인입니다. (<img src="https://latex.codecogs.com/png.latex?f_t%20%5Cpropto%20t">)</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cgamma_i"></strong>: 잠재 요인 <img src="https://latex.codecogs.com/png.latex?f_t">에 반응하는 개체별 민감도(Factor Loading)입니다.</li>
</ul></li>
</ul>
</section>
<section id="non-parallel-trend" class="level3">
<h3 class="anchored" data-anchor-id="non-parallel-trend">Non-Parallel Trend</h3>
<ul>
<li>평행 추세를 깨뜨리기 위해, 처치 여부(<img src="https://latex.codecogs.com/png.latex?D_i">)가 무작위가 아니라 <strong>추세 민감도(<img src="https://latex.codecogs.com/png.latex?%5Cgamma_i">)</strong>에 따라 결정되도록 설정합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AD_i%20=%201%20%5Cquad%20%5Ctext%7Bif%20%7D%20%5Cgamma_i%20%5Ctext%7B%20is%20in%20top%20%7D%2010%5C%25,%20%5Cquad%20%5Ctext%7Belse%20%7D%200%0A"></p>
<div id="661c75e3" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_nonparallel_data(N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, T<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, T_pre<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, treated_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, true_tau<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb3-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> seed: np.random.seed(seed)</span>
<span id="cb3-3">    </span>
<span id="cb3-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Fixed Effects</span></span>
<span id="cb3-5">    alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)      <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Unit FE</span></span>
<span id="cb3-6">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, T) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Time FE (Trend)</span></span>
<span id="cb3-7">    </span>
<span id="cb3-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Latent Factor (Time-varying Confounder) causing Non-parallel trends</span></span>
<span id="cb3-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시간(t)에 따라 증가하는 패턴 (Trend Factor)</span></span>
<span id="cb3-10">    f_t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>, T)</span>
<span id="cb3-11">    </span>
<span id="cb3-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 유닛별 민감도 (Loadings)</span></span>
<span id="cb3-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 처치군이 될 확률이 높은 유닛들이 더 가파른 기울기를 갖도록 설정</span></span>
<span id="cb3-14">    gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb3-15">    </span>
<span id="cb3-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Treatment Assignment (Selection on Trends)</span></span>
<span id="cb3-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># gamma(기울기)가 큰 상위 유닛들을 처치군으로 선정 -&gt; 평행 추세 위배</span></span>
<span id="cb3-18">    N_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> treated_ratio)</span>
<span id="cb3-19">    treated_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argsort(gamma)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>N_tr:]</span>
<span id="cb3-20">    is_treated <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(N, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">bool</span>)</span>
<span id="cb3-21">    is_treated[treated_indices] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb3-22">    </span>
<span id="cb3-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. Generate Outcome Y</span></span>
<span id="cb3-24">    Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((N, T))</span>
<span id="cb3-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N):</span>
<span id="cb3-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(T):</span>
<span id="cb3-27">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Base Model</span></span>
<span id="cb3-28">            y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> alpha[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> beta[t] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> gamma[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> f_t[t] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-29">            </span>
<span id="cb3-30">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add Treatment Effect</span></span>
<span id="cb3-31">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> is_treated[i] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> t <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> T_pre:</span>
<span id="cb3-32">                y_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> true_tau</span>
<span id="cb3-33">            </span>
<span id="cb3-34">            Y[i, t] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_val</span>
<span id="cb3-35">            </span>
<span id="cb3-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prepare Output</span></span>
<span id="cb3-37">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 처치군 데이터를 맨 뒤로 보내거나 인덱스를 반환해야 하지만, </span></span>
<span id="cb3-38">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 여기서는 편의상 SyntheticDiD 클래스 입력 형식에 맞춰 정리</span></span>
<span id="cb3-39">    </span>
<span id="cb3-40">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sort: Control first, Treated last</span></span>
<span id="cb3-41">    sorted_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([np.where(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>is_treated)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], np.where(is_treated)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb3-42">    Y_sorted <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y[sorted_idx, :]</span>
<span id="cb3-43">    </span>
<span id="cb3-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Split</span></span>
<span id="cb3-45">    N_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> N_tr</span>
<span id="cb3-46">    Y_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_sorted[:N_co, :]</span>
<span id="cb3-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SDiD 클래스는 현재 1개의 Treated Unit을 가정하므로, 평균을 내서 1개로 만듭니다.</span></span>
<span id="cb3-48">    Y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_sorted[N_co:, :], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb3-49">    </span>
<span id="cb3-50">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Y_co, Y_tr, true_tau</span></code></pre></div></div>
</details>
</div>
<div id="8cd58fc5" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 단일 시뮬레이션 데이터 생성</span></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-4">Y_co, Y_tr, true_tau <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_nonparallel_data(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>, N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>, treated_ratio<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>)</span>
<span id="cb4-5">T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span></span>
<span id="cb4-6">time_axis <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(Y_co.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb4-7"></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Counterfactuals 계산</span></span>
<span id="cb4-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (1) True Counterfactual (Unobserved Truth)</span></span>
<span id="cb4-12">Y_tr_true_cf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_tr.flatten().copy()</span>
<span id="cb4-13">Y_tr_true_cf[T_pre:] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> true_tau </span>
<span id="cb4-14"></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (2) Naive DiD Counterfactual (What DiD assumes)</span></span>
<span id="cb4-16">mean_tr_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_tr.flatten()[:T_pre])</span>
<span id="cb4-17">mean_co_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(np.mean(Y_co, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)[:T_pre])</span>
<span id="cb4-18">level_gap <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_tr_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mean_co_pre</span>
<span id="cb4-19">Y_did_naive_cf <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_co, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> level_gap</span>
<span id="cb4-20"></span>
<span id="cb4-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 시각화</span></span>
<span id="cb4-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb4-24">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb4-25"></span>
<span id="cb4-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (1) Control Units</span></span>
<span id="cb4-27">plt.plot(time_axis, np.mean(Y_co, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'navy'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Average Control'</span>)</span>
<span id="cb4-28"></span>
<span id="cb4-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (2) Treated Unit (Observed)</span></span>
<span id="cb4-30">plt.plot(time_axis, Y_tr.flatten(), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'firebrick'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Treated (Observed)'</span>)</span>
<span id="cb4-31"></span>
<span id="cb4-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (3) True Counterfactual (Unobserved Truth) - RED Dotted</span></span>
<span id="cb4-33">plt.plot(time_axis, Y_tr_true_cf, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'firebrick'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'True Counterfactual (Target)'</span>)</span>
<span id="cb4-34"></span>
<span id="cb4-35"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (4) Naive DiD Counterfactual - GREEN Dashed</span></span>
<span id="cb4-36">plt.plot(time_axis, Y_did_naive_cf, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Naive DiD Counterfactual (Biased)'</span>)</span>
<span id="cb4-37"></span>
<span id="cb4-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Settings</span></span>
<span id="cb4-39">plt.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>T_pre, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'-'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Intervention'</span>)</span>
<span id="cb4-40">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Why DiD Fails: True Counterfactual vs Naive DiD Projection"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>)</span>
<span id="cb4-41">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Time"</span>)</span>
<span id="cb4-42">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Outcome Y"</span>)</span>
<span id="cb4-43">plt.legend(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'upper left'</span>)</span>
<span id="cb4-44">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb4-45">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/index_files/figure-html/cell-5-output-1.png" width="968" height="526" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="simulation" class="level2">
<h2 class="anchored" data-anchor-id="simulation">2. Simulation</h2>
<ul>
<li>앞서 정의한 데이터 생성 함수(<code>generate_data</code>)와 <code>SyntheticDiD</code> 클래스를 사용하여 몬테카를로 시뮬레이션을 수행합니다.</li>
<li>총 50회의 반복 시행을 통해 <strong>SDiD</strong>, <strong>SCM</strong>, <strong>Standard DID</strong>의 성능을 비교합니다.</li>
</ul>
<div id="13ca9e47" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulation Run</span></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb5-4">N_SIMULATIONS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb5-5">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>: [], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>: [], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>: []}</span>
<span id="cb5-6"></span>
<span id="cb5-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Starting Monte Carlo Simulation (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>N_SIMULATIONS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> runs)..."</span>)</span>
<span id="cb5-8"></span>
<span id="cb5-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_SIMULATIONS):</span>
<span id="cb5-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Generate Data (Non-parallel trends)</span></span>
<span id="cb5-11">    Y_co, Y_tr, true_tau <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_nonparallel_data(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>i)</span>
<span id="cb5-12">    T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span></span>
<span id="cb5-13">    </span>
<span id="cb5-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Run Models</span></span>
<span id="cb5-15">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SyntheticDiD(Y_co, Y_tr, T_pre)</span>
<span id="cb5-16">    </span>
<span id="cb5-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (1) SDiD</span></span>
<span id="cb5-18">    est_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.estimate()</span>
<span id="cb5-19">    </span>
<span id="cb5-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (2) SCM (Intercept=False, Zeta=0)</span></span>
<span id="cb5-21">    w_scm, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb5-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SCM Estimator Calculation</span></span>
<span id="cb5-23">    y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_tr[:, T_pre:])</span>
<span id="cb5-24">    y_sc_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(w_scm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Y_co[:, T_pre:])</span>
<span id="cb5-25">    est_scm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_sc_post</span>
<span id="cb5-26">    </span>
<span id="cb5-27">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (3) DiD (Simple TWFE)</span></span>
<span id="cb5-28">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Tau = (Tr_post - Tr_pre) - (Co_post - Co_pre)</span></span>
<span id="cb5-29">    diff_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_tr[:, T_pre:]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> np.mean(Y_tr[:, :T_pre])</span>
<span id="cb5-30">    diff_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_co[:, T_pre:]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> np.mean(Y_co[:, :T_pre]) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Mean of all controls</span></span>
<span id="cb5-31">    est_did <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> diff_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> diff_co</span>
<span id="cb5-32">    </span>
<span id="cb5-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save</span></span>
<span id="cb5-34">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>].append(est_sdid)</span>
<span id="cb5-35">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>].append(est_scm)</span>
<span id="cb5-36">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>].append(est_did)</span>
<span id="cb5-37"></span>
<span id="cb5-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb5-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Result Visualization</span></span>
<span id="cb5-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb5-41">df_res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results)</span>
<span id="cb5-42">true_tau <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb5-43"></span>
<span id="cb5-44">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb5-45">sns.boxplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df_res)</span>
<span id="cb5-46">plt.axhline(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true_tau, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'True Effect (10)'</span>)</span>
<span id="cb5-47">plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Simulation Results (N=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>N_SIMULATIONS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">): Bias Comparison"</span>)</span>
<span id="cb5-48">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Estimated Effect"</span>)</span>
<span id="cb5-49">plt.legend()</span>
<span id="cb5-50">plt.show()</span>
<span id="cb5-51"></span>
<span id="cb5-52"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate Metrics</span></span>
<span id="cb5-53">bias <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_res.mean() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> true_tau</span>
<span id="cb5-54">rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(((df_res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> true_tau)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).mean())</span>
<span id="cb5-55"></span>
<span id="cb5-56"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"=== Performance Metrics ==="</span>)</span>
<span id="cb5-57"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Method'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bias'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb5-58"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span>)</span>
<span id="cb5-59"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>]:</span>
<span id="cb5-60">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>method<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>upper()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bias[method]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">     | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rmse[method]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Starting Monte Carlo Simulation (50 runs)...</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/anaconda3/envs/causal-inference-study/lib/python3.9/site-packages/cvxpy/problems/problem.py:1539: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/index_files/figure-html/cell-6-output-3.png" width="808" height="505" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=== Performance Metrics ===
Method     | Bias       | RMSE      
-----------------------------------
DID        | 7.5670     | 7.5781
SCM        | 1.3289     | 1.3910
SDID       | 0.6798     | 0.7493</code></pre>
</div>
</div>
<section id="결과" class="level3">
<h3 class="anchored" data-anchor-id="결과">결과</h3>
<ul>
<li><strong>DiD</strong>: 평행 추세 가정이 깨졌기 때문에(처치군이 더 가파른 추세를 가짐), 처치 효과를 과대평가(Bias 발생)하게 됩니다.</li>
<li><strong>SCM</strong>: 대조군을 합성하여 추세를 맞추려 노력하지만, 정규화(Regularization)나 시간 가중치(Time weights)의 부재로 인해 SDiD보다는 성능이 떨어질 수 있습니다.</li>
<li><strong>SDiD</strong>: Unit weights로 추세 차이를 보정하고, Time weights로 시간적 변동성까지 잡아내어 가장 낮은 편향(Bias)과 오차(RMSE)를 보여줍니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="시뮬레이션2-극단적인-레벨-차이-level-shift" class="level1">
<h1>시뮬레이션2: 극단적인 레벨 차이 (Level Shift)</h1>
<ul>
<li>이 시나리오는 <strong>Synthetic Control Method (SCM)의 구조적 한계</strong>를 보여주기 위해 설계되었습니다.</li>
<li>처치 유닛의 결과값(<img src="https://latex.codecogs.com/png.latex?Y">)이 통제 유닛들이 형성하는 범위(Convex Hull)를 벗어나도록 극단적인 <strong>레벨 이동(Level Shift)</strong>을 가합니다.</li>
</ul>
<section id="data-generation-1" class="level2">
<h2 class="anchored" data-anchor-id="data-generation-1">1. Data Generation</h2>
<section id="데이터-생성-모형-dgp-1" class="level3">
<h3 class="anchored" data-anchor-id="데이터-생성-모형-dgp-1">데이터 생성 모형 (DGP)</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20%5Cunderbrace%7B%5Calpha_i%7D_%7B%5Ctext%7BUnit%20FE%7D%7D%20+%20%5Cunderbrace%7B%5Cbeta_t%7D_%7B%5Ctext%7BTime%20FE%7D%7D%20+%20%5Cunderbrace%7B%5Cgamma_i%20f_t%7D_%7B%5Ctext%7BTrend%20Factor%7D%7D%20+%20%5Ctau%20D_%7Bit%7D%20+%20%5Cvarepsilon_%7Bit%7D%0A"></p>
</section>
<section id="level-shift" class="level3">
<h3 class="anchored" data-anchor-id="level-shift"><strong>Level Shift</strong></h3>
<ul>
<li>처치 유닛의 고정 효과(<img src="https://latex.codecogs.com/png.latex?%5Calpha_%7Btr%7D">)를 통제 유닛들의 최댓값보다 훨씬 크게 설정합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Calpha_%7Btr%7D%20%5Cgg%20%5Cmax_%7Bj%20%5Cin%20Co%7D(%5Calpha_j)%20%5Cquad%20(%5Ctext%7BCode:%20%7D%20%5Cmax%20+%2030)%0A"></p>
<ul>
<li>SCM의 가중치는 <img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Comega_i%20=%201,%20%5Comega_i%20%5Cge%200"> 제약을 따르므로, 합성 통제군은 결코 통제 유닛 중 가장 큰 값보다 커질 수 없습니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BY%7D_%7Btr%7D%5E%7BSCM%7D%20=%20%5Csum%20%5Comega_i%20Y_%7Bi%7D%20%5Cle%20%5Cmax(Y_%7Bco%7D)%20%3C%20Y_%7Btr%7D%0A"></li>
<li>즉, <strong>절편 보정이 없는 SCM</strong>은 처치 유닛의 높은 레벨을 절대 따라잡을 수 없습니다.</li>
</ul>
<div id="0b5ee273" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_level_shifted_data(N<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, T<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>, T_pre<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span>, true_tau<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>):</span>
<span id="cb9-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> seed: np.random.seed(seed)</span>
<span id="cb9-3">    </span>
<span id="cb9-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Base Components</span></span>
<span id="cb9-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Control Units의 Alpha는 작게 설정 (0 ~ 10)</span></span>
<span id="cb9-6">    alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>N)</span>
<span id="cb9-7">    </span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Time Trend (Common)</span></span>
<span id="cb9-9">    beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, T)</span>
<span id="cb9-10">    </span>
<span id="cb9-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Level Shift (Make Treated Unit an Outlier)</span></span>
<span id="cb9-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 처치 유닛의 Alpha를 대조군 최댓값보다 훨씬 크게 설정 (+30)</span></span>
<span id="cb9-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -&gt; SCM은 가중치 합이 1이라서 죽었다 깨어나도 이 레벨을 못 맞춤</span></span>
<span id="cb9-14">    treated_idx <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> N <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 마지막 유닛을 처치군으로 지정</span></span>
<span id="cb9-15">    alpha[treated_idx] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(alpha[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span> </span>
<span id="cb9-16">    </span>
<span id="cb9-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Non-Parallel Trend Factor (Optional but good for killing DiD too)</span></span>
<span id="cb9-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 처치 유닛에게만 추가적인 성장 추세 부여</span></span>
<span id="cb9-19">    trend_factor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, T)</span>
<span id="cb9-20">    gamma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros(N)</span>
<span id="cb9-21">    gamma[treated_idx] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 처치 유닛만 더 가파르게 성장</span></span>
<span id="cb9-22">    </span>
<span id="cb9-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. Generate Y</span></span>
<span id="cb9-24">    Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.zeros((N, T))</span>
<span id="cb9-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N):</span>
<span id="cb9-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Y = Alpha + Beta + (Gamma * Trend) + Noise</span></span>
<span id="cb9-27">        Y[i, :] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> alpha[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> beta <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (gamma[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> trend_factor) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, T)</span>
<span id="cb9-28">        </span>
<span id="cb9-29">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add Treatment Effect (Post-period)</span></span>
<span id="cb9-30">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> treated_idx:</span>
<span id="cb9-31">            Y[i, T_pre:] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> true_tau</span>
<span id="cb9-32">            </span>
<span id="cb9-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Output Split</span></span>
<span id="cb9-34">    Y_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y[:<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, :]                 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (N-1) x T</span></span>
<span id="cb9-35">    Y_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, :].reshape(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1 x T</span></span>
<span id="cb9-36">    </span>
<span id="cb9-37">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Y_co, Y_tr, true_tau</span></code></pre></div></div>
</details>
</div>
<div id="8b3f2168" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">Y_co, Y_tr, true_tau <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_level_shifted_data(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb10-2">T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span></span>
<span id="cb10-3">years <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb10-4"></span>
<span id="cb10-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Run Models to get fitted values</span></span>
<span id="cb10-6">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SyntheticDiD(Y_co, Y_tr, T_pre)</span>
<span id="cb10-7"></span>
<span id="cb10-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (1) SCM Fit (Constrained)</span></span>
<span id="cb10-9">w_scm, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb10-10">Y_scm_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w_scm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Y_co</span>
<span id="cb10-11"></span>
<span id="cb10-12"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (2) SDiD Fit (Intercept Allowed)</span></span>
<span id="cb10-13">w_sdid, w0_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb10-14">Y_sdid_fit <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> w0_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> w_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Y_co</span>
<span id="cb10-15"></span>
<span id="cb10-16">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb10-17"></span>
<span id="cb10-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Raw Data</span></span>
<span id="cb10-19">plt.plot(years, Y_co.T, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.15</span>)</span>
<span id="cb10-20">plt.plot(years, Y_tr.flatten(), color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'firebrick'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Treated (Observed)'</span>)</span>
<span id="cb10-21"></span>
<span id="cb10-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SCM Fit (Failure)</span></span>
<span id="cb10-23">plt.plot(years, Y_scm_fit, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SCM Fit (No Intercept)'</span>)</span>
<span id="cb10-24"></span>
<span id="cb10-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># SDiD Fit (Success)</span></span>
<span id="cb10-26">plt.plot(years, Y_sdid_fit, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'SDiD Fit (With Intercept)'</span>)</span>
<span id="cb10-27"></span>
<span id="cb10-28">plt.axvline(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>T_pre, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'black'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">':'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Intervention'</span>)</span>
<span id="cb10-29">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SCM Limitation: Convex Hull Condition Failure"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>)</span>
<span id="cb10-30">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Time"</span>)</span>
<span id="cb10-31">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Outcome Y"</span>)</span>
<span id="cb10-32">plt.legend()</span>
<span id="cb10-33">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb10-34">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/anaconda3/envs/causal-inference-study/lib/python3.9/site-packages/cvxpy/problems/problem.py:1539: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/index_files/figure-html/cell-8-output-2.png" width="957" height="526" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="simulation-1" class="level2">
<h2 class="anchored" data-anchor-id="simulation-1">2. Simulation</h2>
<ul>
<li>앞서 정의한 <strong>Level Shift 데이터(Convex Hull 위배)</strong>를 사용하여 50회 반복 시뮬레이션을 수행합니다.</li>
</ul>
<div id="0b420fef" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Settings</span></span>
<span id="cb12-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-4">N_SIM <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span></span>
<span id="cb12-5">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>: [], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>: [], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>: []}</span>
<span id="cb12-6"></span>
<span id="cb12-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Starting Level-Shift Simulation (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>N_SIM<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> runs)..."</span>)</span>
<span id="cb12-8"></span>
<span id="cb12-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Simulation Loop</span></span>
<span id="cb12-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(N_SIM):</span>
<span id="cb12-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Generate Data (Outlier Treated Unit)</span></span>
<span id="cb12-14">    Y_co, Y_tr, true_tau <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_level_shifted_data(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>i)</span>
<span id="cb12-15">    T_pre <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span></span>
<span id="cb12-16">    </span>
<span id="cb12-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Initialize Model</span></span>
<span id="cb12-18">    model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SyntheticDiD(Y_co, Y_tr, T_pre)</span>
<span id="cb12-19">    </span>
<span id="cb12-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (1) SDiD Estimate (Intercept Allowed)</span></span>
<span id="cb12-21">    est_sdid <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.estimate()</span>
<span id="cb12-22">    </span>
<span id="cb12-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (2) SCM Estimate (Intercept False)</span></span>
<span id="cb12-24">    w_scm, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.get_unit_weights(intercept<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, zeta<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb12-25">    </span>
<span id="cb12-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Counterfactual: w * Y_co (No intercept adjustment)</span></span>
<span id="cb12-27">    y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_tr[:, T_pre:])</span>
<span id="cb12-28">    y_sc_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(w_scm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> Y_co[:, T_pre:]) </span>
<span id="cb12-29">    est_scm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_tr_post <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_sc_post</span>
<span id="cb12-30">    </span>
<span id="cb12-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (3) DiD Estimate (Standard TWFE)</span></span>
<span id="cb12-32">    diff_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_tr[:, T_pre:]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> np.mean(Y_tr[:, :T_pre])</span>
<span id="cb12-33">    diff_co <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.mean(Y_co[:, T_pre:]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> np.mean(Y_co[:, :T_pre])</span>
<span id="cb12-34">    est_did <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> diff_tr <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> diff_co</span>
<span id="cb12-35">    </span>
<span id="cb12-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save</span></span>
<span id="cb12-37">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>].append(est_sdid)</span>
<span id="cb12-38">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>].append(est_scm)</span>
<span id="cb12-39">    results[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>].append(est_did)</span>
<span id="cb12-40"></span>
<span id="cb12-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Visualization</span></span>
<span id="cb12-43"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-44">df_res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame(results)</span>
<span id="cb12-45"></span>
<span id="cb12-46">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>))</span>
<span id="cb12-47">sns.boxplot(data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df_res)</span>
<span id="cb12-48"></span>
<span id="cb12-49"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 참값(True Tau) 표시</span></span>
<span id="cb12-50">plt.axhline(y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>true_tau, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r'</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'True Effect (</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>true_tau<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)'</span>)</span>
<span id="cb12-51"></span>
<span id="cb12-52">plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Simulation Results (N=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>N_SIM<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">): Level-Shift &amp; Trend Failure"</span>)</span>
<span id="cb12-53">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Estimated Treatment Effect"</span>)</span>
<span id="cb12-54">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Method"</span>)</span>
<span id="cb12-55">plt.legend()</span>
<span id="cb12-56">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>)</span>
<span id="cb12-57">plt.show()</span>
<span id="cb12-58"></span>
<span id="cb12-59"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-60"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Performance Metrics</span></span>
<span id="cb12-61"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -------------------------------------------------------</span></span>
<span id="cb12-62">bias <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df_res.mean() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> true_tau</span>
<span id="cb12-63">rmse <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.sqrt(((df_res <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> true_tau)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>).mean())</span>
<span id="cb12-64"></span>
<span id="cb12-65"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">=== Performance Metrics (SCM Limitation Scenario) ==="</span>)</span>
<span id="cb12-66"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Method'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bias'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'RMSE'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb12-67"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"-"</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span>)</span>
<span id="cb12-68"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> method <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'did'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'scm'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sdid'</span>]:</span>
<span id="cb12-69">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>method<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>upper()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:&lt;10}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bias[method]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">     | </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>rmse[method]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Starting Level-Shift Simulation (50 runs)...</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/index_files/figure-html/cell-9-output-2.png" width="808" height="523" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
=== Performance Metrics (SCM Limitation Scenario) ===
Method     | Bias       | RMSE      
-----------------------------------
DID        | 2.5843     | 2.6168
SCM        | 34.6187     | 34.6213
SDID       | 2.6393     | 2.7111</code></pre>
</div>
</div>
<section id="결과-1" class="level3">
<h3 class="anchored" data-anchor-id="결과-1">결과</h3>
<ul>
<li><strong>SCM</strong>: Bias와 RMSE가 매우 큽니다. 이는 SCM이 처치 유닛의 높은 초기값(Level)을 합성해내지 못했기 때문입니다. 즉, <strong>“Convex Hull Condition”이 위배되면 SCM은 신뢰할 수 없습니다.</strong></li>
<li><strong>DID</strong>: 레벨 차이가 커도 차분 과정에서 사라지므로 SCM보다는 훨씬 나은 성능을 보입니다. 단, 미세한 추세 차이로 인한 편향은 남아 있습니다.</li>
<li><strong>SDiD</strong>: 절편()으로 레벨 차이를 완벽히 잡고, 가중치로 추세까지 보정하여 Bias와 RMSE 모두 가장 낮습니다.</li>
</ul>



</section>
</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L15A/SDiD/part-02/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 15A. Staggered DiD</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L15A/Staggered-DiD/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>1. Introduction</h1>
<p>전통적인 이중차분법(Difference-in-Differences, DiD)은 두 개의 그룹(Treatment/Control)과 두 개의 시점(Pre/Post)이 존재하는 2x2 Canonical Design에서 인과 효과를 추정하는 강력한 도구입니다. 그러나 실제 현실 데이터, 특히 정책 시행이나 서비스 도입과 같은 환경에서는 모든 실험군이 동시에 처치를 받는 경우는 드뭅니다.</p>
<p>대신, <strong>Staggered Adoption Design (SAD)</strong>이라 불리는, 처치 시점이 유닛마다 다른 상황이 훨씬 빈번하게 발생합니다. 예를 들어, 어떤 지역은 정책을 2020년에 도입하고, 어떤 지역은 2021년에 도입하는 식입니다.</p>
<p>이번 포스트에서는 전통적인 Two-Way Fixed Effects (TWFE) 회귀분석이 이러한 Staggered setup에서 왜 편향된(biased) 추정치를 낳는지 살펴보고, 이를 해결하기 위한 최신 방법론인 <strong>Callaway and Sant’Anna (2021)</strong>의 프레임워크를 상세히 정리해보겠습니다.</p>
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/Staggered-DiD/images/staggered_timeline.png" class="img-fluid" alt="Figure 1: Staggered Adoption의 개념도. 실험군 A, B, C가 서로 다른 시점에 처치를 받기 시작하며, Never-Treated Group은 끝까지 처치를 받지 않는다."> &gt; <strong>Figure 1 설명</strong>: Staggered Treatment Timing을 나타내는 도식입니다. &gt; - <strong>Treated Group A</strong>: 가장 먼저 처치를 받기 시작함. &gt; - <strong>Treated Group B</strong>: A보다 나중에 처치를 받기 시작함. &gt; - <strong>Treated Group C</strong>: 가장 늦게 처치를 받기 시작함. &gt; - <strong>Never-Treated Group</strong>: 관찰 기간 내내 처치를 받지 않음. &gt; - 핵심은 처치를 받은 유닛은 다시 통제 상태로 돌아가지 않는다는(Irreversible) 가정입니다.</p>
<hr>
</section>
<section id="why-twfe-fails-in-staggered-did" class="level1">
<h1>2. Why TWFE Fails in Staggered DiD?</h1>
<section id="the-limitation-of-twfe" class="level2">
<h2 class="anchored" data-anchor-id="the-limitation-of-twfe">2.1. The Limitation of TWFE</h2>
<p>일반적으로 연구자들은 패널 데이터에서 다음과 같은 <strong>Two-Way Fixed Effects (TWFE)</strong> 선형 회귀 모형을 사용하여 <img src="https://latex.codecogs.com/png.latex?%5Cbeta">를 인과 효과(ATT)로 해석해 왔습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AY_%7Bit%7D%20=%20%5Calpha_i%20+%20%5Clambda_t%20+%20%5Cbeta%20D_%7Bit%7D%20+%20%5Cepsilon_%7Bit%7D%0A"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?D_%7Bit%7D">는 유닛 <img src="https://latex.codecogs.com/png.latex?i">가 시점 <img src="https://latex.codecogs.com/png.latex?t">에 처치를 받았으면 1, 아니면 0인 더미 변수입니다. 하지만 처치 시점이 다양하고(Staggered), 처치 효과가 시간에 따라 변하거나(Dynamic), 유닛마다 다르다면(Heterogeneous), <img src="https://latex.codecogs.com/png.latex?%5Cbeta">는 우리가 원하는 <strong>Average Treatment Effect on the Treated (ATT)</strong>와 일치하지 않습니다.</p>
</section>
<section id="the-problem-of-bad-comparisons" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-of-bad-comparisons">2.2. The Problem of “Bad Comparisons”</h2>
<p>Goodman-Bacon (2021) 등의 연구에 따르면, TWFE 추정량은 가능한 모든 2x2 DiD 비교의 가중 평균으로 분해됩니다. 이때 <strong>“Bad Comparisons” (Invalid Comparisons)</strong> 문제가 발생합니다.</p>
<ul>
<li><strong>Valid Comparisons (좋은 비교):</strong>
<ul>
<li>Treated Unit vs.&nbsp;Never-treated Unit</li>
<li>Treated Unit vs.&nbsp;Not-yet-treated Unit</li>
</ul></li>
<li><strong>Invalid Comparisons (나쁜 비교):</strong>
<ul>
<li><strong>Later-treated vs.&nbsp;Earlier-treated (Already-treated):</strong> 늦게 처치를 받는 그룹을 실험군으로, 이미 처치를 받은 그룹을 통제군으로 사용하는 경우입니다.</li>
<li><strong>문제점:</strong> 만약 처치 효과가 시간에 따라 변한다면(Dynamic Effects), 이미 처치를 받은 그룹의 결과값 변화(<img src="https://latex.codecogs.com/png.latex?%5CDelta%20Y">)에는 <strong>시간 트렌드뿐만 아니라 처치 효과의 변화분</strong>이 섞여 있습니다. 이를 통제군으로 사용하면 평행 추세 가정(Parallel Trends)이 깨지게 되어 편향(Bias)이 발생합니다.</li>
</ul></li>
</ul>
<p><img src="https://shsha0110.github.io/posts/lecture/L15A/Staggered-DiD/images/outcome_trends.png" class="img-fluid" alt="Figure 2: Staggered DiD 상황에서의 Outcome 변화. A, B, C 그룹의 결과값이 처치 시점에 따라 계단식으로 상승하는 모습을 보인다."> &gt; <strong>Figure 2 설명</strong>: 각 그룹(A, B, C)의 시간에 따른 Outcome <img src="https://latex.codecogs.com/png.latex?Y">의 변화를 나타냅니다. &gt; - 점선(세모 마커)은 반사실적(Counterfactual) 상황, 즉 처치를 받지 않았을 때의 잠재적 결과를 의미합니다. &gt; - 실선(동그라미 마커)은 관측된 결과입니다. &gt; - 각 그룹이 처치를 받는 시점 이후로 Outcome이 급격히 상승하는 것을 볼 수 있으며, 이는 처치 효과의 이질성(Heterogeneity)을 시사합니다.</p>
<hr>
</section>
</section>
<section id="mathematical-framework-callaway-santanna" class="level1">
<h1>3. Mathematical Framework (Callaway &amp; Sant’Anna)</h1>
<p>이러한 문제를 해결하기 위해 Callaway &amp; Sant’Anna (2021)은 <strong>Group-Time ATT</strong>라는 개념을 도입합니다.</p>
<section id="notation-definitions" class="level2">
<h2 class="anchored" data-anchor-id="notation-definitions">3.1. Notation &amp; Definitions</h2>
<ul>
<li><strong>Time Periods:</strong> <img src="https://latex.codecogs.com/png.latex?t%20=%201,%20...,%20%5Cmathcal%7BT%7D"></li>
<li><strong>Treatment Group (<img src="https://latex.codecogs.com/png.latex?G_g">):</strong> 시점 <img src="https://latex.codecogs.com/png.latex?g">에 처음으로 처치를 받기 시작한 유닛들의 집합. (즉, <img src="https://latex.codecogs.com/png.latex?G_i%20=%20g">이면 유닛 <img src="https://latex.codecogs.com/png.latex?i">는 시점 <img src="https://latex.codecogs.com/png.latex?g">부터 처치 상태).</li>
<li><strong>Never-Treated Group (<img src="https://latex.codecogs.com/png.latex?C">):</strong> 관찰 기간 동안 처치를 받지 않은 유닛 (<img src="https://latex.codecogs.com/png.latex?G_i%20=%20%5Cinfty"> 또는 <img src="https://latex.codecogs.com/png.latex?C_i%20=%201">).</li>
<li><strong>Potential Outcomes:</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D(0)">: 시점 <img src="https://latex.codecogs.com/png.latex?t">에서 처치를 받지 않았을 때의 잠재적 결과.</li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D(g)">: 시점 <img src="https://latex.codecogs.com/png.latex?t">에서, 시점 <img src="https://latex.codecogs.com/png.latex?g">에 처치를 받기 시작했을 때의 잠재적 결과.</li>
</ul></li>
<li><strong>Observed Outcome:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%20%20Y_%7Bit%7D%20=%20Y_%7Bit%7D(0)%20%5Ccdot%20%5Cmathbb%7B1%7D%5C%7BG_i%20%3E%20t%5C%7D%20+%20Y_%7Bit%7D(G_i)%20%5Ccdot%20%5Cmathbb%7B1%7D%5C%7BG_i%20%5Cle%20t%5C%7D%0A%20%20"> 즉, 처치 전에는 <img src="https://latex.codecogs.com/png.latex?Y(0)">를, 처치 후에는 해당 처치 시점에 종속된 <img src="https://latex.codecogs.com/png.latex?Y(g)">를 관측합니다.</li>
</ul>
</section>
<section id="group-time-average-treatment-effect-attg-t" class="level2">
<h2 class="anchored" data-anchor-id="group-time-average-treatment-effect-attg-t">3.2. Group-Time Average Treatment Effect, <img src="https://latex.codecogs.com/png.latex?ATT(g,%20t)"></h2>
<p>가장 핵심적인 파라미터는 <strong>특정 처치 그룹 <img src="https://latex.codecogs.com/png.latex?g">가 특정 시점 <img src="https://latex.codecogs.com/png.latex?t">에 누리는 평균 처치 효과</strong>입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AATT(g,%20t)%20=%20%5Cmathbb%7BE%7D%5BY_t(g)%20-%20Y_t(0)%20%5Cmid%20G=g%5D%0A"></p>
<p>이 정의는 매우 직관적입니다. “시점 <img src="https://latex.codecogs.com/png.latex?g">에 처치를 받은 사람들이, 시점 <img src="https://latex.codecogs.com/png.latex?t">에 실제로 겪은 효과”를 의미합니다. 만약 <img src="https://latex.codecogs.com/png.latex?t%20%5Cge%20g">라면 처치 효과(Post-treatment effect)이고, <img src="https://latex.codecogs.com/png.latex?t%20%3C%20g">라면 처치 전 효과(Pre-treatment effect, 일반적으로 0이어야 함)가 됩니다.</p>
<hr>
</section>
</section>
<section id="identification-strategy" class="level1">
<h1>4. Identification Strategy</h1>
<p><img src="https://latex.codecogs.com/png.latex?ATT(g,%20t)">를 식별(Identification)하기 위해서는 관측되지 않은 반사실적 결과 <img src="https://latex.codecogs.com/png.latex?Y_t(0)">를 대체할 수 있는 적절한 통제군이 필요합니다. 이를 위해 <strong>Parallel Trends Assumption (평행 추세 가정)</strong>이 필요합니다.</p>
<section id="parallel-trends-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="parallel-trends-assumptions">4.1. Parallel Trends Assumptions</h2>
<p>CS(2021) 방법론은 연구자가 통제군을 어떻게 정의하느냐에 따라 두 가지 버전의 가정을 제시합니다.</p>
<section id="option-1-based-on-never-treated-units" class="level3">
<h3 class="anchored" data-anchor-id="option-1-based-on-never-treated-units">Option 1: Based on Never-Treated Units</h3>
<p>처치를 전혀 받지 않은 그룹(<img src="https://latex.codecogs.com/png.latex?C=1">)을 통제군으로 사용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bt-1%7D(0)%20%5Cmid%20G=g%5D%20=%20%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bt-1%7D(0)%20%5Cmid%20C=1%5D%0A"></p>
<ul>
<li><strong>해석:</strong> 처치가 없었다면, 그룹 <img src="https://latex.codecogs.com/png.latex?g">의 결과값 변화 추세는 Never-treated 그룹의 변화 추세와 같았을 것이다.</li>
</ul>
</section>
<section id="option-2-based-on-not-yet-treated-units" class="level3">
<h3 class="anchored" data-anchor-id="option-2-based-on-not-yet-treated-units">Option 2: Based on Not-Yet-Treated Units</h3>
<p>시점 <img src="https://latex.codecogs.com/png.latex?t">까지 아직 처치를 받지 않은 그룹들(<img src="https://latex.codecogs.com/png.latex?D_s=0">)을 통제군으로 사용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bt-1%7D(0)%20%5Cmid%20G=g%5D%20=%20%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bt-1%7D(0)%20%5Cmid%20D_t=0,%20G%20%5Cne%20g%5D%0A"></p>
<ul>
<li><strong>해석:</strong> 처치가 없었다면, 그룹 <img src="https://latex.codecogs.com/png.latex?g">의 변화 추세는 해당 시점에 아직 처치를 받지 않은 그룹들의 추세와 같았을 것이다.</li>
<li><strong>장점:</strong> Never-treated 그룹이 없거나 매우 작을 때 유용합니다.</li>
</ul>
</section>
</section>
<section id="derivation-of-the-estimator" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-the-estimator">4.2. Derivation of the Estimator</h2>
<p>Never-treated 가정을 사용할 때, <img src="https://latex.codecogs.com/png.latex?ATT(g,%20t)">는 다음과 같이 유도됩니다.</p>
<ol type="1">
<li><strong>목표:</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_t(g)%20-%20Y_t(0)%20%5Cmid%20G=g%5D"> 구하기.</li>
<li><strong>분해:</strong> 기대값의 선형성에 의해: <img src="https://latex.codecogs.com/png.latex?%0AATT(g,t)%20=%20%5Cmathbb%7BE%7D%5BY_t(g)%20%5Cmid%20G=g%5D%20-%20%5Cmathbb%7BE%7D%5BY_t(0)%20%5Cmid%20G=g%5D%0A"> 앞항 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_t(g)%20%5Cmid%20G=g%5D">는 데이터에서 관측 가능합니다(<img src="https://latex.codecogs.com/png.latex?t%20%5Cge%20g">일 때). 뒷항은 반사실적이므로 관측 불가합니다.</li>
<li><strong>평행 추세 가정 적용:</strong> <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bg-1%7D(0)%20%5Cmid%20G=g%5D%20=%20%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bg-1%7D(0)%20%5Cmid%20C=1%5D%0A"> 이를 <img src="https://latex.codecogs.com/png.latex?Y_t(0)">에 대해 정리하면(Baseline 시점을 <img src="https://latex.codecogs.com/png.latex?g-1">로 설정): <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_t(0)%20%5Cmid%20G=g%5D%20=%20%5Cmathbb%7BE%7D%5BY_%7Bg-1%7D(0)%20%5Cmid%20G=g%5D%20+%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_t(0)%20-%20Y_%7Bg-1%7D(0)%20%5Cmid%20C=1%5D%7D_%7B%5Ctext%7BControl%20Group's%20Trend%7D%7D%0A"></li>
<li><strong>최종 식:</strong> <img src="https://latex.codecogs.com/png.latex?%0AATT(g,%20t)%20=%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_t%20-%20Y_%7Bg-1%7D%20%5Cmid%20G=g%5D%7D_%7B%5Ctext%7BChange%20for%20Treated%7D%7D%20-%20%5Cunderbrace%7B%5Cmathbb%7BE%7D%5BY_t%20-%20Y_%7Bg-1%7D%20%5Cmid%20C=1%5D%7D_%7B%5Ctext%7BChange%20for%20Control%7D%7D%0A"></li>
</ol>
<p>즉, 각 그룹-시점별(<img src="https://latex.codecogs.com/png.latex?g,%20t">)로 고전적인 2x2 DiD를 수행하는 것과 같습니다. 이때 중요한 것은 <strong>“이미 처치된 그룹”을 통제군으로 섞지 않는다</strong>는 점입니다.</p>
<hr>
</section>
</section>
<section id="aggregation-summary-parameters" class="level1">
<h1>5. Aggregation: Summary Parameters</h1>
<p><img src="https://latex.codecogs.com/png.latex?ATT(g,%20t)">를 구하면 너무 많은 파라미터가 생성됩니다 (예: 그룹 5개 <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> 기간 10개 = 50개). 따라서 이를 해석 가능한 형태로 요약(Aggregation)해야 합니다.</p>
<section id="event-study-aggregation-theta_de" class="level2">
<h2 class="anchored" data-anchor-id="event-study-aggregation-theta_de">5.1. Event-Study Aggregation (<img src="https://latex.codecogs.com/png.latex?%5Ctheta_D(e)">)</h2>
<p>처치 후 경과 시간(Event time, <img src="https://latex.codecogs.com/png.latex?e">)에 따른 동태적 효과를 보고 싶을 때 사용합니다. <img src="https://latex.codecogs.com/png.latex?e%20=%20t%20-%20g"> (처치 후 경과 기간).</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7BD%7D(e)%20=%20%5Csum_%7Bg%7D%20w_g%20%5Ccdot%20ATT(g,%20g+e)%0A"> 여기서 <img src="https://latex.codecogs.com/png.latex?w_g">는 각 그룹의 샘플 비중입니다. * <img src="https://latex.codecogs.com/png.latex?e=0">: 처치 원년의 효과 * <img src="https://latex.codecogs.com/png.latex?e=1,%202,%20%5Cdots">: 처치 1년 후, 2년 후 효과 (Dynamic Effects) * <img src="https://latex.codecogs.com/png.latex?e%20%3C%200">: 처치 전 효과 (Pre-trend test로 활용 가능)</p>
</section>
<section id="simple-overall-aggregation-theta_so" class="level2">
<h2 class="anchored" data-anchor-id="simple-overall-aggregation-theta_so">5.2. Simple Overall Aggregation (<img src="https://latex.codecogs.com/png.latex?%5Ctheta_S%5EO">)</h2>
<p>단일 숫자로 전체 처치 효과를 요약하고 싶을 때 사용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctheta_%7BS%7D%5E%7BO%7D%20=%20%5Cfrac%7B1%7D%7B%5Csum%20P(G=g)%7D%20%5Csum_%7Bg%7D%20%5Csum_%7Bt%20%5Cge%20g%7D%20ATT(g,%20t)%20P(G=g)%0A"> 이는 모든 <img src="https://latex.codecogs.com/png.latex?ATT(g,t)"> (단, <img src="https://latex.codecogs.com/png.latex?t%20%5Cge%20g">)를 평균 낸 값으로, “처치를 받은 모든 기간 동안의 평균 효과”로 해석됩니다.</p>
<hr>
</section>
</section>
<section id="conclusion-checklist" class="level1">
<h1>6. Conclusion &amp; Checklist</h1>
<p>Staggered DiD 환경에서 단순 TWFE를 사용하는 것은 이질적 처치 효과(Heterogeneous Treatment Effects)가 존재할 때 심각한 편향을 초래할 수 있습니다. Callaway &amp; Sant’Anna (2021) 등의 현대적 방법론은 <strong>“Clean Control” (Never-treated or Not-yet-treated)</strong> 만을 사용하여 <img src="https://latex.codecogs.com/png.latex?ATT(g,t)">를 식별하고, 이를 적절히 집계(Aggregation)함으로써 이 문제를 해결합니다.</p>
<p>연구자들은 이제 단순히 <code>lm(y ~ treat + time + unit_fe)</code>를 돌리기보다는, 자신의 데이터 구조(처치 시점의 다양성)와 처치 효과의 특성(동태성)을 고려하여 적절한 Estimator를 선택해야 합니다.</p>
<section id="lecture-contents-checklist" class="level3">
<h3 class="anchored" data-anchor-id="lecture-contents-checklist">Lecture Contents Checklist</h3>
<p>본 포스트는 제공된 강의 자료를 기반으로 작성되었으며, 다음 항목들을 포함하고 있습니다.</p>
<ul class="task-list">
<li><label><input type="checkbox" checked=""><strong>Staggered Adoption의 정의와 시각화:</strong> (Slide 3-5)</label></li>
<li><label><input type="checkbox" checked=""><strong>기존 TWFE의 한계:</strong> “Bad Comparisons” 및 이질성 문제 (Slide 7, 10)</label></li>
<li><label><input type="checkbox" checked=""><strong>주요 용어 정의:</strong> Static/Dynamic effects, Heterogeneity (Slide 9)</label></li>
<li><label><input type="checkbox" checked=""><strong>Notation:</strong> <img src="https://latex.codecogs.com/png.latex?Y_%7Bit%7D(g)">, <img src="https://latex.codecogs.com/png.latex?G_i">, <img src="https://latex.codecogs.com/png.latex?C_i"> 등의 엄밀한 정의 (Slide 12-14)</label></li>
<li><label><input type="checkbox" checked=""><strong>핵심 가정(Assumptions):</strong> Never-treated 및 Not-yet-treated 기반의 평행 추세 가정 (Slide 15-16, 19)</label></li>
<li><label><input type="checkbox" checked=""><strong>Identification:</strong> <img src="https://latex.codecogs.com/png.latex?ATT(g,t)">의 정의 및 유도 (Slide 17-18)</label></li>
<li><label><input type="checkbox" checked=""><strong>Aggregation Methods:</strong> Event-study형 및 Overall Average형 집계 (Slide 20-21)</label></li>
<li><label><input type="checkbox" checked=""><strong>최신 대안 방법론 언급:</strong> Callaway &amp; Sant’Anna, Sun &amp; Abraham 등 (Slide 11, 23)</label></li>
</ul>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li>[1] Causality Lab. (2025). <em>Staggered Difference-in-Differences</em> [Lecture slides]. Seoul National University.</li>
<li>[2] Callaway, B., &amp; Sant’Anna, P. H. (2021). Difference-in-differences with multiple time periods. <em>Journal of Econometrics</em>, 225(2), 200-230.</li>
<li>[3] Goodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. <em>Journal of Econometrics</em>, 225(2), 254-277.</li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L15A/Staggered-DiD/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-01/</link>
  <description><![CDATA[ 





<section id="introduction-all-data-is-not-created-equal" class="level1">
<h1>1. Introduction: All Data is Not Created Equal</h1>
<ul>
<li><p>현대의 데이터 과학(Data Science)은 수많은 데이터를 다루지만, 이 데이터들이 모두 동일한 가치를 지니거나 같은 방식으로 생성된 것은 아닙니다.</p></li>
<li><p><strong>“All data is not created equal”</strong>이라는 명제는 Causal Data Science의 가장 핵심적인 출발점입니다.</p></li>
<li><p>우리가 현실에서 마주하는 데이터는 거의 예외 없이 다음과 같은 문제점들을 안고 있습니다:</p>
<ul>
<li><ol type="1">
<li><strong>상이한 실험 조건 (Different Experimental Conditions):</strong> 관찰 데이터(Observational)인가, 실험 데이터(Experimental)인가?</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>상이한 모집단 (Different Underlying Populations):</strong> 데이터가 수집된 집단이 우리가 알고자 하는 대상과 같은가?</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>비무작위 표본 추출 (Non-random Sampling):</strong> 샘플링 과정에서 편향(Bias)이 발생했는가?</li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>비무작위 처치 할당 (Non-random Treatment Assignment):</strong> 처치(Treatment)가 무작위로 배정되었는가, 아니면 선택 편향이 있는가?</li>
</ol></li>
<li><ol start="5" type="1">
<li><strong>측정되지 않은 변수 (Unmeasured Variables):</strong> 인과 관계를 파악하는 데 필요한 변수가 누락되었는가?</li>
</ol></li>
</ul></li>
<li><p>이러한 문제들로 인해 수집된 데이터는 “지저분(messy)”하며, 우리가 추론하고자 하는 <strong>Target</strong>과 완벽하게 일치하는 경우는 매우 드뭅니다.</p></li>
<li><p><strong>Causal Data Science</strong>의 목표는 이러한 이질적인(Heterogeneous) 데이터셋들을 결합하여 과학적이고 원칙적인(principled) 방법으로 인과 추론을 수행하는 것입니다.</p></li>
</ul>
<hr>
</section>
<section id="motivation-why-causal-data-science" class="level1">
<h1>2. Motivation: Why Causal Data Science?</h1>
<ul>
<li>왜 우리는 단순히 “빅데이터”를 모으는 것을 넘어, 데이터의 생성 과정을 고민해야 할까요? 아래 세 가지 주요 사례가 그 동기를 보여줍니다.</li>
</ul>
<section id="genetics-transportability" class="level2">
<h2 class="anchored" data-anchor-id="genetics-transportability">2.1. Genetics (Transportability)</h2>
<ul>
<li>쥐(Rats)를 대상으로 한 실험 연구에서 특정 물질이 발암성(Carcinogenic)이라는 결과가 나왔다고 가정해 봅시다.</li>
<li>우리의 질문은 다음과 같습니다.
<ul>
<li><strong>“이 결과가 인간에게도 그대로 적용될 것인가?”</strong></li>
</ul></li>
<li>이는 동물 모델에서 얻은 지식을 인간이라는 다른 모집단으로 옮길 수 있는지(Transportability)의 문제입니다.</li>
</ul>
</section>
<section id="advertisement-transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="advertisement-transfer-learning">2.2. Advertisement (Transfer Learning)</h2>
<ul>
<li>어떤 회사에서 ’제품 A’의 판매량을 높이기 위해 다양한 광고 전략의 효과를 분석했습니다. 이제 새로운 ’제품 B’를 출시하려고 합니다.
<ul>
<li><strong>“제품 A에서 얻은 데이터를 제품 B의 광고 전략 수립에 활용할 수 있는가?”</strong></li>
</ul></li>
<li>이는 기존 도메인의 지식을 새로운 도메인으로 전이(Transfer)하는 문제입니다.</li>
</ul>
</section>
<section id="robotics-domain-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="robotics-domain-adaptation">2.3. Robotics (Domain Adaptation)</h2>
<ul>
<li>캘리포니아 사막에서 암석을 채굴하도록 훈련된 화성 탐사 로봇(Rover)이 있습니다.
<ul>
<li><strong>“지구에서 학습한 내용을 바탕으로, 화성에서의 비용 소모적인 탐색을 최소화할 수 있는가?”</strong></li>
</ul></li>
<li>환경이 급격히 변화했을 때, 에이전트가 어떻게 적응해야 하는지에 대한 문제입니다.</li>
</ul>
<hr>
</section>
</section>
<section id="heterogeneous-datasets-formalizing-the-messiness" class="level1">
<h1>3. Heterogeneous Datasets: Formalizing the Messiness</h1>
<ul>
<li>데이터가 지저분하다는 것은 직관적이지만, 이를 수학적으로 다루기 위해서는 형식이 필요합니다.</li>
<li>우리는 데이터를 다음과 같은 속성들의 집합으로 분류할 수 있습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-01/images/heterogeneous_datasets_table.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: 이질적인 데이터셋들의 구조화. Target은 우리가 알고자 하는 인과 효과 <img src="https://latex.codecogs.com/png.latex?Q=P%5E*(y%7Cdo(x))">이다. 반면 우리가 가진 데이터들(<img src="https://latex.codecogs.com/png.latex?d_1,%20d_2,%20%5Cdots">)은 서로 다른 지역(Population), 수집 방식(Obs/Exp), 표본 추출 방식(Sampling), 측정 변수(Measured)를 가진다.</figcaption>
</figure>
</div>
<ul>
<li>위 그림(Figure 1)은 다양한 데이터셋(<img src="https://latex.codecogs.com/png.latex?d_1,%20d_2,%20%5Cdots">)이 어떻게 다른지를 보여줍니다.</li>
<li><strong>Target (<img src="https://latex.codecogs.com/png.latex?Q">):</strong> 우리의 목표는 <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))">를 알아내는 것입니다.</li>
<li><strong>Source Data:</strong>
<ul>
<li><strong>Population:</strong> LA, NY, Seoul, Boston 등 모집단이 다를 수 있습니다.</li>
<li><strong>Regime:</strong> 관찰(Observational) 데이터일 수도, 무작위 대조군 실험(RCT) 데이터일 수도 있습니다.</li>
<li><strong>Sampling:</strong> 나이(Age)나 사회경제적 지위(SES)에 따라 선택적으로 추출되었을 수 있습니다.</li>
<li><strong>Measurement:</strong> 어떤 데이터셋은 변수 <img src="https://latex.codecogs.com/png.latex?W">를 포함하지만, 다른 데이터셋은 포함하지 않을 수 있습니다.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="the-big-picture-in-causal-inference" class="level1">
<h1>4. The “Big Picture” in Causal Inference</h1>
<ul>
<li>전통적인 인과 추론(Classic Causal Inference)과 Causal Data Science가 바라보는 관점의 차이를 이해해야 합니다.</li>
</ul>
<section id="classic-causal-inference-engine" class="level2">
<h2 class="anchored" data-anchor-id="classic-causal-inference-engine">4.1. Classic Causal Inference Engine</h2>
<ul>
<li>전통적인 접근법(예: Pearl의 프레임워크)은 단일 모집단 내에서의 식별(Identifiability) 문제에 집중했습니다.
<ul>
<li><ol type="1">
<li><strong>Query:</strong> <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))"></li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Causal Diagram (Knowledge):</strong> 변수들 간의 인과 구조 (DAG).</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Data:</strong> 관찰된 데이터 분포 <img src="https://latex.codecogs.com/png.latex?P(V)">.</li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>Engine:</strong> 주어진 다이어그램에서 데이터만으로 쿼리를 계산할 수 있는지 판별(Yes/No)하고, 가능하다면 추정식(Estimand)을 도출.</li>
</ol></li>
</ul></li>
</ul>
</section>
<section id="causal-data-science-framework" class="level2">
<h2 class="anchored" data-anchor-id="causal-data-science-framework">4.2. Causal Data Science Framework</h2>
<ul>
<li>하지만 현실은 더 복잡합니다.</li>
<li>우리가 목표로 하는 <strong>Target Population (<img src="https://latex.codecogs.com/png.latex?%5CPi%5E*">)</strong>과 데이터가 수집되는 <strong>Source Populations (a, b, c, …)</strong>이 다르기 때문입니다 [cite: 104-135].</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-01/images/big_picture_graphs.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Causal Data Science의 전체 그림. 중앙의 Target Population(US)에 대한 인과 효과를 추정하고 싶지만, 가용 데이터는 구조적으로 상이한 여러 지역(NY, LA, Boston, Texas 등)에서 수집되었다. 각 지역은 Causal Graph 상에서 화살표가 추가되거나(Selection Bias), 노드가 회색으로 처리되는(Missingness) 등의 구조적 차이를 보인다.</figcaption>
</figure>
</div>
<ul>
<li>위 그림(Figure 2)에서 볼 수 있듯이, 각 소스 데이터는 구조적 차이를 가집니다.
<ul>
<li><strong>Target (US):</strong> <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20W%20%5Crightarrow%20Y">와 같은 기본 구조.</li>
<li><strong>NY:</strong> <img src="https://latex.codecogs.com/png.latex?W">가 측정되지 않음 (Grey Node).</li>
<li><strong>LA:</strong> <img src="https://latex.codecogs.com/png.latex?Z">가 선택 편향의 원인이 됨 (Selection Node <img src="https://latex.codecogs.com/png.latex?S">의 개입).</li>
<li><strong>Utah:</strong> <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">에 영향을 주지 않는 실험적 환경(Randomized).</li>
</ul></li>
<li>Causal Data Science의 목표는 이러한 <strong>Multiple, Heterogeneous Datasets</strong>를 융합(Fusion)하여 <strong>Automated Scientist</strong>처럼 타겟 질의에 답하는 것입니다.</li>
</ul>
<hr>
</section>
</section>
<section id="dimensions-and-tasks-of-causal-data-science" class="level1">
<h1>5. Dimensions and Tasks of Causal Data Science</h1>
<ul>
<li>이질적인 데이터셋을 통합적으로 다루기 위해, 데이터 수집 조건을 4가지 차원(Dimensions)의 튜플(Tuple)로 정의합니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BData%20Collection%20Tuple:%20%7D%20(d_1,%20d_2,%20d_3,%20d_4)%0A">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?d_1">: Population (모집단)</li>
<li><img src="https://latex.codecogs.com/png.latex?d_2">: Regime (관찰 vs 실험)</li>
<li><img src="https://latex.codecogs.com/png.latex?d_3">: Sampling (표본 추출 기전)</li>
<li><img src="https://latex.codecogs.com/png.latex?d_4">: Measurement (측정된 변수 집합)</li>
</ul></li>
<li>이 튜플의 변화는 우리가 해결해야 할 <strong>데이터 과학의 핵심 과제</strong>들과 1:1로 대응됩니다.</li>
</ul>
<section id="task-1-causal-inference-from-observational-studies" class="level2">
<h2 class="anchored" data-anchor-id="task-1-causal-inference-from-observational-studies">Task 1: Causal Inference (from Observational Studies)</h2>
<ul>
<li><strong>Transition:</strong> <img src="https://latex.codecogs.com/png.latex?(d_1,%20%5Ctext%7BObs%7D,%20d_3,%20d_4)%20%5Clongrightarrow%20(d_1,%20do(x),%20d_3,%20d_4)"></li>
<li><strong>Description:</strong> 가장 고전적인 인과 추론의 영역입니다. 관찰 데이터(<img src="https://latex.codecogs.com/png.latex?P(y%7Cx)">)로부터 실험적 결론(<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">)을 도출합니다. 교란 요인(Confounding Bias)을 통제하는 것이 핵심입니다.</li>
</ul>
</section>
<section id="task-2-experimental-inference-generalized-ivs" class="level2">
<h2 class="anchored" data-anchor-id="task-2-experimental-inference-generalized-ivs">Task 2: Experimental Inference (Generalized IVs)</h2>
<ul>
<li><strong>Transition:</strong> <img src="https://latex.codecogs.com/png.latex?(d_1,%20do(z),%20d_3,%20d_4)%20%5Clongrightarrow%20(d_1,%20do(x),%20d_3,%20d_4)"></li>
<li><strong>Description:</strong> 우리가 원하는 것은 <img src="https://latex.codecogs.com/png.latex?X">에 대한 개입(<img src="https://latex.codecogs.com/png.latex?do(x)">)의 효과인데, 실제 데이터는 도구 변수(Instrumental Variable, <img src="https://latex.codecogs.com/png.latex?Z">)에 대한 개입(<img src="https://latex.codecogs.com/png.latex?do(z)">)만 있는 경우입니다. 불완전한 순응(Imperfect Compliance) 문제를 다룹니다.</li>
</ul>
</section>
<section id="task-3-sampling-selection-bias" class="level2">
<h2 class="anchored" data-anchor-id="task-3-sampling-selection-bias">Task 3: Sampling Selection Bias</h2>
<ul>
<li><strong>Transition:</strong> <img src="https://latex.codecogs.com/png.latex?(d_1,%20d_2,%20%5Ctext%7BSelect%7D(Age),%20d_4)%20%5Clongrightarrow%20(d_1,%20d_2,%20%5C%7B%5C%7D,%20d_4)"></li>
<li><strong>Description:</strong> 데이터가 특정 조건(예: 나이, 소득)에 따라 편향되게 수집되었을 때, 이를 전체 모집단(Random Sample)의 분포로 복원하는 문제입니다.</li>
</ul>
</section>
<section id="task-4-transportability-external-validity" class="level2">
<h2 class="anchored" data-anchor-id="task-4-transportability-external-validity">Task 4: Transportability (External Validity)</h2>
<ul>
<li><strong>Transition:</strong> <img src="https://latex.codecogs.com/png.latex?(%5Ctext%7BBonobos%7D,%20d_2,%20d_3,%20d_4)%20%5Clongrightarrow%20(%5Ctext%7BHumans%7D,%20d_2,%20d_3,%20d_4)"></li>
<li><strong>Description:</strong> 소스 모집단(예: 보노보 원숭이, LA)에서 얻은 지식을 타겟 모집단(예: 인간, US 전체)으로 이송(Transport)하는 문제입니다. 환경적 조건의 차이를 극복해야 합니다.</li>
</ul>
</section>
<section id="summary-of-dimensions" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-dimensions">Summary of Dimensions</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Dimension</th>
<th style="text-align: left;">Problem Domain</th>
<th style="text-align: left;">Key Challenge</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>1. Experimental Cond.</strong></td>
<td style="text-align: left;">Causal Identification</td>
<td style="text-align: left;">Confounding Bias</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>2. Environmental Cond.</strong></td>
<td style="text-align: left;">Transportability</td>
<td style="text-align: left;">External Validity</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>3. Sampling Cond.</strong></td>
<td style="text-align: left;">Selection Bias</td>
<td style="text-align: left;">Sample Selection</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>4. Responding Cond.</strong></td>
<td style="text-align: left;">Missing Data</td>
<td style="text-align: left;">Recovering from Missingness</td>
</tr>
</tbody>
</table>
<ul>
<li>과거의 문헌들은 이 문제들을 각각 고립된 상태에서 특수한 모수적(parametric) 가정하에 다루었습니다.</li>
<li>하지만 <strong>Causal Data Science</strong>는 이 4가지 차원이 현실에서 복합적으로 나타난다는 점을 인식하고, 이를 통합적으로 해결하기 위한 일반화된 알고리즘과 조건을 연구합니다.</li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-01/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-02/</link>
  <description><![CDATA[ 





<section id="overview-the-challenge-of-experimental-conditions" class="level1">
<h1>Overview: The Challenge of Experimental Conditions</h1>
<ul>
<li><p>이전 포스트에서는 Causal Data Science가 다루는 4가지 차원(Dimensions)에 대해 개괄적으로 살펴보았습니다.</p></li>
<li><p>이번 포스트에서는 그 첫 번째이자 가장 기초가 되는 문제인 <strong>Experimental Conditions</strong>와 <strong>General Identifiability (g-ID)</strong>에 대해 깊이 있게 다룹니다.</p></li>
<li><p>우리가 <img src="https://latex.codecogs.com/png.latex?do(x)">에 대한 인과 효과 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 알고 싶을 때, 이상적인 상황은 <img src="https://latex.codecogs.com/png.latex?X">에 대한 무작위 대조군 실험(RCT) 데이터가 있는 것입니다.</p></li>
<li><p>하지만 현실에서는 다음과 같은 제약이 따릅니다:</p>
<ul>
<li><ol type="1">
<li><strong>윤리적/비용적 문제:</strong> <img src="https://latex.codecogs.com/png.latex?X">(예: 흡연, 유해 물질 노출)를 직접 실험할 수 없음.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>대리 실험(Surrogate Experiments):</strong> <img src="https://latex.codecogs.com/png.latex?X"> 대신 <img src="https://latex.codecogs.com/png.latex?Z">(예: 식이요법, 보조 약물)에 대한 실험 데이터만 존재함.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>이질적 데이터의 결합:</strong> 여러 개의 서로 다른 실험 결과(<img src="https://latex.codecogs.com/png.latex?do(x_1),%20do(x_2)">)를 결합하여 새로운 개입(<img src="https://latex.codecogs.com/png.latex?do(x_1,%20x_2)">)의 효과를 추정해야 함.</li>
</ol></li>
</ul></li>
<li><p>이 문제는 <strong>“우리가 가진 다양한 실험 및 관찰 데이터(<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D">)를 활용하여, 타겟 질의(<img src="https://latex.codecogs.com/png.latex?Q">)를 식별할 수 있는가?”</strong>라는 <strong>General Identifiability</strong> 문제로 귀결됩니다.</p></li>
</ul>
<hr>
</section>
<section id="z-id-experimental-identifiability" class="level1">
<h1>2. z-ID: Experimental Identifiability</h1>
<section id="motivation-diet-cholesterol-and-heart-attack" class="level2">
<h2 class="anchored" data-anchor-id="motivation-diet-cholesterol-and-heart-attack">2.1. Motivation: Diet, Cholesterol, and Heart Attack</h2>
<ul>
<li>가장 단순한 형태의 대리 실험 문제를 살펴보겠습니다.</li>
<li>우리의 목표는 콜레스테롤 수치(<img src="https://latex.codecogs.com/png.latex?X">)가 심장마비(<img src="https://latex.codecogs.com/png.latex?Y">)에 미치는 인과 효과 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 알아내는 것입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-02/images/zid_motivation_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: z-ID의 기본 모티베이션 그래프. Z(식이요법)는 X(콜레스테롤)에 영향을 주고, X는 Y(심장마비)에 영향을 준다. 점선 화살표는 측정되지 않은 교란 변수(Confounder)를 의미한다. Z와 Y 사이의 직접적인 화살표가 없다는 점(Exclusion Restriction)이 중요하다.</figcaption>
</figure>
</div>
<ul>
<li><strong>Query:</strong> <img src="https://latex.codecogs.com/png.latex?Q%20=%20P(y%7Cdo(x))"></li>
<li><strong>Problem:</strong> <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에 관측되지 않은 교란 요인(Confounder)이 존재하여(위 그림의 <img src="https://latex.codecogs.com/png.latex?X%20%5Cleftrightarrow%20Y"> 점선), 관찰 데이터 <img src="https://latex.codecogs.com/png.latex?P(x,y,z)">만으로는 <img src="https://latex.codecogs.com/png.latex?Q">를 식별할 수 없습니다.</li>
<li><strong>Available Data:</strong>
<ul>
<li>Observational: <img src="https://latex.codecogs.com/png.latex?P(x,y,z)"></li>
<li>Experimental (Surrogate): <img src="https://latex.codecogs.com/png.latex?P(x,y%7Cdo(z))"> — 식이요법(<img src="https://latex.codecogs.com/png.latex?Z">)은 실험 가능함.</li>
</ul></li>
<li>이 경우, <img src="https://latex.codecogs.com/png.latex?Z">에 대한 실험 데이터를 사용하여 <img src="https://latex.codecogs.com/png.latex?X">의 효과를 식별할 수 있을까요? 이를 <strong>z-ID</strong> 문제라고 합니다.</li>
</ul>
</section>
<section id="instrumental-variable-formula-derived-from-experiments" class="level2">
<h2 class="anchored" data-anchor-id="instrumental-variable-formula-derived-from-experiments">2.2. Instrumental Variable Formula derived from Experiments</h2>
<p>만약 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">를 통해서만 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 미친다면(즉, <img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20Y"> 직접 경로가 없고, <img src="https://latex.codecogs.com/png.latex?Z">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 교란이 없다면), 우리는 다음 식을 유도할 수 있습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cdo(x))%20=%20%5Cfrac%7BP(x,y%7Cdo(z))%7D%7BP(x%7Cdo(z))%7D%20=%20P(y%7Cx,%20do(z))%0A"></p>
<section id="derivation" class="level3">
<h3 class="anchored" data-anchor-id="derivation">Derivation</h3>
<ul>
<li>이 식은 Do-Calculus 규칙을 적용하여 단계적으로 유도할 수 있습니다.</li>
<li>핵심은 <img src="https://latex.codecogs.com/png.latex?do(x)">라는 가상의 개입을 <img src="https://latex.codecogs.com/png.latex?do(z)">라는 실제 가능한 실험으로 변환하는 것입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cdo(x))%20%5Cxrightarrow%7B%5Ctext%7BStep%201%7D%7D%20P(y%7Cdo(x),%20do(z))%20%5Cxrightarrow%7B%5Ctext%7BStep%202%7D%7D%20P(y%7Cx,%20do(z))%0A"></p>
<section id="step-1-배제-제약-exclusion-restriction-적용" class="level4">
<h4 class="anchored" data-anchor-id="step-1-배제-제약-exclusion-restriction-적용">Step 1: 배제 제약 (Exclusion Restriction) 적용</h4>
<p><img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))%20=%20P(y%7Cdo(x),%20do(z))"></p>
<ul>
<li><strong>논리:</strong> 도구변수의 핵심 가정에 따르면 <img src="https://latex.codecogs.com/png.latex?Z">는 오직 <img src="https://latex.codecogs.com/png.latex?X">를 통해서만 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 줍니다.</li>
<li><strong>해석:</strong> 이미 <img src="https://latex.codecogs.com/png.latex?X">를 <img src="https://latex.codecogs.com/png.latex?do(x)">로 고정하여 <img src="https://latex.codecogs.com/png.latex?Y">에 대한 <img src="https://latex.codecogs.com/png.latex?X">의 영향력을 통제하고 있다면, <img src="https://latex.codecogs.com/png.latex?Z">를 추가로 <img src="https://latex.codecogs.com/png.latex?do(z)">로 고정하더라도 <img src="https://latex.codecogs.com/png.latex?Y">의 분포에는 아무런 변화가 없습니다. (<img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20Y"> 직접 경로 부재)</li>
</ul>
</section>
<section id="step-2-관측과-개입의-교환-rule-2" class="level4">
<h4 class="anchored" data-anchor-id="step-2-관측과-개입의-교환-rule-2">Step 2: 관측과 개입의 교환 (Rule 2)</h4>
<p><img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x),%20do(z))%20=%20P(y%7Cx,%20do(z))"></p>
<ul>
<li><strong>논리:</strong> <img src="https://latex.codecogs.com/png.latex?do(z)">가 수행된 실험적 환경에서는 <img src="https://latex.codecogs.com/png.latex?Z">가 무작위로 할당되므로, <img src="https://latex.codecogs.com/png.latex?Z">로 인해 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 교란(confounding) 경로가 차단됩니다.</li>
<li><strong>해석:</strong> 교란 요인이 없는 환경(<img src="https://latex.codecogs.com/png.latex?do(z)">) 하에서는, <img src="https://latex.codecogs.com/png.latex?X">를 강제로 고정(<img src="https://latex.codecogs.com/png.latex?do(x)">)했을 때의 결과나, 자연스럽게 <img src="https://latex.codecogs.com/png.latex?X=x">가 된 것을 관측했을 때의 결과가 동일합니다. 즉, <img src="https://latex.codecogs.com/png.latex?do(x)">를 조건부 확률 <img src="https://latex.codecogs.com/png.latex?x">로 바꿀 수 있습니다.</li>
</ul>
</section>
<section id="step-3-조건부-확률-정의-bayes-rule" class="level4">
<h4 class="anchored" data-anchor-id="step-3-조건부-확률-정의-bayes-rule">Step 3: 조건부 확률 정의 (Bayes’ Rule)</h4>
<ul>
<li>최종적으로 조건부 확률의 정의에 따라 우변을 다시 씁니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?P(y%7Cx,%20do(z))%20=%20%5Cfrac%7BP(x,y%7Cdo(z))%7D%7BP(x%7Cdo(z))%7D"></p>
<ul>
<li>이로써 우리가 실험 데이터(<img src="https://latex.codecogs.com/png.latex?do(z)">)를 통해 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 인과적 효과(<img src="https://latex.codecogs.com/png.latex?do(x)">)를 식별(Identify)할 수 있음이 증명됩니다.</li>
</ul>
</section>
</section>
</section>
<section id="subtleties-of-z-id" class="level2">
<h2 class="anchored" data-anchor-id="subtleties-of-z-id">2.3. Subtleties of z-ID</h2>
<ul>
<li>모든 경우에 대리 실험이 유효한 것은 아닙니다. 그래프 구조에 따라 식별 가능 여부가 달라집니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-02/images/zid_vs_nonzid.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: z-ID 가능 그래프와 불가능 그래프의 비교. 상단 그래프들은 Z에 대한 실험으로 X의 효과를 식별할 수 있는 경우(z-ID)이고, 하단 그래프들은 식별 불가능한 경우(non-z-ID)이다. 핵심은 Z의 개입이 X와 Y 사이의 교란 요인을 통제하거나 우회할 수 있는지 여부이다.</figcaption>
</figure>
</div>
<ul>
<li>위 그림에서 <strong>z-ID</strong>가 가능한 경우와 그렇지 않은 경우(non-z-ID)를 구분하는 것은, <img src="https://latex.codecogs.com/png.latex?Z">에 대한 개입이 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y"> 관계를 교란하는 뒷문 경로(Back-door path)를 차단하거나, <img src="https://latex.codecogs.com/png.latex?X">의 변동을 충분히 설명할 수 있는지와 관련이 있습니다.</li>
</ul>
<hr>
</section>
</section>
<section id="advanced-derivation-combining-causal-mechanisms-z-id" class="level1">
<h1>3. Advanced Derivation: Combining Causal Mechanisms (z-ID)</h1>
<ul>
<li>복잡한 인과 그래프에서 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D)">(관측 데이터)만으로는 식별 불가능한 효과를, <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D%7Cdo(z))">(실험 데이터)를 통해 어떻게 계산해 낼 수 있는지 단계별로 유도합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-02/images/complex_zid_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: z-ID 문제 상황. <img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20X%20%5Cto%20W%20%5Cto%20Y"> 경로 외에 점선으로 표시된 다양한 교란 요인(Confounder)들이 존재하여, 일반적인 Back-door criterion으로는 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 구할 수 없다.</figcaption>
</figure>
</div>
<section id="problem-definition-goal" class="level2">
<h2 class="anchored" data-anchor-id="problem-definition-goal">1. Problem Definition &amp; Goal</h2>
<ul>
<li><strong>Target:</strong> <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))"></li>
<li><strong>Challenge:</strong> 그래프에 존재하는 많은 교란 경로(Bi-directed arcs) 때문에, 관측 데이터 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D)">만으로는 이 효과를 식별할 수 없습니다 (<strong>Non-identifiable from <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D)"></strong>).</li>
<li><strong>Solution:</strong> <img src="https://latex.codecogs.com/png.latex?Z">에 대한 실험 데이터, 즉 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D%7Cdo(z))">가 가용하다면(Available), 이를 활용해 타겟 효과를 계산할 수 있습니다.</li>
</ul>
</section>
<section id="derivation-step-1-decomposition-c-component" class="level2">
<h2 class="anchored" data-anchor-id="derivation-step-1-decomposition-c-component">2. Derivation Step 1: Decomposition (C-Component)</h2>
<ul>
<li>우선 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 중간 매개변수 <img src="https://latex.codecogs.com/png.latex?W">를 이용하여 두 개의 부분 문제(<img src="https://latex.codecogs.com/png.latex?Q%5BY%5D">와 <img src="https://latex.codecogs.com/png.latex?Q%5BW%5D">)로 분해합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP(y%7Cdo(x))%20&amp;=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20w)%20P(w%7Cdo(x))%20%5Cquad%20%5Ctext%7B(Law%20of%20Total%20Probability)%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20do(w))%20P(w%7Cdo(x))%20%5Cquad%20%5Ctext%7B(Rule%202:%20Action/Observation%20Exchange%20of%20W)%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bw%7D%20%5Cunderbrace%7BP(y%7Cdo(w))%7D_%7BQ%5BY%5D%7D%20%5Cunderbrace%7BP(w%7Cdo(x))%7D_%7BQ%5BW%5D%7D%20%5Cquad%20%5Ctext%7B(Rule%203:%20Removing%20Action%20of%20%7D%20X%20%5Ctext%7B)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li><strong>해석:</strong> <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 영향은 <img src="https://latex.codecogs.com/png.latex?W">를 통하는 경로밖에 없으므로(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20W%20%5Cto%20Y">), <img src="https://latex.codecogs.com/png.latex?Y">에 대해 <img src="https://latex.codecogs.com/png.latex?W">를 직접 조작(<img src="https://latex.codecogs.com/png.latex?do(w)">)한다면 <img src="https://latex.codecogs.com/png.latex?X">의 조작(<img src="https://latex.codecogs.com/png.latex?do(x)">) 여부는 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 주지 않습니다.</li>
<li>이제 우리는 두 가지 항 <img src="https://latex.codecogs.com/png.latex?Q%5BY%5D">와 <img src="https://latex.codecogs.com/png.latex?Q%5BW%5D">를 각각 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D%7Cdo(z))">로 표현하면 됩니다.</li>
</ul>
</section>
<section id="derivation-step-2-identification-from-pmathbfvdoz" class="level2">
<h2 class="anchored" data-anchor-id="derivation-step-2-identification-from-pmathbfvdoz">3. Derivation Step 2: Identification from <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7BV%7D%7Cdo(z))"></h2>
<ul>
<li>이미지의 유도 과정을 따라 각 항을 실험 분포 <img src="https://latex.codecogs.com/png.latex?P(%5Ccdot%7Cdo(z))">로 변환합니다.</li>
</ul>
<section id="a.-identifying-qy-pydow" class="level3">
<h3 class="anchored" data-anchor-id="a.-identifying-qy-pydow">A. Identifying <img src="https://latex.codecogs.com/png.latex?Q%5BY%5D%20=%20P(y%7Cdo(w))"></h3>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y">에 대한 <img src="https://latex.codecogs.com/png.latex?W">의 효과를 구하는 과정입니다. <img src="https://latex.codecogs.com/png.latex?W">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에도 교란이 있으므로 <img src="https://latex.codecogs.com/png.latex?Z">를 도구로 사용합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQ%5BY%5D%20&amp;=%20P(y%7Cdo(w))%20%5C%5C%0A&amp;=%20P(y%7Cdo(w,%20z))%20%5Cquad%20%5Ctext%7B(Rule%203:%20Adding%20Action%20of%20%7D%20Z%20%5Ctext%7B)%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%7D%20P(y%7Cdo(w,%20z),%20x)%20P(x%7Cdo(w,%20z))%20%5Cquad%20%5Ctext%7B(Law%20of%20Total%20Probability%20on%20%7D%20X%20%5Ctext%7B)%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%7D%20P(y%7Cdo(w,%20z),%20x)%20P(x%7Cdo(z))%20%5Cquad%20%5Ctext%7B(Rule%203:%20Removing%20Action%20of%20%7D%20W%20%5Ctext%7B)%7D%20%5C%5C%0A&amp;=%20%5Csum_%7Bx%7D%20%5Cunderbrace%7BP(y%7Cdo(z),%20w,%20x)%7D_%7B%5Ctext%7BAvailable%20in%20Data%7D%7D%20%5Cunderbrace%7BP(x%7Cdo(z))%7D_%7B%5Ctext%7BAvailable%20in%20Data%7D%7D%20%5Cquad%20%5Ctext%7B(Rule%202:%20Action/Observation%20Exchange%20of%20%7D%20W%20%5Ctext%7B)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li><strong>핵심:</strong> <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z),%20w,%20x)">는 <img src="https://latex.codecogs.com/png.latex?do(z)"> 실험 데이터에서 관측 가능한 조건부 확률입니다. 즉, <img src="https://latex.codecogs.com/png.latex?do(w)">라는 가상의 개입을 관측값 <img src="https://latex.codecogs.com/png.latex?w">로 치환하는 데 성공했습니다.</li>
</ul>
</section>
<section id="b.-identifying-qw-pwdox" class="level3">
<h3 class="anchored" data-anchor-id="b.-identifying-qw-pwdox">B. Identifying <img src="https://latex.codecogs.com/png.latex?Q%5BW%5D%20=%20P(w%7Cdo(x))"></h3>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?W">에 미치는 효과를 구하는 과정입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AQ%5BW%5D%20&amp;=%20P(w%7Cdo(x))%20%5C%5C%0A&amp;=%20P(w%7Cdo(x,%20z))%20%5Cquad%20%5Ctext%7B(Rule%203:%20Adding%20Action%20of%20%7D%20Z%20%5Ctext%7B)%7D%20%5C%5C%0A&amp;=%20%5Cunderbrace%7BP(w%7Cdo(z),%20x)%7D_%7B%5Ctext%7BAvailable%20in%20Data%7D%7D%20%5Cquad%20%5Ctext%7B(Rule%202:%20Action/Observation%20Exchange%20of%20%7D%20X%20%5Ctext%7B)%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li><strong>핵심:</strong> <img src="https://latex.codecogs.com/png.latex?do(z)"> 환경에서는 <img src="https://latex.codecogs.com/png.latex?X">에서 <img src="https://latex.codecogs.com/png.latex?W">로 가는 경로의 교란이 해결되므로, <img src="https://latex.codecogs.com/png.latex?do(x)">를 관측값 <img src="https://latex.codecogs.com/png.latex?x">로 바꿀 수 있습니다.</li>
</ul>
</section>
</section>
<section id="final-formula" class="level2">
<h2 class="anchored" data-anchor-id="final-formula">4. Final Formula</h2>
<ul>
<li>위의 두 결과를 결합하면 최종적으로 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 실험 데이터 <img src="https://latex.codecogs.com/png.latex?do(z)">만으로 계산하는 식(calculus)이 완성됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cdo(x))%20=%20%5Csum_%7Bw%7D%20%5Cleft%5B%20%5Cleft(%20%5Csum_%7Bx'%7D%20P(y%7Cdo(z),%20w,%20x')P(x'%7Cdo(z))%20%5Cright)%20%5Ctimes%20P(w%7Cdo(z),%20x)%20%5Cright%5D%0A"></p>
<ul>
<li><strong>결론:</strong> 이처럼 복잡한 인과 그래프에서도 문제를 더 작은 단위(C-component)로 쪼개고, 각 단위를 가용한 실험 데이터(<img src="https://latex.codecogs.com/png.latex?do(z)">)로 환원(Reduce)시킴으로써 인과 효과를 식별해 낼 수 있습니다.</li>
</ul>
<hr>
</section>
</section>
<section id="example-drug-drug-interactions-combining-experiments" class="level1">
<h1>4. Example: Drug-Drug Interactions (Combining Experiments)</h1>
<ul>
<li>이 이론의 가장 강력한 응용 사례는 <strong>약물 상호작용(Drug-Drug Interaction)</strong> 분석입니다.</li>
<li>개별 약물 실험 데이터만 있을 때, 두 약물을 동시에 처방했을 때의 효과를 예측할 수 있을까요?</li>
</ul>
<section id="problem-setup" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup">4.1. Problem Setup</h2>
<ul>
<li><strong>Variables:</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X_1">: 고혈압 치료제 (Anti-hypertensive drug)</li>
<li><img src="https://latex.codecogs.com/png.latex?X_2">: 당뇨 치료제 (Anti-diabetic drug)</li>
<li><img src="https://latex.codecogs.com/png.latex?B">: 혈압 (Blood pressure)</li>
<li><img src="https://latex.codecogs.com/png.latex?Y">: 심혈관 질환 (CVD)</li>
</ul></li>
<li><strong>Data Sources:</strong>
<ul>
<li>Study 1: <img src="https://latex.codecogs.com/png.latex?X_1">에 대한 RCT <img src="https://latex.codecogs.com/png.latex?%5Crightarrow%20P(v%7Cdo(x_1))"></li>
<li>Study 2: <img src="https://latex.codecogs.com/png.latex?X_2">에 대한 RCT <img src="https://latex.codecogs.com/png.latex?%5Crightarrow%20P(v%7Cdo(x_2))"></li>
</ul></li>
<li><strong>Target Query:</strong>
<ul>
<li>Joint Intervention: <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x_1,%20x_2))"></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-02/images/drug_interaction_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: 약물 상호작용 인과 그래프. X1은 B에 영향을 주고, B는 Y에 영향을 준다. X2는 Y에 직접 영향을 준다. X1과 B, X2와 Y, X1와 X2 사이에는 교란 요인(점선)이 존재한다. 목표는 X1과 X2를 동시에 개입했을 때 Y의 분포를 구하는 것이다.</figcaption>
</figure>
</div>
</section>
<section id="derivation-1" class="level2">
<h2 class="anchored" data-anchor-id="derivation-1">4.2. Derivation</h2>
<ul>
<li>우리는 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x_1,%20x_2))">를 구해야 합니다. 혈압 <img src="https://latex.codecogs.com/png.latex?B">가 <img src="https://latex.codecogs.com/png.latex?X_1">과 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 매개체 역할을 한다는 점에 착안하여 식을 전개합니다.</li>
</ul>
<ol type="1">
<li><p><strong>Total Probability Theorem over B:</strong> <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x_1,%20x_2))%20=%20%5Csum_%7Bb%7D%20P(y%7Cdo(x_1,%20x_2),%20b)%20P(b%7Cdo(x_1,%20x_2))"></p></li>
<li><p><strong>Factor 1: <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x_1,%20x_2),%20b)"></strong></p>
<ul>
<li>그래프에서 <img src="https://latex.codecogs.com/png.latex?X_1">은 <img src="https://latex.codecogs.com/png.latex?B">로 가는 화살표를 제외하면 <img src="https://latex.codecogs.com/png.latex?Y">에 직접 영향을 주지 않습니다(Block).</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?X_1">을 조건부에서 제거할 수 있습니다(Rule 3). <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x_1,%20x_2),%20b)%20=%20P(y%7Cdo(x_2),%20b)"></li>
<li>이 항은 <strong>Study 2 (<img src="https://latex.codecogs.com/png.latex?do(x_2)">)</strong> 데이터에서 <img src="https://latex.codecogs.com/png.latex?B">를 관측함으로써 얻을 수 있습니다 (<img src="https://latex.codecogs.com/png.latex?P_%7Bx_2%7D(y%7Cb)">).</li>
</ul></li>
<li><p><strong>Factor 2: <img src="https://latex.codecogs.com/png.latex?P(b%7Cdo(x_1,%20x_2))"></strong></p>
<ul>
<li>그래프에서 <img src="https://latex.codecogs.com/png.latex?X_2">는 <img src="https://latex.codecogs.com/png.latex?B">에 영향을 주지 않습니다 (<img src="https://latex.codecogs.com/png.latex?Y">에만 영향).</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?X_2">를 제거할 수 있습니다. <img src="https://latex.codecogs.com/png.latex?P(b%7Cdo(x_1,%20x_2))%20=%20P(b%7Cdo(x_1))"></li>
<li>이 항은 <strong>Study 1 (<img src="https://latex.codecogs.com/png.latex?do(x_1)">)</strong> 데이터에서 바로 얻을 수 있습니다 (<img src="https://latex.codecogs.com/png.latex?P_%7Bx_1%7D(b)">).</li>
</ul></li>
</ol>
<section id="final-formula-1" class="level3">
<h3 class="anchored" data-anchor-id="final-formula-1">4.3. Final Formula</h3>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cdo(x_1,%20x_2))%20=%20%5Csum_%7Bb%7D%20P_%7Bx_1%7D(b)%20P_%7Bx_2%7D(y%7Cb)%0A"></p>
<ul>
<li>이 결과는 매우 강력합니다.</li>
<li><strong>두 약물을 동시에 사용하는 실험을 한 번도 수행하지 않았음에도</strong>, 개별 약물 실험 데이터를 수학적으로 결합하여 그 효과를 정확히 예측할 수 있기 때문입니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="algorithm-for-general-identifiability-g-id" class="level1">
<h1>5. Algorithm for General Identifiability (g-ID)</h1>
<ul>
<li>위의 사례들을 일반화하면 <strong>General Identifiability (g-ID)</strong> 알고리즘을 만들 수 있습니다.</li>
</ul>
<section id="the-g-id-algorithm-flow" class="level2">
<h2 class="anchored" data-anchor-id="the-g-id-algorithm-flow">The g-ID Algorithm Flow</h2>
<ol type="1">
<li><strong>Decomposition (분해):</strong>
<ul>
<li>주어진 쿼리 <img src="https://latex.codecogs.com/png.latex?Q%20=%20P_x(y)">를 Causal Graph의 구조(C-components)를 이용하여 더 작은 <strong>Factors (요인들)</strong>의 곱과 합(<img src="https://latex.codecogs.com/png.latex?%5Csum%20%5Cprod">)으로 분해합니다. <img src="https://latex.codecogs.com/png.latex?Q%20=%20%5Csum%20%5Cprod%20P_%7B%5Cbullet%7D(%5Cbullet)"></li>
</ul></li>
<li><strong>Identification (식별):</strong>
<ul>
<li>분해된 각 Factor가 가용한 데이터 소스 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%20=%20%5C%7B%20P(obs),%20P(do(z_1)),%20P(do(z_2)),%20%5Cdots%20%5C%7D"> 중 하나로부터 식별 가능한지 확인합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-02/images/gid_algorithm_flow.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: g-ID 알고리즘의 도식화. 쿼리(좌측)가 여러 Factor로 분해되고, 각 Factor가 우측의 가용 데이터 소스(P_z1, P_zm…) 중 하나와 매칭되어 식별되는 과정을 보여준다.</figcaption>
</figure>
</div></li>
<li><strong>Conclusion:</strong>
<ul>
<li>만약 모든 Factor가 데이터 소스로부터 식별 가능하다면 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>Identifiable</strong>.</li>
<li>하나라도 식별 불가능한 Factor가 남는다면 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>Fail</strong>.</li>
</ul></li>
</ol>
</section>
<section id="significance" class="level2">
<h2 class="anchored" data-anchor-id="significance">Significance</h2>
<ul>
<li>이 알고리즘과 Do-calculus는 Experimental Identifiability 문제에 대해 <strong>Completeness(완전성)</strong>를 가집니다.</li>
<li>즉, 이 알고리즘으로 식별할 수 없다면, 그 인과 효과는 주어진 데이터와 그래프 가정하에서는 이론적으로 식별이 불가능한 것입니다.</li>
</ul>
<hr>
</section>
</section>
<section id="conclusion" class="level1">
<h1>6. Conclusion</h1>
<ul>
<li>이번 포스트에서는 Causal Data Science의 첫 번째 차원인 <strong>Experimental Conditions</strong>를 다루었습니다.
<ul>
<li><strong>z-ID:</strong> 직접 실험할 수 없는 변수의 효과를 대리 실험(<img src="https://latex.codecogs.com/png.latex?Z">)을 통해 식별하는 방법.</li>
<li><strong>Data Fusion:</strong> 서로 다른 실험 데이터(<img src="https://latex.codecogs.com/png.latex?do(x_1),%20do(x_2)">)를 결합하여 새로운 인과 효과를 추론하는 메커니즘.</li>
<li><strong>g-ID Algorithm:</strong> 이를 일반화하여 복잡한 쿼리를 분해하고 가용 데이터와 매핑하는 체계적인 방법론.</li>
</ul></li>
</ul>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-02/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-03/</link>
  <description><![CDATA[ 





<section id="introduction-moving-from-lab-to-real-world" class="level1">
<h1>1. Introduction: Moving from Lab to Real-World</h1>
<ul>
<li><p>데이터 과학과 인과 추론에서 가장 빈번하면서도 어려운 질문 중 하나는 <strong>“어떤 환경(Source)에서 얻은 지식을 다른 환경(Target)에 적용할 수 있는가?”</strong>입니다.</p></li>
<li><p>예를 들어:</p>
<ul>
<li>미국의 교육 정책 효과를 한국에 그대로 적용할 수 있는가?</li>
<li>LA에서 수행된 임상 시험 결과를 NYC의 환자들에게 적용할 수 있는가?</li>
<li>통제된 실험실 환경(Lab)의 로봇 학습 데이터를 실제 도로(Real-World)에 쓸 수 있는가?</li>
</ul></li>
<li><p>이 문제는 사회과학에서는 <strong>외부 타당성(External Validity)</strong>, 통계학에서는 <strong>일반화(Generalizability)</strong>, 머신러닝에서는 <strong>도메인 적응(Domain Adaptation)</strong> 등으로 불려왔습니다.</p></li>
<li><p><strong>Causal Data Science</strong>는 이 문제를 <strong>Transportability(이송 가능성)</strong>라는 수학적 프레임워크로 정의하고, <strong>Selection Diagram</strong>이라는 도구를 통해 데이터가 언제, 어떻게 이송 가능한지를 공식화합니다.</p></li>
</ul>
<hr>
</section>
<section id="the-transportability-problem" class="level1">
<h1>2. The Transportability Problem</h1>
<ul>
<li>우리의 목표는 Source Domain(<img src="https://latex.codecogs.com/png.latex?%5CPi">)에서 수집된 관찰(<img src="https://latex.codecogs.com/png.latex?P">) 및 실험(<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">) 데이터를 사용하여, Target Domain(<img src="https://latex.codecogs.com/png.latex?%5CPi%5E*">)에서의 인과 효과 <img src="https://latex.codecogs.com/png.latex?Q%20=%20P%5E*(y%7Cdo(x))">를 계산하는 것입니다.</li>
</ul>
<section id="trivial-vs.-non-trivial-cases" class="level2">
<h2 class="anchored" data-anchor-id="trivial-vs.-non-trivial-cases">2.1. Trivial vs.&nbsp;Non-Trivial Cases</h2>
<ul>
<li><p>가장 단순한 가정(<img src="https://latex.codecogs.com/png.latex?H_0">)은 Source와 Target의 모든 조건이 동일하다는 것입니다.</p></li>
<li><p>이 경우 결과는 자명하게 이식 가능합니다(Trivially Transportable).</p></li>
<li><p>하지만 현실(<img src="https://latex.codecogs.com/png.latex?H_a">)에서는 두 도메인 간에 차이가 존재합니다.</p>
<ul>
<li><strong>분포의 차이:</strong> <img src="https://latex.codecogs.com/png.latex?f_z%20%5Cneq%20f%5E*_z"> (예: LA와 NYC의 연령 분포가 다름)</li>
<li><strong>메커니즘의 차이:</strong> <img src="https://latex.codecogs.com/png.latex?f_y%20%5Cneq%20f%5E*_y"> (예: 동일한 치료제라도 인종적 특성에 따라 반응률이 다름)</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/transportability_spectrum.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Source Domain과 Target Domain의 차이. 모든 구조적 방정식(Structural Equations) f가 동일하다면(H0) 문제는 간단하지만, 현실(Ha)에서는 변수들의 분포나 메커니즘이 다르다. 이를 ’Spectrum’으로 표현할 수 있다.</figcaption>
</figure>
</div>
<ul>
<li>위 그림(Figure 1)은 Source와 Target 사이에 구조적 불일치가 존재할 때, 단순한 데이터 결합이 불가능함을 보여줍니다.</li>
</ul>
<hr>
</section>
</section>
<section id="selection-diagrams-encoding-differences" class="level1">
<h1>3. Selection Diagrams: Encoding Differences</h1>
<ul>
<li>도메인 간의 차이를 체계적으로 표현하기 위해 Pearl과 Bareinboim은 <strong>Selection Diagram</strong>을 도입했습니다.</li>
</ul>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">Definition</h2>
<ul>
<li>Selection Diagram은 기존의 Causal Graph <img src="https://latex.codecogs.com/png.latex?G">에 <strong>Selection Node (<img src="https://latex.codecogs.com/png.latex?S">)</strong>를 추가한 확장된 그래프입니다.</li>
<li><strong>Selection Node (Yellow Square):</strong> 특정 변수의 메커니즘이 도메인 간에 차이가 있음을 나타냅니다.</li>
<li>만약 변수 <img src="https://latex.codecogs.com/png.latex?V">에 대해 <img src="https://latex.codecogs.com/png.latex?f_V%20%5Cneq%20f%5E*_V">라면, <img src="https://latex.codecogs.com/png.latex?S%20%5Cto%20V"> 화살표를 추가합니다.</li>
<li>반대로, <img src="https://latex.codecogs.com/png.latex?S">가 가리키지 않는 변수는 도메인 간에 메커니즘이 동일(Invariant)하다고 가정합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/selection_diagram_definition.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Selection Diagram의 예시. 상단 그래프(G)는 일반적인 인과 그래프이고, 하단 우측 그래프(D)는 Selection Diagram이다. Z와 Y에 노란색 사각형(Selection Node)이 화살표를 보내고 있다. 이는 Z의 분포와 Y를 결정하는 메커니즘이 도메인 간에 다르다는 것을 의미한다. 반면 X와 W는 Selection Node가 없으므로 두 도메인에서 동일한 메커니즘을 가진다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="deriving-transport-formulas" class="level1">
<h1>4. Deriving Transport Formulas</h1>
<ul>
<li>Selection Diagram을 사용하면, 타겟 도메인의 데이터를 사용하지 않고도(혹은 일부만 사용하여) 타겟의 인과 효과를 계산하는 <strong>Transport Formula</strong>를 유도할 수 있습니다.</li>
</ul>
<section id="the-general-theorem-reduction-to-calculus" class="level2">
<h2 class="anchored" data-anchor-id="the-general-theorem-reduction-to-calculus">4.1. The General Theorem: Reduction to Calculus</h2>
<ul>
<li>Pearl &amp; Bareinboim은 Transportability 문제를 해결하기 위한 일반적인 정리를 제시했습니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem:</strong> A causal relation <img src="https://latex.codecogs.com/png.latex?Q"> is transportable from <img src="https://latex.codecogs.com/png.latex?%5CPi"> to <img src="https://latex.codecogs.com/png.latex?%5CPi%5E*"> <strong>if and only if</strong> there exists a do-calculus reduction of <img src="https://latex.codecogs.com/png.latex?Q(%5CPi%5E*)"> to an estimand that is a function of the observed distributions.</p>
<p>즉, 타겟 도메인의 인과 효과 <img src="https://latex.codecogs.com/png.latex?Q">가 관측 가능한 분포들의 함수로 변환(Reduction)될 수 있을 때만, 해당 효과는 이전(Transportable) 가능합니다.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/transportability_calculus_slide.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Transportability의 일반화된 유도 과정. Selection Node(노란색 사각형)가 <img src="https://latex.codecogs.com/png.latex?Z">에 영향을 미치는 구조에서 <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))">를 유도하고 있다.</figcaption>
</figure>
</div>
<section id="derivation-step-by-step" class="level3">
<h3 class="anchored" data-anchor-id="derivation-step-by-step">Derivation Step-by-Step</h3>
<ul>
<li><p>위의 그래프(DAG)는 Selection Node(■)가 <img src="https://latex.codecogs.com/png.latex?Z">에만 영향을 미치고(<img src="https://latex.codecogs.com/png.latex?S%20%5Cto%20Z">), <img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20W%20%5Cto%20Y">의 경로를 가지는 상황을 보여줍니다.</p></li>
<li><p>이 경우 타겟 도메인의 효과 <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))">는 다음과 같이 유도됩니다.</p></li>
<li><ol type="1">
<li><strong>Definition of Target Quantity:</strong></li>
</ol>
<ul>
<li>타겟 도메인(<img src="https://latex.codecogs.com/png.latex?%5CPi%5E*">)에서의 효과는 Selection Node(<img src="https://latex.codecogs.com/png.latex?S">)가 켜진 조건부 확률(<img src="https://latex.codecogs.com/png.latex?S=%5Cblacksquare">)과 같습니다. <img src="https://latex.codecogs.com/png.latex?Q%20=%20P%5E*(y%7Cdo(x))%20=%20P(y%7Cdo(x),%20S)"></li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Probability Axioms (Conditioning on W):</strong></li>
</ol>
<ul>
<li>중간 변수 <img src="https://latex.codecogs.com/png.latex?W">에 대해 전체 확률의 법칙을 적용합니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20S,%20w)P(w%7Cdo(x),%20S)"></li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>Rule 1 &amp; Graph Properties (Removal of S):</strong></li>
</ol>
<ul>
<li>그래프에서 <img src="https://latex.codecogs.com/png.latex?W">가 주어졌을 때, <img src="https://latex.codecogs.com/png.latex?Y">는 Selection Node(<img src="https://latex.codecogs.com/png.latex?S">)와 분리(d-separated)됩니다(<img src="https://latex.codecogs.com/png.latex?S%20%5Cto%20Z%20%5Cto%20W%20%5Cto%20Y">).</li>
<li>따라서 첫 번째 항에서 <img src="https://latex.codecogs.com/png.latex?S">를 제거할 수 있습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20w)P(w%7Cdo(x),%20S)"></li>
</ul></li>
<li><ol start="4" type="1">
<li><strong>Rule 3 (Removal of do(x)):</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">에 대한 개입(<img src="https://latex.codecogs.com/png.latex?do(x)">)은 <img src="https://latex.codecogs.com/png.latex?W">에 영향을 주지 않습니다(그래프상 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?W"> 사이의 경로는 <img src="https://latex.codecogs.com/png.latex?Z">를 통하거나 교란 경로뿐인데, 개입 시 차단됨).</li>
<li>따라서 두 번째 항에서 <img src="https://latex.codecogs.com/png.latex?do(x)">를 제거할 수 있습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20w)P(w%7CS)"></li>
</ul></li>
<li><ol start="5" type="1">
<li><strong>Final Transport Formula:</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?P(w%7CS)">는 타겟 도메인에서의 <img src="https://latex.codecogs.com/png.latex?W"> 분포인 <img src="https://latex.codecogs.com/png.latex?P%5E*(w)">와 같습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bw%7D%20P(y%7Cdo(x),%20w)P%5E*(w)"></li>
</ul></li>
<li><p><strong>해석:</strong> 이 식은 타겟 도메인에서 <img src="https://latex.codecogs.com/png.latex?Y">나 <img src="https://latex.codecogs.com/png.latex?Z">에 대한 실험을 할 필요 없이, <strong>소스 도메인의 실험 결과(<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x),%20w)">)</strong>와 <strong>타겟 도메인의 관측 데이터(<img src="https://latex.codecogs.com/png.latex?P%5E*(w)">)</strong>만 결합하면 타겟의 인과 효과를 계산할 수 있음을 보여줍니다.</p></li>
</ul>
</section>
</section>
<section id="specific-cases-derived-from-graph-structure" class="level2">
<h2 class="anchored" data-anchor-id="specific-cases-derived-from-graph-structure">4.2. Specific Cases derived from Graph Structure</h2>
<ul>
<li>이제 그래프 구조(Causal Story)에 따라 공식이 어떻게 달라지는지 구체적인 세 가지 사례를 살펴보겠습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/three_transportability_cases.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: 세 가지 다른 인과 구조에 따른 Transportability. (a) Z가 교란 변수(Confounder)인 경우, (b) Z가 결과의 결과(Outcome’s outcome)인 경우, (c) Z가 매개 변수(Mediator)인 경우. 각 경우마다 Selection Node(노란색 사각형)의 위치가 다르며, 이에 따라 유도되는 공식도 다르다.</figcaption>
</figure>
</div>
<section id="case-a-z-represents-age-confounder" class="level3">
<h3 class="anchored" data-anchor-id="case-a-z-represents-age-confounder">Case (a): Z represents Age (Confounder)</h3>
<ul>
<li><strong>Scenario:</strong> <img src="https://latex.codecogs.com/png.latex?Z">(나이)는 <img src="https://latex.codecogs.com/png.latex?X">(치료)와 <img src="https://latex.codecogs.com/png.latex?Y">(결과) 모두에 영향을 미치는 교란 요인입니다. 나이 분포(<img src="https://latex.codecogs.com/png.latex?P(z)">)는 도메인마다 다릅니다(<img src="https://latex.codecogs.com/png.latex?S%20%5Cto%20Z">).</li>
<li><strong>Formula:</strong> <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))%20=%20%5Csum_%7Bz%7D%20P(y%7Cdo(x),%20z)%20P%5E*(z)"></li>
<li><strong>Interpretation:</strong> Source에서 <img src="https://latex.codecogs.com/png.latex?Z">별 인과 효과(<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x),z)">)를 구한 뒤, 이를 Target의 나이 분포(<img src="https://latex.codecogs.com/png.latex?P%5E*(z)">)에 맞춰 가중 평균(Re-weighting)합니다. 이것이 표준적인 <strong>Adjustment Formula</strong>입니다.</li>
</ul>
</section>
<section id="case-b-z-represents-language-skill-proxy" class="level3">
<h3 class="anchored" data-anchor-id="case-b-z-represents-language-skill-proxy">Case (b): Z represents Language Skill (Proxy)</h3>
<ul>
<li><strong>Scenario:</strong> <img src="https://latex.codecogs.com/png.latex?Z">(언어 능력)는 <img src="https://latex.codecogs.com/png.latex?Y">의 결과물일 뿐, <img src="https://latex.codecogs.com/png.latex?X">나 <img src="https://latex.codecogs.com/png.latex?Y">의 원인이 아닙니다. <img src="https://latex.codecogs.com/png.latex?Z">의 메커니즘이 도메인마다 다릅니다.</li>
<li><strong>Formula:</strong> <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))%20=%20P(y%7Cdo(x))"></li>
<li><strong>Interpretation:</strong> <img src="https://latex.codecogs.com/png.latex?Z">는 인과 경로에 개입하지 않으므로, <img src="https://latex.codecogs.com/png.latex?Z">의 차이는 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y"> 효과에 영향을 주지 않습니다. 즉, Source의 결과를 그대로 Target에 적용할 수 있습니다.</li>
</ul>
</section>
<section id="case-c-z-represents-bio-marker-mediator" class="level3">
<h3 class="anchored" data-anchor-id="case-c-z-represents-bio-marker-mediator">Case (c): Z represents Bio-marker (Mediator)</h3>
<ul>
<li><strong>Scenario:</strong> <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 매개 변수입니다. <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Z">에 미치는 영향은 동일하지만, <img src="https://latex.codecogs.com/png.latex?Z">의 기저 분포나 측정 방식이 다를 수 있습니다(<img src="https://latex.codecogs.com/png.latex?S%20%5Cto%20Z">).</li>
<li><strong>Formula:</strong> <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))%20=%20%5Csum_%7Bz%7D%20P(y%7Cdo(x),%20z)%20P%5E*(z%7Cx)"></li>
<li><strong>Interpretation:</strong> <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z"> 메커니즘이 다르다면, Target 도메인에서의 조건부 확률 <img src="https://latex.codecogs.com/png.latex?P%5E*(z%7Cx)"> 정보를 사용하여 보정해야 합니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="algorithm-to-determine-if-an-effect-is-transportable" class="level1">
<h1>5. Algorithm to Determine if an Effect is Transportable</h1>
<ul>
<li>지금까지 살펴본 사례들은 비교적 단순한 구조였지만, 현실의 인과 그래프는 훨씬 복잡할 수 있습니다.</li>
<li>Pearl &amp; Bareinboim은 임의의 그래프 구조에 대해 Transportability를 판단하고 공식을 도출하는 일반화된 알고리즘을 제시했습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/transportability_algorithm_slide.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: Transportability 알고리즘의 예시. 복잡한 그래프(입력)에 대해 Selection Node의 위치를 분석하여, 타겟과 소스 데이터를 결합한 공식(출력)을 도출한다.</figcaption>
</figure>
</div>
<section id="input-output" class="level2">
<h2 class="anchored" data-anchor-id="input-output">5.1. Input &amp; Output</h2>
<ul>
<li><strong>INPUT:</strong> Selection Node(노란색 사각형, <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">)가 표시된 인과 그래프(Annotated Causal Graph).</li>
<li><strong>OUTPUT:</strong>
<ol type="1">
<li><strong>판단(Decision):</strong> 타겟 도메인의 인과 효과 <img src="https://latex.codecogs.com/png.latex?P%5E*(y%7Cdo(x))">가 이전을 통해 식별 가능한가(Transportable)?</li>
<li><strong>공식(Formula):</strong> 식별 가능하다면, 다음 두 가지 데이터를 결합한 수식:
<ul>
<li>소스 도메인의 실험 데이터 (Measurements from Source experiments)</li>
<li>타겟 도메인의 관측 데이터 (Measurements from Target observations)</li>
</ul></li>
</ol></li>
</ul>
</section>
<section id="the-logic-handling-non-identifiable-factors" class="level2">
<h2 class="anchored" data-anchor-id="the-logic-handling-non-identifiable-factors">5.2. The Logic: Handling Non-Identifiable Factors</h2>
<ul>
<li>이 알고리즘의 핵심은 전체 문제를 <strong>Q-factor(C-component)</strong> 단위로 쪼개고, 각 조각을 어디서 가져올지 결정하는 것입니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Key Logic:</strong> “Any <img src="https://latex.codecogs.com/png.latex?Q">-factors non-identifiable from <img src="https://latex.codecogs.com/png.latex?P%5E*(%5Cmathbf%7BV%7D)"> should be identified from experiments in the source domain. This corresponds to checking no <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare"> points to the variables in the non-identified <img src="https://latex.codecogs.com/png.latex?Q">-factor!”</p>
</blockquote>
<ul>
<li><ol type="1">
<li><strong>Target Priority:</strong></li>
</ol>
<ul>
<li>먼저 타겟 도메인의 관측 데이터 <img src="https://latex.codecogs.com/png.latex?P%5E*(%5Cmathbf%7BV%7D)">만으로 계산 가능한 요소인지 확인합니다. 가능하다면 <img src="https://latex.codecogs.com/png.latex?P%5E*">를 그대로 사용합니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Source Fallback:</strong></li>
</ol>
<ul>
<li>타겟 데이터만으로 식별 불가능한(Non-identifiable) 요소가 있다면, 소스 도메인의 실험 결과(<img src="https://latex.codecogs.com/png.latex?P(v%7Cdo(x))">)를 가져와야 합니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong>Transportability Condition:</strong></li>
</ol>
<ul>
<li>이때 소스 데이터를 가져오기 위해서는 <strong>“해당 요소에 영향을 주는 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">)가 없어야 한다”</strong>는 조건이 붙습니다.</li>
<li>만약 Selection Node가 가리키고 있다면, 메커니즘이 다르다는 뜻이므로 소스 데이터를 타겟에 대입할 수 없습니다. (즉, Transportable 하지 않음)</li>
</ul></li>
</ul>
</section>
<section id="derivation-example" class="level2">
<h2 class="anchored" data-anchor-id="derivation-example">5.3. Derivation Example</h2>
<ul>
<li>하단의 수식은 이 알고리즘을 적용한 결과입니다. 그래프의 각 부분(변수)이 어떻게 처리되었는지 분석해 봅시다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%5E*(y%7Cdo(x))%20=%20%5Csum_%7Bz%7D%20P(y%7Cdo(x),z)%20%5Csum_%7Bw%7D%20P%5E*(z%7Cw)%20%5Csum_%7Bt%7D%20P(w%7Cdo(x),t)P%5E*(t)%0A"></p>
<ul>
<li><p>이 식은 크게 네 부분으로 나뉩니다.</p></li>
<li><ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?P%5E*(t)"> (Target Data):</strong></li>
</ol>
<ul>
<li>변수 <img src="https://latex.codecogs.com/png.latex?T">에는 Selection Node가 없습니다. 또한 <img src="https://latex.codecogs.com/png.latex?T">는 외생 변수이므로 타겟 도메인의 분포를 그대로 사용합니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?P(w%7Cdo(x),t)"> (Source Experiment):</strong></li>
</ol>
<ul>
<li>변수 <img src="https://latex.codecogs.com/png.latex?W">를 구하는 부분입니다. <img src="https://latex.codecogs.com/png.latex?W">에는 직접적인 Selection Node가 붙어있지 않으므로, 소스 도메인의 실험 결과(<img src="https://latex.codecogs.com/png.latex?X">에 개입했을 때의 <img src="https://latex.codecogs.com/png.latex?W">)를 가져와서 사용합니다.</li>
</ul></li>
<li><ol start="3" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?P%5E*(z%7Cw)"> (Target Observation):</strong></li>
</ol>
<ul>
<li>변수 <img src="https://latex.codecogs.com/png.latex?Z">에는 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare%20%5Cto%20Z">)가 붙어 있습니다. 즉, 소스와 타겟 간에 메커니즘 차이가 있습니다.</li>
<li>따라서 소스 데이터를 쓰면 안 됩니다. 다행히 <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?W">가 주어졌을 때 타겟 도메인의 관측 데이터(<img src="https://latex.codecogs.com/png.latex?P%5E*">)만으로 식별이 가능하므로, <img src="https://latex.codecogs.com/png.latex?P%5E*(z%7Cw)">를 직접 측정하여 사용합니다.</li>
</ul></li>
<li><ol start="4" type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x),z)"> (Source Experiment):</strong></li>
</ol>
<ul>
<li>결과 변수 <img src="https://latex.codecogs.com/png.latex?Y">에는 Selection Node가 없습니다. 따라서 소스 도메인에서 <img src="https://latex.codecogs.com/png.latex?X">에 개입했을 때 <img src="https://latex.codecogs.com/png.latex?Z">에 따른 <img src="https://latex.codecogs.com/png.latex?Y">의 반응을 측정한 실험 데이터를 그대로 가져옵니다.</li>
</ul></li>
<li><p><strong>결론:</strong></p>
<ul>
<li>이 공식은 타겟 도메인에서 직접 실험을 하지 않고도(<img src="https://latex.codecogs.com/png.latex?do(x)"> in Target), <strong>소스의 실험 결과</strong>(1, 4번 항)와 <strong>타겟의 관측 결과</strong>(2, 3번 항)를 정교하게 조립하여 타겟의 인과 효과를 계산해 낸 것입니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="general-transportability-data-fusion-from-multiple-domains" class="level1">
<h1>6. General Transportability: Data Fusion from Multiple Domains</h1>
<ul>
<li>가장 복잡하면서도 강력한 시나리오는, 단일 소스만으로는 문제를 해결할 수 없고 여러 소스 도메인(<img src="https://latex.codecogs.com/png.latex?%5CPi%5Ea,%20%5CPi%5Eb,%20%5Cdots">)의 데이터를 결합(Data Fusion)해야만 타겟(<img src="https://latex.codecogs.com/png.latex?%5CPi%5E*">)을 추론할 수 있는 경우입니다.</li>
</ul>
<section id="motivation-the-la-nyc-example" class="level2">
<h2 class="anchored" data-anchor-id="motivation-the-la-nyc-example">Motivation: The LA &amp; NYC Example</h2>
<ul>
<li><strong>Goal:</strong> 타겟 도메인(Target)에서의 인과 효과 <img src="https://latex.codecogs.com/png.latex?Q%20=%20P%5E*(y%7Cdo(x))">를 구하고 싶습니다.</li>
<li><strong>Problem:</strong> 그 어떤 도메인도 타겟과 완벽하게 일치하지 않습니다.
<ul>
<li><strong>Source A (LA, <img src="https://latex.codecogs.com/png.latex?%5CPi%5Ea">):</strong> <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">에 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">)가 있습니다. 즉, <img src="https://latex.codecogs.com/png.latex?Y">가 생성되는 메커니즘이 타겟과 다릅니다. (하지만 <img src="https://latex.codecogs.com/png.latex?Z">는 타겟과 동일)</li>
<li><strong>Source B (NYC, <img src="https://latex.codecogs.com/png.latex?%5CPi%5Eb">):</strong> <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Z">에 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">)가 있습니다. 즉, <img src="https://latex.codecogs.com/png.latex?Z">가 생성되는 메커니즘이 타겟과 다릅니다. (하지만 <img src="https://latex.codecogs.com/png.latex?Y">는 타겟과 동일)</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/multi_domain_fusion.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6: 다중 도메인 데이터 융합(Data Fusion). 좌측(LA)은 Y와 X에, 우측(NYC)은 Z와 X에 구조적 차이(Selection Node)가 있다. 이 두 불완전한 소스를 결합하여 타겟의 효과를 추정해야 한다.</figcaption>
</figure>
</div>
</section>
<section id="derivation-steps-reduced-to-calculus" class="level2">
<h2 class="anchored" data-anchor-id="derivation-steps-reduced-to-calculus">Derivation Steps (Reduced to Calculus)</h2>
<ul>
<li>우리는 <strong>do-calculus</strong>와 확률의 법칙을 사용하여, 타겟 쿼리를 각 소스 도메인에서 식별 가능한(Transportable) 부분들로 분해하고 매핑합니다.</li>
</ul>
<ol type="1">
<li><p><strong>Definition &amp; Axioms (Decomposition):</strong> 먼저 타겟의 Selection Node 집합(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_%7Bxzy%7D">)을 조건부에 포함시킨 뒤, <img src="https://latex.codecogs.com/png.latex?Z">에 대해 전체 확률의 법칙을 적용합니다. <img src="https://latex.codecogs.com/png.latex?Q%20=%20P%5E*(y%7Cdo(x))%20=%20P(y%7Cdo(x),%20%5Cblacksquare_%7Bxzy%7D)"> <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bz%7D%20P(y%7Cdo(x),%20z,%20%5Cblacksquare_%7Bxzy%7D)%20P(z%7Cdo(x),%20%5Cblacksquare_%7Bxzy%7D)"></p></li>
<li><p><strong>Rule 2 (Action/Observation Exchange):</strong> 첫 번째 항에서 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y"> 경로를 차단하므로, 관측된 <img src="https://latex.codecogs.com/png.latex?z">를 개입 <img src="https://latex.codecogs.com/png.latex?do(z)">로 바꿀 수 있습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bz%7D%20P(y%7Cdo(x),%20do(z),%20%5Cblacksquare_%7Bxzy%7D)%20P(z%7Cdo(x),%20%5Cblacksquare_%7Bxzy%7D)"></p></li>
<li><p><strong>Rule 3 (Deletion of Action):</strong> 첫 번째 항에서 <img src="https://latex.codecogs.com/png.latex?Z">를 고정(<img src="https://latex.codecogs.com/png.latex?do(z)">)하면 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 영향은 사라집니다. 따라서 <img src="https://latex.codecogs.com/png.latex?do(x)">를 제거합니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7Bz%7D%20P(y%7Cdo(z),%20%5Cblacksquare_%7Bxzy%7D)%20P(z%7Cdo(x),%20%5Cblacksquare_%7Bxzy%7D)"></p></li>
<li><p><strong>Rule 1 (Removal of Irrelevant Selection Nodes)</strong></p>
<ul>
<li><p><strong>논리:</strong> “이 변수(<img src="https://latex.codecogs.com/png.latex?Y"> 또는 <img src="https://latex.codecogs.com/png.latex?Z">)를 결정하는 데 있어, 관련 없는 Selection Node는 지울 수 있다.” (d-separation)</p></li>
<li><p><strong>(Term 1) <img src="https://latex.codecogs.com/png.latex?Y">에 대한 분석:</strong></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y">는 <img src="https://latex.codecogs.com/png.latex?Z">가 주어졌을 때, <img src="https://latex.codecogs.com/png.latex?Z">나 <img src="https://latex.codecogs.com/png.latex?X">에 붙은 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_z,%20%5Cblacksquare_x">)와는 독립적입니다. (그래프상 <img src="https://latex.codecogs.com/png.latex?Y">로 오는 화살표가 없음)</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_y">만 남기고 나머지는 지웁니다. <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z),%20%5Cblacksquare_%7Bx,z,y%7D)%20%5Cxrightarrow%7B%5Ctext%7BRule%201%7D%7D%20P(y%7Cdo(z),%20%5Cblacksquare_y)"></li>
</ul></li>
<li><p><strong>(Term 2) <img src="https://latex.codecogs.com/png.latex?Z">에 대한 분석:</strong></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때, <img src="https://latex.codecogs.com/png.latex?Y">에 붙은 Selection Node(<img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_x,%20%5Cblacksquare_y">)와 독립적입니다.</li>
<li>따라서 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_z">만 남기고 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_x,%20%5Cblacksquare_y">는 지웁니다. <img src="https://latex.codecogs.com/png.latex?P(z%7Cdo(x),%20%5Cblacksquare_%7Bx,z,y%7D)%20%5Cxrightarrow%7B%5Ctext%7BRule%201%7D%7D%20P(z%7Cdo(x),%20%5Cblacksquare_%7Bz%7D)"></li>
</ul></li>
</ul></li>
<li><p><strong>Definition (Mapping to Sources)</strong></p>
<ul>
<li><p><strong>논리:</strong> “남아있는 Selection Node 구성과 일치하는 도메인(도시)을 찾아 연결한다.”</p></li>
<li><p><strong>(Term 1) <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z),%20%5Cblacksquare_y)"> 매핑:</strong></p>
<ul>
<li>우리는 <img src="https://latex.codecogs.com/png.latex?Y">에 대한 메커니즘이 타겟과 동일한(즉, <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_y">가 없는/영향을 안 주는) 도메인을 찾아야 합니다.</li>
<li><strong>LA (<img src="https://latex.codecogs.com/png.latex?%5CPi%5Ea">):</strong> <img src="https://latex.codecogs.com/png.latex?Y">에 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">가 있음 (Bad).</li>
<li><strong>NYC (<img src="https://latex.codecogs.com/png.latex?%5CPi%5Eb">):</strong> <img src="https://latex.codecogs.com/png.latex?Y">에 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">가 <strong>없음</strong> (Good).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 따라서 <strong>NYC 데이터(<img src="https://latex.codecogs.com/png.latex?P%5E%7B(b)%7D">)</strong>를 사용합니다. <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z),%20%5Cblacksquare_y)%20%5Crightarrow%20P%5E%7B(b)%7D(y%7Cdo(z))"></li>
</ul></li>
<li><p><strong>(Term 2) <img src="https://latex.codecogs.com/png.latex?P(z%7Cdo(x),%20%5Cblacksquare_%7Bz,x%7D)"> 매핑:</strong></p>
<ul>
<li>우리는 <img src="https://latex.codecogs.com/png.latex?Z">에 대한 메커니즘이 타겟과 동일한(즉, <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare_z">가 없는) 도메인을 찾아야 합니다.</li>
<li><strong>NYC (<img src="https://latex.codecogs.com/png.latex?%5CPi%5Eb">):</strong> <img src="https://latex.codecogs.com/png.latex?Z">에 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">가 있음 (Bad).</li>
<li><strong>LA (<img src="https://latex.codecogs.com/png.latex?%5CPi%5Ea">):</strong> <img src="https://latex.codecogs.com/png.latex?Z">에 <img src="https://latex.codecogs.com/png.latex?%5Cblacksquare">가 <strong>없음</strong> (Good).</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 따라서 <strong>LA 데이터(<img src="https://latex.codecogs.com/png.latex?P%5E%7B(a)%7D">)</strong>를 사용합니다. <img src="https://latex.codecogs.com/png.latex?P(z%7Cdo(x),%20%5Cblacksquare_%7Bz%7D)%20%5Crightarrow%20P%5E%7B(a)%7D(z%7Cdo(x))"></li>
</ul></li>
</ul></li>
</ol>
</section>
<section id="final-transport-formula" class="level2">
<h2 class="anchored" data-anchor-id="final-transport-formula">Final Transport Formula</h2>
<p>최종적으로 두 도메인의 데이터를 결합한 공식은 다음과 같습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP%5E*(y%7Cdo(x))%20=%20%5Csum_%7Bz%7D%20%5Cunderbrace%7BP%5E%7B(b)%7D(y%7Cdo(z))%7D_%7B%5Ctext%7Bfrom%20NYC%7D%7D%20%5Cunderbrace%7BP%5E%7B(a)%7D(z%7Cdo(x))%7D_%7B%5Ctext%7Bfrom%20LA%7D%7D%0A"></p>
<ul>
<li><strong>의미:</strong>
<ul>
<li><strong>LA 데이터</strong>에서는 <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Z">에 미치는 효과를 가져옵니다 (<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Z"> 메커니즘 공유).</li>
<li><strong>NYC 데이터</strong>에서는 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 효과를 가져옵니다 (<img src="https://latex.codecogs.com/png.latex?Z%20%5Cto%20Y"> 메커니즘 공유).</li>
<li>이들을 결합함으로써, <strong>그 어떤 도시에서도 수행된 적 없는 전체 실험(<img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">)의 결과</strong>를 정확하게 예측해 낼 수 있습니다.</li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="conclusion-is-the-gold-standard-golden" class="level1">
<h1>7. Conclusion: Is the Gold Standard Golden?</h1>
<ul>
<li>무작위 대조군 실험(RCT)은 인과 추론의 “Gold Standard”로 여겨집니다.</li>
<li>하지만 Transportability 이론은 RCT조차 완벽하지 않음을 시사합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-03/images/rct_limitations.png" class="img-fluid figure-img"></p>
<figcaption>Figure 7: RCT의 한계. 랜덤화(Randomization)는 X로 들어오는 화살표를 제거(우측 그래프)하여 교란을 없애주지만, 환경적 차이를 나타내는 Selection Node(상단 노란 사각형)는 제거하지 못한다. 즉, RCT는 내부 타당성(Internal Validity)만 보장할 뿐, 외부 타당성(External Validity)은 별도의 문제이다.</figcaption>
</figure>
</div>
<ul>
<li><strong>Lesson:</strong> 완벽한 RCT 데이터가 있더라도, 모집단 간의 차이(Selection Node)가 존재한다면 반드시 <strong>Transportability Exercise</strong>를 거쳐야 합니다.</li>
<li><strong>Completeness:</strong> 다행히도, Selection Diagram과 do-calculus를 이용한 이송 알고리즘은 <strong>완전(Complete)</strong>합니다. 즉, 이 방법으로 이송 공식을 유도할 수 없다면, 해당 데이터만으로는 이론적으로 타겟 효과를 식별할 수 없음이 증명된 것입니다.</li>
</ul>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-03/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 4)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-04/</link>
  <description><![CDATA[ 





<section id="introduction-the-man-made-bias" class="level1">
<h1>1. Introduction: The “Man-Made” Bias</h1>
<ul>
<li>데이터 과학에서 우리는 종종 <strong>“데이터가 스스로 말하게 하라(Let the data speak)”</strong>는 격언을 듣습니다.</li>
<li>하지만 데이터가 수집되는 과정에서 이미 입이 막혀 있거나, 왜곡된 목소리만 내고 있다면 어떨까요? 이것이 바로 <strong>선택 편향(Selection Bias)</strong>의 문제입니다.</li>
<li>선택 편향은 데이터 샘플링 과정에서 특정 개체가 우선적으로 포함되거나 배제됨으로써 발생합니다.</li>
<li>이는 단순한 통계적 오차를 넘어, 인과 추론의 타당성을 근본적으로 위협하는 주요 장애물입니다.</li>
</ul>
<section id="confounding-vs.-selection-bias" class="level2">
<h2 class="anchored" data-anchor-id="confounding-vs.-selection-bias">Confounding vs.&nbsp;Selection Bias</h2>
<ul>
<li>우리는 앞서 교란(Confounding)에 대해 다뤘습니다. 두 편향은 근본적으로 다릅니다.</li>
<li><strong>Confounding:</strong>
<ul>
<li>자연(Nature)적으로 발생하는 Treatment와 Outcome 사이의 정보 흐름(Common Cause)입니다.</li>
<li>“치료를 받은 사람이 더 건강해서 결과가 좋은가?”의 문제입니다.</li>
</ul></li>
<li><strong>Selection Bias:</strong>
<ul>
<li>인간(Man-made)에 의해, 혹은 측정 장비에 의해 발생하는 데이터 수집 과정의 편향입니다.</li>
<li>“특정 조건을 만족하는 사람만 설문에 응답했는가?”의 문제입니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-04/images/confounding_vs_selection.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Confounding vs Selection Bias. 왼쪽은 Z가 X와 Y에 영향을 주는 교란(Confounding) 구조이고, 오른쪽은 X와 Y가 S(선택 여부)에 영향을 주는 선택 편향(Selection Bias) 구조이다. S=1인 샘플만 관측됨으로써 X와 Y 사이에 허위 상관관계(Spurious Correlation)가 발생한다.</figcaption>
</figure>
</div>
<ul>
<li>이 포스트에서는 선택 편향이 있는 데이터(<img src="https://latex.codecogs.com/png.latex?S=1">)만 관측 가능한 상황에서, 어떻게 전체 모집단의 분포(<img src="https://latex.codecogs.com/png.latex?P(y%7Cx)">)나 인과 효과(<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">)를 복원(Recover)할 수 있는지 다룹니다.</li>
</ul>
<hr>
</section>
</section>
<section id="modeling-selection-bias-with-causal-graphs" class="level1">
<h1>2. Modeling Selection Bias with Causal Graphs</h1>
<ul>
<li><p>선택 편향을 수학적으로 다루기 위해 우리는 <strong>선택 노드(Selection Node) <img src="https://latex.codecogs.com/png.latex?S"></strong>를 인과 그래프에 도입합니다.</p></li>
<li><p><strong>Definition:</strong> <img src="https://latex.codecogs.com/png.latex?S">는 이진 변수로, <img src="https://latex.codecogs.com/png.latex?S=1">인 샘플만 데이터셋에 포함됩니다.</p></li>
<li><p><strong>Problem:</strong> 우리의 목표는 <img src="https://latex.codecogs.com/png.latex?S=1"> 조건부 분포 <img src="https://latex.codecogs.com/png.latex?P(v%7CS=1)">로부터, 모집단 분포 <img src="https://latex.codecogs.com/png.latex?P(v)"> 혹은 인과 효과 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 추정하는 것입니다.</p></li>
</ul>
<section id="the-origin-of-selection-bias-collider-bias" class="level3">
<h3 class="anchored" data-anchor-id="the-origin-of-selection-bias-collider-bias">The Origin of Selection Bias (Collider Bias)</h3>
<p>선택 편향은 그래프상에서 <strong>Collider</strong> 구조로 설명될 수 있습니다. 예를 들어, 두 개의 독립적인 변수 <img src="https://latex.codecogs.com/png.latex?X">(재능)와 <img src="https://latex.codecogs.com/png.latex?Y">(미모)가 있고, 이 두 가지를 모두 갖춘 사람만이 연예인이 되어 TV에 나온다(<img src="https://latex.codecogs.com/png.latex?S=1">)고 가정해 봅시다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%20%5Crightarrow%20S%20%5Cleftarrow%20Y%0A"></p>
<p>전체 모집단에서 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 독립(<img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y">)이지만, 우리가 관측하는 TV 속 세상(<img src="https://latex.codecogs.com/png.latex?S=1">)에서는 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에 강한 음의 상관관계가 생깁니다(Berkson’s Paradox). 즉, 재능이 없으면 미모라도 뛰어나야 하기 때문입니다.</p>
<hr>
</section>
</section>
<section id="recoverability-without-external-information" class="level1">
<h1>3. Recoverability without External Information</h1>
<p>가장 먼저 던져야 할 질문은 <strong>“편향된 데이터만 가지고 편향을 제거할 수 있는가?”</strong>입니다 [cite: 2329-2345].</p>
<section id="theorem-recoverability-of-conditional-probability" class="level3">
<h3 class="anchored" data-anchor-id="theorem-recoverability-of-conditional-probability">Theorem: Recoverability of Conditional Probability</h3>
<p>우리가 구하고 싶은 분포가 <img src="https://latex.codecogs.com/png.latex?Q%20=%20P(y%7Cx)">이고, 가진 데이터가 <img src="https://latex.codecogs.com/png.latex?P(y%7Cx,%20S=1)">일 때, 복원 가능 조건은 다음과 같습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cx)%20%5Ctext%7B%20is%20recoverable%7D%20%5Ciff%20Y%20%5Cperp%20S%20%5Cmid%20X%0A"></p>
<p>즉, 그래프상에서 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?Y">와 <img src="https://latex.codecogs.com/png.latex?S">가 <strong>d-separation</strong> 되어야 합니다.</p>
<ul>
<li><strong>직관:</strong> <img src="https://latex.codecogs.com/png.latex?X">라는 조건(예: 나이)을 알면, 데이터에 선택되었는지 여부(<img src="https://latex.codecogs.com/png.latex?S">)가 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 대한 추가적인 정보를 주지 않아야 합니다. 이 경우 <img src="https://latex.codecogs.com/png.latex?P(y%7Cx)%20=%20P(y%7Cx,%20S=1)">이 성립하여, 편향된 데이터를 그대로 사용할 수 있습니다.</li>
</ul>
<hr>
</section>
</section>
<section id="recoverability-with-external-information" class="level1">
<h1>4. Recoverability with External Information</h1>
<p>만약 <img src="https://latex.codecogs.com/png.latex?Y%20%5Cperp%20S%20%5Cmid%20X"> 조건이 성립하지 않는다면 어떻게 해야 할까요? 이때는 편향되지 않은 <strong>외부 데이터(External Data)</strong>의 도움이 필요합니다 [cite: 2346-2373].</p>
<ul>
<li><strong>Biased Data:</strong> <img src="https://latex.codecogs.com/png.latex?P(y,%20x,%20c%20%7C%20S=1)"> (풍부하지만 편향됨)</li>
<li><strong>Unbiased External Data:</strong> <img src="https://latex.codecogs.com/png.latex?P(c,%20x)"> (일부 변수에 대한 모집단 통계, 예: 인구센서스)</li>
</ul>
<section id="theorem-sufficiency-for-recoverability" class="level3">
<h3 class="anchored" data-anchor-id="theorem-sufficiency-for-recoverability">Theorem: Sufficiency for Recoverability</h3>
<p>다음 조건을 만족하는 변수 집합 <img src="https://latex.codecogs.com/png.latex?C">가 존재하면 <img src="https://latex.codecogs.com/png.latex?P(y%7Cx)">를 복원할 수 있습니다.</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?Y%20%5Cperp%20S%20%5Cmid%20%5C%7BC,%20X%5C%7D"> in Graph <img src="https://latex.codecogs.com/png.latex?G"></li>
<li><img src="https://latex.codecogs.com/png.latex?P(C,%20X)"> is estimable (외부 데이터 존재)</li>
</ol>
<p>이때 복원 공식은 다음과 같습니다[cite: 2361]:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cx)%20=%20%5Csum_%7Bc%7D%20P(y%7Cx,%20c,%20S=1)%20P(c%7Cx)%0A"></p>
<ul>
<li><strong>해석:</strong> <img src="https://latex.codecogs.com/png.latex?C">라는 변수(예: 지역, 소득)를 통해 선택 편향의 고리를 끊을 수 있다면, 편향된 데이터에서 <img src="https://latex.codecogs.com/png.latex?C">별 <img src="https://latex.codecogs.com/png.latex?Y">의 분포를 구하고(<img src="https://latex.codecogs.com/png.latex?P(y%7Cx,c,S=1)">), 이를 외부 데이터의 <img src="https://latex.codecogs.com/png.latex?C"> 분포(<img src="https://latex.codecogs.com/png.latex?P(c%7Cx)">)로 가중 평균(Re-weighting)하여 모집단 분포를 재구성합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-04/images/recoverability_external_c.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: 외부 정보를 이용한 복원 가능성 판단 예시. C={W1, W2}를 조건부로 했을 때 S와 Y가 독립이 된다면(d-separation), 외부의 P(W1, W2) 데이터를 이용하여 편향을 제거할 수 있다. 반면 C가 S와 Y 사이의 경로를 차단하지 못하면 복원 불가능하다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="generalized-adjustment-criterion" class="level1">
<h1>5. Generalized Adjustment Criterion</h1>
<p>이제 가장 일반적이고 강력한 시나리오를 다룹니다. 현실의 데이터는 <strong>교란(Confounding)과 선택 편향(Selection Bias)이 동시에 존재</strong>합니다. 우리는 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">를 구하기 위해 이 두 마리 토끼를 동시에 잡아야 합니다 [cite: 2428-2462].</p>
<section id="definition-generalized-adjustment-set" class="level3">
<h3 class="anchored" data-anchor-id="definition-generalized-adjustment-set">Definition: Generalized Adjustment Set</h3>
<p>변수 집합 <img src="https://latex.codecogs.com/png.latex?(Z,%20Z%5ET)">가 다음 세 가지 조건을 만족하면, 이를 통해 인과 효과를 식별할 수 있습니다 [cite: 2456-2461]. 여기서 <img src="https://latex.codecogs.com/png.latex?Z%5ET%20%5Csubseteq%20Z">는 편향 없이 측정 가능한 외부 데이터가 있는 부분집합입니다.</p>
<ol type="1">
<li><strong>Causal Path protection:</strong> <img src="https://latex.codecogs.com/png.latex?Z">는 <img src="https://latex.codecogs.com/png.latex?X%20%5Cto%20Y">로 가는 어떠한 적절한 인과 경로(Proper Causal Path)도 차단해서는 안 됩니다. (과도한 통제 방지)</li>
<li><strong>Back-door Blocking:</strong> <img src="https://latex.codecogs.com/png.latex?Z%20%5Ccup%20%5C%7BS%5C%7D">는 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 모든 <strong>비인과적 경로(Non-causal paths)</strong>를 차단해야 합니다. (교란 제거)</li>
<li><strong>Selection Blocking:</strong> <img src="https://latex.codecogs.com/png.latex?Z%5ET">는 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 인과 경로가 끊어진 그래프에서, <img src="https://latex.codecogs.com/png.latex?Y">와 <img src="https://latex.codecogs.com/png.latex?S">를 d-separation 시켜야 합니다. (선택 편향 제거)</li>
</ol>
</section>
<section id="the-generalized-adjustment-formula" class="level3">
<h3 class="anchored" data-anchor-id="the-generalized-adjustment-formula">The Generalized Adjustment Formula</h3>
<p>위 조건이 만족될 때, 인과 효과는 다음과 같이 계산됩니다[cite: 2472]:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(y%7Cdo(x))%20=%20%5Csum_%7Bz%7D%20P(y%7Cx,%20z,%20S=1)%20P(z%20%5Csetminus%20z%5ET%20%7C%20z%5ET,%20S=1)%20P(z%5ET)%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?P(y%7Cx,%20z,%20S=1)">: 편향된 데이터에서 추정한 모형.</li>
<li><img src="https://latex.codecogs.com/png.latex?P(z%20%5Csetminus%20z%5ET%20%7C%20z%5ET,%20S=1)">: 편향된 데이터에서 <img src="https://latex.codecogs.com/png.latex?Z"> 내부 변수 간의 관계.</li>
<li><img src="https://latex.codecogs.com/png.latex?P(z%5ET)">: 외부에서 가져온 편향 없는 모집단 데이터.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-04/images/generalized_adjustment_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: Generalized Adjustment 예시. 교란 요인과 선택 편향 메커니즘이 복잡하게 얽힌 그래프에서, 적절한 Z 집합(일부는 biased, 일부는 external)을 찾아내어 인과 효과를 계산하는 과정을 보여준다.</figcaption>
</figure>
</div>
<p>이 공식은 기존의 Back-door Adjustment Formula (<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))%20=%20%5Csum_z%20P(y%7Cx,z)P(z)">)를 선택 편향이 있는 상황으로 확장한 것입니다. <img src="https://latex.codecogs.com/png.latex?S=1"> 조건이 붙은 항들과 외부 데이터 <img src="https://latex.codecogs.com/png.latex?P(z%5ET)">가 결합되는 형태를 주목하세요.</p>
<hr>
</section>
</section>
<section id="summary-the-power-of-causal-data-science" class="level1">
<h1>6. Summary: The Power of Causal Data Science</h1>
<p>이번 포스트를 통해 우리는 데이터 과학의 난제인 <strong>선택 편향</strong>을 인과 그래프를 통해 체계적으로 해결하는 방법을 배웠습니다.</p>
<ol type="1">
<li><strong>Problem Identification:</strong> 선택 편향은 데이터 수집 과정의 인과 구조(<img src="https://latex.codecogs.com/png.latex?S"> 노드)로 모델링됩니다.</li>
<li><strong>Pure Recoverability:</strong> 외부 데이터 없이도 <img src="https://latex.codecogs.com/png.latex?Y%20%5Cperp%20S%20%7C%20X"> 조건이 만족되면 복원 가능합니다.</li>
<li><strong>External Data Fusion:</strong> 외부 데이터(<img src="https://latex.codecogs.com/png.latex?P(c)">)가 있다면, 이를 연결고리(Re-weighting bridge)로 사용하여 복원할 수 있습니다.</li>
<li><strong>Generalized Adjustment:</strong> 교란과 선택 편향이 섞여 있어도, <strong>Generalized Adjustment Criterion</strong>을 통해 올바른 통제 변수 집합(<img src="https://latex.codecogs.com/png.latex?Z">)과 외부 데이터(<img src="https://latex.codecogs.com/png.latex?Z%5ET">)를 찾아내어 인과 효과를 편향 없이 추정할 수 있습니다.</li>
</ol>
<p>Bareinboim과 Tian 등의 연구에 따르면, 이 알고리즘들은 <strong>완전(Complete)</strong>합니다[cite: 2477]. 즉, 이 방법으로 복원할 수 없다면, 주어진 가정하에서는 그 어떤 방법으로도 편향을 제거하는 것이 불가능하다는 뜻입니다.</p>
<hr>
<section id="appendix-verification-checklist" class="level3">
<h3 class="anchored" data-anchor-id="appendix-verification-checklist"><strong>Appendix: Verification Checklist</strong></h3>
<ul>
<li><strong>포함된 내용:</strong>
<ul class="task-list">
<li><label><input type="checkbox" checked="">선택 편향의 정의 (Preferential inclusion) 및 Confounding과의 차이점 비교</label></li>
<li><label><input type="checkbox" checked="">선택 노드(S)를 포함한 인과 그래프 모델링</label></li>
<li><label><input type="checkbox" checked="">외부 정보 없는 복원 가능성 조건 (<img src="https://latex.codecogs.com/png.latex?Y%20%5Cperp%20S%20%7C%20X">) 및 정리(Theorem)</label></li>
<li><label><input type="checkbox" checked="">외부 정보(<img src="https://latex.codecogs.com/png.latex?P(c,x)">)를 이용한 복원 가능성(Sufficiency) 및 복원 공식 유도</label></li>
<li><label><input type="checkbox" checked="">Confounding과 Selection Bias가 동시에 존재하는 상황 (Generalized Adjustment)</label></li>
<li><label><input type="checkbox" checked="">Generalized Adjustment Criterion의 3가지 조건 상세 서술</label></li>
<li><label><input type="checkbox" checked="">Generalized Adjustment Formula (<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))">)의 LaTeX 수식 정확한 재현</label></li>
<li><label><input type="checkbox" checked="">주요 도표(Origin of Bias, Recoverability examples)에 대한 Placeholder 및 캡션</label></li>
</ul></li>
<li><strong>생략된 내용:</strong>
<ul>
<li>강의 자료 마지막 부분의 Odds Ratio 관련 내용은 Selection Bias의 통계적 변형(Statistical Variant)으로 소개되었으나, 인과 추론(Causal Inference)의 핵심 흐름인 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(x))"> 복원에 집중하기 위해 본문에서는 간략히 언급하거나 생략하고 Recoverability Theorem에 집중함.</li>
</ul></li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-04/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 5)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-05/</link>
  <description><![CDATA[ 





<section id="introduction-missingness-is-a-causal-concept" class="level2">
<h2 class="anchored" data-anchor-id="introduction-missingness-is-a-causal-concept">1. Introduction: Missingness is a Causal Concept</h2>
<p>데이터 분석, 특히 사회과학이나 의료 데이터를 다룰 때 결측치(Missing Data)는 피할 수 없는 현실입니다. [cite_start]설문조사에서 응답자가 특정 문항을 건너뛰거나, 센서가 오작동하거나, 환자가 과거 기록을 기억하지 못하는 등 다양한 이유로 발생합니다[cite: 13, 14, 15].</p>
<p>통계학 문헌, 특히 Rubin의 선구적인 연구 이후 대부분의 결측치 처리 방식은 <strong>MAR(Missing At Random)</strong> 가정을 기반으로 합니다. [cite_start]이는 Maximum Likelihood나 Multiple Imputation의 수렴성을 보장하기 위한 필수적인 가정이지만, 실제 데이터에서 이 가정이 성립하는지 검증하는 것은 매우 어렵습니다[cite: 19, 20, 21, 23].</p>
<p>Mohan과 Pearl(2021)은 결측치 문제를 <strong>통계적 문제(Statistical Problem)가 아닌 인과적 문제(Causal Problem)</strong>로 재정의합니다. 데이터가 왜 비었는지에 대한 “이유(Reason)”는 데이터 생성 과정(Data Generating Process)의 일부이기 때문입니다. [cite_start]본 포스트에서는 그래프 모형(Graphical Models), 특히 <strong>m-graphs</strong>를 통해 결측 메커니즘을 시각화하고, 데이터의 복원 가능성(Recoverability)을 판단하는 방법을 정리합니다[cite: 31, 32].</p>
<hr>
</section>
<section id="formalism-the-m-graph" class="level2">
<h2 class="anchored" data-anchor-id="formalism-the-m-graph">2. Formalism: The m-graph</h2>
<p>결측 데이터 문제를 그래프로 표현하기 위해, 우리는 변수를 관측 여부와 역할에 따라 분류하고 <strong>Proxy Variable</strong>이라는 개념을 도입합니다.</p>
<section id="variables-classification" class="level3">
<h3 class="anchored" data-anchor-id="variables-classification">2.1. Variables Classification</h3>
<p>[cite_start]Causal DAG <img src="https://latex.codecogs.com/png.latex?G">의 노드들은 다음 다섯 가지 카테고리로 분류됩니다[cite: 115, 116]:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?U">: <strong>Latent Variables</strong> (관측되지 않는 잠재 변수)</li>
<li><img src="https://latex.codecogs.com/png.latex?V_o">: <strong>Fully Observed Variables</strong> (모든 레코드에서 관측된 변수)</li>
<li><img src="https://latex.codecogs.com/png.latex?V_m">: <strong>Missing Variables</strong> (적어도 하나의 레코드에서 결측이 발생한 변수)</li>
<li><img src="https://latex.codecogs.com/png.latex?R">: <strong>Missingness Mechanisms</strong> (결측을 유발하는 인과적 메커니즘을 나타내는 변수)</li>
<li><img src="https://latex.codecogs.com/png.latex?V%5E*">: <strong>Proxy Variables</strong> (실제로 관측된 변수)</li>
</ol>
</section>
<section id="the-proxy-variable-mechanism" class="level3">
<h3 class="anchored" data-anchor-id="the-proxy-variable-mechanism">2.2. The Proxy Variable Mechanism</h3>
<p>[cite_start]변수 <img src="https://latex.codecogs.com/png.latex?X">가 결측될 수 있다면, 우리는 실제 값 <img src="https://latex.codecogs.com/png.latex?X">와 결측 여부를 결정하는 스위치 <img src="https://latex.codecogs.com/png.latex?R_X">를 통해 관측된 값 <img src="https://latex.codecogs.com/png.latex?X%5E*">를 다음과 같이 정의합니다[cite: 49, 50].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%5E*%20=%20f(R_X,%20X)%20=%20%5Cbegin%7Bcases%7D%0AX%20&amp;%20%5Ctext%7Bif%20%7D%20R_X%20=%200%20%5Cquad%20(%5Ctext%7BObserved%7D)%20%5C%5C%0Am%20&amp;%20%5Ctext%7Bif%20%7D%20R_X%20=%201%20%5Cquad%20(%5Ctext%7BMissing%7D)%0A%5Cend%7Bcases%7D%0A"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?m">은 결측(missing value)을 나타내는 기호입니다. [cite_start]<img src="https://latex.codecogs.com/png.latex?R_X=1">이면 변수의 값은 가려지고(masked), <img src="https://latex.codecogs.com/png.latex?R_X=0">이면 드러납니다(revealed)[cite: 54, 55].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-05/images/basic_proxy_structure.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: 비만도(Obesity) 예시를 통한 m-graph의 기본 구조. G(성별), A(나이)는 완전히 관측되지만, O(비만도)는 결측 메커니즘 <img src="https://latex.codecogs.com/png.latex?R_O">에 의해 <img src="https://latex.codecogs.com/png.latex?O%5E*">로 관측된다.</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><strong>Figure 1 설명</strong>: 위 그림은 학교 데이터 예시를 보여줍니다. <img src="https://latex.codecogs.com/png.latex?G">(Gender)와 <img src="https://latex.codecogs.com/png.latex?A">(Age)는 학교 기록부에 있어 완전히 관측되지만(<img src="https://latex.codecogs.com/png.latex?V_o">), <img src="https://latex.codecogs.com/png.latex?O">(Obesity)는 학생이 응답을 거부할 수 있어 결측이 발생합니다(<img src="https://latex.codecogs.com/png.latex?V_m">). [cite_start]<img src="https://latex.codecogs.com/png.latex?R_O">는 <img src="https://latex.codecogs.com/png.latex?O">가 결측될지 여부를 결정하는 변수이며, 실제로 우리가 데이터셋에서 보는 것은 <img src="https://latex.codecogs.com/png.latex?O%5E*">(<img src="https://latex.codecogs.com/png.latex?V%5E*">)입니다[cite: 39, 44, 45, 49].</p>
</blockquote>
<hr>
</section>
</section>
<section id="categories-of-missingness-in-graphs" class="level2">
<h2 class="anchored" data-anchor-id="categories-of-missingness-in-graphs">3. Categories of Missingness in Graphs</h2>
<p>Rubin의 분류법인 MCAR, MAR, MNAR을 m-graph 상에서의 조건부 독립성(Conditional Independence)으로 명확히 정의할 수 있습니다. [cite_start]그래프 구조를 통해 데이터가 어떤 결측 유형에 속하는지 시각적으로 판별(Inspection)이 가능해집니다[cite: 34].</p>
<section id="mcar-missing-completely-at-random" class="level3">
<h3 class="anchored" data-anchor-id="mcar-missing-completely-at-random">3.1. MCAR (Missing Completely At Random)</h3>
<p>결측이 완전히 무작위로 발생하는 경우입니다. [cite_start]설문지를 잃어버리는 등의 상황이 이에 해당합니다[cite: 69].</p>
<ul>
<li><strong>정의</strong>: 결측 메커니즘 <img src="https://latex.codecogs.com/png.latex?R">이 데이터의 모든 변수(<img src="https://latex.codecogs.com/png.latex?V_m,%20V_o,%20U">)와 독립입니다.</li>
<li>[cite_start]<strong>그래프 조건</strong>: <img src="https://latex.codecogs.com/png.latex?R"> 변수들과 <img src="https://latex.codecogs.com/png.latex?V_o%20%5Ccup%20V_m"> 사이에 엣지가 없습니다[cite: 138, 143].</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_m,%20V_o,%20U%20%5Cperp%20%5Cmathbb%7BR%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-05/images/mcar_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: MCAR의 그래프 구조. <img src="https://latex.codecogs.com/png.latex?R_O">로 들어오는 화살표가 없다. 즉, 나이(A)나 성별(G), 실제 비만도(O)가 결측 여부(<img src="https://latex.codecogs.com/png.latex?R_O">)에 영향을 주지 않는다.</figcaption>
</figure>
</div>
</section>
<section id="mar-missing-at-random" class="level3">
<h3 class="anchored" data-anchor-id="mar-missing-at-random">3.2. MAR (Missing At Random)</h3>
<p>결측이 <strong>완전히 관측된 변수(<img src="https://latex.codecogs.com/png.latex?V_o">)</strong>에만 의존하여 발생하는 경우입니다. [cite_start]예를 들어, 10대(<img src="https://latex.codecogs.com/png.latex?A">)들이 반항심에 몸무게를 보고하지 않는 경우 등입니다[cite: 77].</p>
<ul>
<li>[cite_start]<strong>정의</strong>: 관측된 변수 <img src="https://latex.codecogs.com/png.latex?V_o">가 주어졌을 때, 결측 메커니즘 <img src="https://latex.codecogs.com/png.latex?R">은 나머지 변수들과 독립입니다[cite: 146].</li>
<li>[cite_start]<strong>그래프 조건</strong>: (i) <img src="https://latex.codecogs.com/png.latex?R">과 부분적으로 관측된 변수(<img src="https://latex.codecogs.com/png.latex?V_m">) 사이에 엣지가 없고, (ii) <img src="https://latex.codecogs.com/png.latex?R">과 <img src="https://latex.codecogs.com/png.latex?V_o"> 사이에 양방향 엣지(bidirected edge, Latent confounder)가 없어야 합니다[cite: 147].</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AV_m,%20U%20%5Cperp%20%5Cmathbb%7BR%7D%20%5Cmid%20V_o%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-05/images/mar_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 3: MAR의 그래프 구조. 나이(A)가 결측 메커니즘(<img src="https://latex.codecogs.com/png.latex?R_O">)에 영향을 준다(<img src="https://latex.codecogs.com/png.latex?A%20%5Crightarrow%20R_O">). 즉, 결측 여부는 관측된 나이에 따라 달라진다.</figcaption>
</figure>
</div>
</section>
<section id="mnar-missing-not-at-random" class="level3">
<h3 class="anchored" data-anchor-id="mnar-missing-not-at-random">3.3. MNAR (Missing Not At Random)</h3>
<p>데이터가 MCAR나 MAR이 아닌 모든 경우입니다. 특히, 결측된 변수 그 자체가 결측 원인이 되는 경우가 포함됩니다. [cite_start]예를 들어, 비만인 학생(<img src="https://latex.codecogs.com/png.latex?O">)이 부끄러워서 몸무게를 숨기는 경우입니다[cite: 87, 112].</p>
<ul>
<li><strong>특징</strong>: <img src="https://latex.codecogs.com/png.latex?R"> 변수로 들어오는 화살표가 결측 변수(<img src="https://latex.codecogs.com/png.latex?V_m">)나 잠재 변수(<img src="https://latex.codecogs.com/png.latex?U">)에서 시작됩니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-05/images/mnar_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: MNAR의 그래프 구조. 실제 비만도(O)가 결측 여부(<img src="https://latex.codecogs.com/png.latex?R_O">)에 직접 영향을 준다(<img src="https://latex.codecogs.com/png.latex?O%20%5Crightarrow%20R_O">). 비만일수록 응답하지 않을 확률이 높아지는 메커니즘이다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="recoverability-can-we-restore-the-truth" class="level2">
<h2 class="anchored" data-anchor-id="recoverability-can-we-restore-the-truth">4. Recoverability: Can We Restore the Truth?</h2>
<p>[cite_start]<strong>Recoverability(복원 가능성)</strong>란, 결측이 포함된 관측 데이터(<img src="https://latex.codecogs.com/png.latex?V%5E*,%20V_o,%20R">)의 분포로부터 우리가 알고 싶은 원래 분포(Target Distribution, <img src="https://latex.codecogs.com/png.latex?P(V_o,%20V_m)">)를 일관되게(consistently) 추정할 수 있는지에 대한 문제입니다[cite: 35].</p>
<section id="recoverability-of-mcar" class="level3">
<h3 class="anchored" data-anchor-id="recoverability-of-mcar">4.1. Recoverability of MCAR</h3>
<p>MCAR의 경우, <img src="https://latex.codecogs.com/png.latex?R_O">는 모든 변수와 독립이므로, 데이터를 삭제(List-wise deletion)하고 남은 데이터만 써도 편향(Bias)이 없습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(G,%20O,%20A)%20=%20P(G,%20O,%20A%20%5Cmid%20R_O%20=%200)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?R_O%20=%200">인 조건 하에서는 <img src="https://latex.codecogs.com/png.latex?O%20=%20O%5E*">이므로,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A=%20P(G,%20O%5E*,%20A%20%5Cmid%20R_O%20=%200)%0A"></p>
<p>[cite_start]즉, 관측된 데이터만으로 원래 분포를 완벽히 복원할 수 있습니다[cite: 187, 189].</p>
</section>
<section id="recoverability-of-mar" class="level3">
<h3 class="anchored" data-anchor-id="recoverability-of-mar">4.2. Recoverability of MAR</h3>
<p>MAR의 경우, 단순 삭제는 편향을 낳지만, 조건부 확률을 이용해 복원할 수 있습니다. 예를 들어 <img src="https://latex.codecogs.com/png.latex?A%20%5Crightarrow%20R_O">인 상황(Figure 3)을 봅시다.</p>
<p>[cite_start]우리는 결합 확률 <img src="https://latex.codecogs.com/png.latex?P(G,%20O,%20A)">를 다음과 같이 분해(Factorization)할 수 있습니다[cite: 198]: <img src="https://latex.codecogs.com/png.latex?%0AP(G,%20O,%20A)%20=%20P(G,%20O%20%5Cmid%20A)P(A)%0A"></p>
<p>그래프에서 <img src="https://latex.codecogs.com/png.latex?A">는 <img src="https://latex.codecogs.com/png.latex?R_O">와 <img src="https://latex.codecogs.com/png.latex?%5C%7BG,%20O%5C%7D"> 사이를 <strong>d-separation</strong> 합니다. [cite_start]따라서 <img src="https://latex.codecogs.com/png.latex?R_O">에 조건을 걸어도 확률은 변하지 않습니다[cite: 199]. <img src="https://latex.codecogs.com/png.latex?%0AP(G,%20O%20%5Cmid%20A)%20=%20P(G,%20O%20%5Cmid%20A,%20R_O%20=%200)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?R_O=0">일 때 <img src="https://latex.codecogs.com/png.latex?O=O%5E*">이므로, 최종적으로 관측 가능한 변수들로 표현됩니다: <img src="https://latex.codecogs.com/png.latex?%0AP(G,%20O,%20A)%20=%20P(G,%20O%5E*%20%5Cmid%20A,%20R_O%20=%200)P(A)%0A"></p>
<p>[cite_start]이것이 MAR 상황에서 데이터를 복원하는 핵심 논리입니다[cite: 202].</p>
</section>
<section id="the-challenge-of-mnar" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge-of-mnar">4.3. The Challenge of MNAR</h3>
<p>MNAR, 예를 들어 <img src="https://latex.codecogs.com/png.latex?O%20%5Crightarrow%20R_O">인 경우(Figure 4), 위와 같은 분해나 d-separation을 사용할 수 없습니다. [cite_start]<img src="https://latex.codecogs.com/png.latex?O">를 관측하지 못했는데 <img src="https://latex.codecogs.com/png.latex?O">가 결측 원인이 되므로, 일반적으로 복원이 불가능(Non-recoverable)합니다[cite: 218, 224].</p>
<p>하지만 <strong>모든 MNAR이 복원 불가능한 것은 아닙니다.</strong> m-graph 구조에 따라 MNAR임에도 복원 가능한 특수한 케이스들이 존재합니다.</p>
<hr>
</section>
</section>
<section id="advanced-recoverability-theorems" class="level2">
<h2 class="anchored" data-anchor-id="advanced-recoverability-theorems">5. Advanced Recoverability Theorems</h2>
<p>MNAR 상황에서도 복원 가능한 조건을 다루는 두 가지 중요한 정리가 있습니다.</p>
<section id="sequential-factorization-순차적-분해" class="level3">
<h3 class="anchored" data-anchor-id="sequential-factorization-순차적-분해">5.1. Sequential Factorization (순차적 분해)</h3>
<p>[cite_start]타겟 분포 <img src="https://latex.codecogs.com/png.latex?Q">를 인수분해했을 때, 각 인수가 <img src="https://latex.codecogs.com/png.latex?P(Y_i%20%7C%20X_i)"> 꼴을 띠고, 각각의 <img src="https://latex.codecogs.com/png.latex?Y_i">가 <img src="https://latex.codecogs.com/png.latex?X_i">가 주어졌을 때 자신의 결측 메커니즘 <img src="https://latex.codecogs.com/png.latex?R_%7BY_i%7D">과 독립이라면 복원 가능합니다[cite: 247].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(Y_i%20%5Cmid%20X_i)%20=%20P(Y_i%5E*%20%5Cmid%20X_i%5E*,%20R_%7BY_i%7D=0,%20R_%7BX_i%7D=0)%0A"></p>
</section>
<section id="r-factorization-theorem" class="level3">
<h3 class="anchored" data-anchor-id="r-factorization-theorem">5.2. R-Factorization Theorem</h3>
<p>전체 결합 분포 <img src="https://latex.codecogs.com/png.latex?P(V)">의 복원 가능성에 대한 필요충분조건입니다. [cite_start]<img src="https://latex.codecogs.com/png.latex?R"> 변수들 간에 엣지가 없다고 가정할 때, 다음 두 조건에 해당하는 변수 <img src="https://latex.codecogs.com/png.latex?X%20%5Cin%20V_m">이 <strong>없다면</strong> 복원 가능합니다[cite: 254, 255, 256].</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?R_X">가 이웃(neighbor)함.</li>
<li><img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?R_X">가 <img src="https://latex.codecogs.com/png.latex?V_m%20%5Ccup%20V_o"> 내의 collider들로만 이루어진 경로로 연결됨.</li>
</ol>
<p>[cite_start]복원 가능하다면, 분포는 다음과 같이 주어집니다[cite: 258]:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(V)%20=%20%5Cfrac%7BP(R=0,%20V%5E*)%7D%7B%5Cprod_i%20P(R_i=0%20%5Cmid%20Mb_%7BR_i%7D%5Eo,%20Mb_%7BR_i%7D%5Em,%20R_%7BMb_%7BR_i%7D%5Em%7D=0)%7D%0A"></p>
<p>여기서 <img src="https://latex.codecogs.com/png.latex?Mb_%7BR_i%7D">는 <img src="https://latex.codecogs.com/png.latex?R_i">의 Markov Blanket을 의미합니다.</p>
<hr>
</section>
</section>
<section id="case-study-recovering-causal-effect-pydoz" class="level2">
<h2 class="anchored" data-anchor-id="case-study-recovering-causal-effect-pydoz">6. Case Study: Recovering Causal Effect (<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z))">)</h2>
<p>[cite_start]마지막으로, 결측 데이터가 있는 상황에서 인과 효과(Causal Effect)를 추정하는 예제를 살펴보겠습니다[cite: 261].</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-05/images/causal_effect_recovery_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: Causal Effect 복원 예시 그래프. <img src="https://latex.codecogs.com/png.latex?W%20%5Crightarrow%20Z%20%5Crightarrow%20Y">의 인과 경로가 있고, <img src="https://latex.codecogs.com/png.latex?W">가 <img src="https://latex.codecogs.com/png.latex?Y">의 결측(<img src="https://latex.codecogs.com/png.latex?R_Y">)에 영향을 주며, <img src="https://latex.codecogs.com/png.latex?W">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에 잠재적 교란 요인(dashed bidirectional edge)이 존재하는 복잡한 MNAR 상황이다.</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p><strong>Figure 5 설명</strong>: 이 그래프에서 우리는 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 인과 효과 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z))">를 알고 싶습니다. 하지만 <img src="https://latex.codecogs.com/png.latex?Y">에는 결측이 있고, 결측 메커니즘 <img src="https://latex.codecogs.com/png.latex?R_Y">는 <img src="https://latex.codecogs.com/png.latex?W">에 의존합니다(<img src="https://latex.codecogs.com/png.latex?W%20%5Crightarrow%20R_Y">). 또한 <img src="https://latex.codecogs.com/png.latex?W">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에는 Confounder가 있습니다.</p>
</blockquote>
<p><strong>유도 과정:</strong></p>
<ol type="1">
<li><p><strong>Backdoor Adjustment</strong>: <img src="https://latex.codecogs.com/png.latex?W">가 <img src="https://latex.codecogs.com/png.latex?Z%20%5Crightarrow%20Y"> 관계의 confounder 역할을 하므로, <img src="https://latex.codecogs.com/png.latex?W">에 대해 조정(adjustment)을 수행합니다. <img src="https://latex.codecogs.com/png.latex?P(y%20%5Cmid%20do(z))%20=%20%5Csum_w%20P(y%20%5Cmid%20w,%20z)%20P(w)"> [cite_start]<em>(주의: 슬라이드 수식 [cite: 269]에서는 <img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z))">를 바로 전개하지만, 여기서는 이해를 돕기 위해 Backdoor 기준을 적용한 형태를 풀어서 설명합니다. 슬라이드의 전개는 do-calculus와 결측 매커니즘 분해를 결합한 것입니다.)</em></p>
<p>슬라이드의 유도 과정을 따라가면: <img src="https://latex.codecogs.com/png.latex?P(y%20%5Cmid%20do(z))%20=%20P(y%20%5Cmid%20do(z),%20r_y)%20%5Cquad%20%5Ctext%7B($R_y$%EB%8A%94%20%EA%B0%9C%EC%9E%85%20%EC%9D%B4%ED%9B%84%20%EB%B3%80%EC%88%98%EA%B0%80%20%EC%95%84%EB%8B%98)%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?R_y">가 조건부에 포함되면, 우리는 <img src="https://latex.codecogs.com/png.latex?Y"> 대신 <img src="https://latex.codecogs.com/png.latex?Y%5E*">를 사용할 수 있습니다 (<img src="https://latex.codecogs.com/png.latex?Y%20=%20Y%5E*"> when <img src="https://latex.codecogs.com/png.latex?R_Y=0"> or implicit context). <img src="https://latex.codecogs.com/png.latex?=%20P(y%5E*%20%5Cmid%20do(z),%20r_y)"></p>
<p>이 확률을 <img src="https://latex.codecogs.com/png.latex?W">를 통해 분해합니다: <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_w%20P(y%5E*%20%5Cmid%20w,%20do(z),%20r_y)%20P(w%20%5Cmid%20do(z),%20r_y)"></p>
<p>그래프 상의 독립성을 활용합니다. <img src="https://latex.codecogs.com/png.latex?Z">에 개입(<img src="https://latex.codecogs.com/png.latex?do(z)">)하면 <img src="https://latex.codecogs.com/png.latex?W">에서 오는 화살표는 무시되거나, <img src="https://latex.codecogs.com/png.latex?W">가 <img src="https://latex.codecogs.com/png.latex?Z">의 부모이므로 <img src="https://latex.codecogs.com/png.latex?do(z)">와 독립일 수 있습니다. 중요한 점은 <img src="https://latex.codecogs.com/png.latex?Y%5E*">가 관측 가능해진다는 것입니다.</p>
<p>[cite_start]최종적으로 슬라이드는 다음과 같은 형태의 복원식을 제시합니다[cite: 269]: <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_w%20P(y%5E*%20%5Cmid%20w,%20z,%20r_y)%20P(w%20%5Cmid%20r_y)"></p>
<p>이 식의 의미는, <strong>결측이 있는 상태(<img src="https://latex.codecogs.com/png.latex?Y%5E*">)에서도, 결측 메커니즘(<img src="https://latex.codecogs.com/png.latex?R_Y">)과 관련된 변수(<img src="https://latex.codecogs.com/png.latex?W">)들을 적절히 통제하면 원래의 인과 효과를 계산해낼 수 있다</strong>는 것입니다.</p></li>
</ol>
<hr>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">7. Conclusion</h2>
<p>이번 포스트에서는 결측 데이터 문제를 인과적 관점에서 바라보는 <strong>m-graph</strong> 프레임워크를 다뤘습니다.</p>
<ol type="1">
<li>[cite_start]<strong>Transparency</strong>: 그래프 모델은 결측의 원인(Missingness Mechanism)을 명시적으로 표현하여, MAR/MCAR 가정을 시각적으로 검증(Testability)할 수 있게 해줍니다[cite: 32, 36].</li>
<li><strong>Recoverability</strong>: 단순히 데이터를 삭제하거나 평균을 채워 넣는 것이 아니라, 그래프 구조에 기반하여 이론적으로 타당한 복원 공식을 유도할 수 있습니다.</li>
<li>[cite_start]<strong>Beyond MAR</strong>: 기존 통계학에서 다루기 어려웠던 MNAR 상황에서도, 구조적 특성을 이용해(Sequential Factorization 등) 편향 없는 추정이 가능함을 확인했습니다[cite: 24, 253].</li>
</ol>
<p>[cite_start]통계학자와 데이터 과학자는 결측치를 단순한 “전처리 대상”이 아니라, <strong>데이터 생성 과정의 일부</strong>로 보고 모델링해야 합니다[cite: 272].</p>
<hr>
<section id="누락-방지-검증-체크리스트" class="level3">
<h3 class="anchored" data-anchor-id="누락-방지-검증-체크리스트">누락 방지 검증 체크리스트</h3>
<ul>
<li><strong>포함된 내용</strong>:
<ul class="task-list">
<li><label><input type="checkbox" checked="">결측 데이터의 동기 및 기존 방법론(Rubin)의 한계</label></li>
<li><label><input type="checkbox" checked="">m-graph의 정의 및 구성 요소 (<img src="https://latex.codecogs.com/png.latex?V%5E*,%20R"> 등)</label></li>
<li><label><input type="checkbox" checked="">MCAR, MAR, MNAR의 그래프적 정의 및 예시</label></li>
<li><label><input type="checkbox" checked="">MCAR, MAR, MNAR 상황별 복원 가능성(Recoverability) 유도 과정</label></li>
<li><label><input type="checkbox" checked="">고급 정리: Sequential Factorization, R-Factorization Theorem 수식</label></li>
<li><label><input type="checkbox" checked="">Causal Effect (<img src="https://latex.codecogs.com/png.latex?P(y%7Cdo(z))">) 복원 예제</label></li>
</ul></li>
<li><strong>생략된 내용</strong>:
<ul>
<li>슬라이드 24의 복잡한 MNAR 예제 (<img src="https://latex.codecogs.com/png.latex?Z_1,%20Z_2,%20X,%20Y">)에 대한 세부 유도 과정은 R-Factorization Theorem의 일반론으로 갈음하고, 대신 Causal Effect 예제를 상세히 다루었습니다. (흐름상 핵심 정리를 보여주는 것이 더 중요하다고 판단)</li>
<li>JASA 논문 원문의 증명(Proof) 등은 슬라이드 범위를 벗어나므로 제외했습니다.</li>
</ul></li>
</ul>
<hr>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-05/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 17. Causal Data Science (Part 6)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L17/part-06/</link>
  <description><![CDATA[ 





<section id="introduction-데이터-융합data-fusion의-필요성" class="level2">
<h2 class="anchored" data-anchor-id="introduction-데이터-융합data-fusion의-필요성">1. Introduction: 데이터 융합(Data Fusion)의 필요성</h2>
<p>현대 데이터 과학, 특히 사회과학과 공학이 교차하는 지점에서는 <strong>“모든 변수가 완벽하게 측정된 단 하나의 데이터셋”</strong>을 확보하는 것이 거의 불가능합니다. 현실에서는 다음과 같은 상황이 빈번하게 발생합니다.</p>
<ul>
<li><strong>실험 연구(Experimental Study):</strong> 핵심적인 처치(Treatment)와 주요 결과 변수만 측정하며, 비용 문제로 많은 공변량을 측정하지 못함.</li>
<li><strong>관찰 연구(Observational Study):</strong> 방대한 공변량과 결과 변수가 존재하지만, 처치 변수가 없거나 무작위 배정(Randomization)이 되어 있지 않음.</li>
</ul>
<p>이러한 상황에서, 서로 다른 변수 집합(<img src="https://latex.codecogs.com/png.latex?V_i%20%5Csubseteq%20V">)을 포함하는 이질적인 데이터셋들을 결합하여, 전체 시스템의 인과 효과(Causal Effect)를 추정할 수 있을까요? 이번 포스트에서는 <strong>General Identifiability with Partial Observation (GID-PO)</strong> 개념을 통해 이 문제를 해결하는 과정을 다룹니다.</p>
</section>
<section id="problem-setup-gid-po" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-gid-po">2. Problem Setup: GID-PO</h2>
<p>우리가 관심을 가지는 전체 변수 집합을 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BV%7D">라고 합시다. 하지만 우리는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BV%7D"> 전체를 관측한 데이터가 없습니다. 대신, <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BV%7D">의 부분집합인 <img src="https://latex.codecogs.com/png.latex?V_i">만을 관측한 여러 개의 데이터셋(실험 또는 관찰) 모음 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D=%5C%7BP_%7BZ_%7Bi%7D%7D(V_%7Bi%7D)%5C%7D_%7Bi%7D">를 가지고 있습니다.</p>
<p>이때 우리의 목표는 타겟 인과 효과 <img src="https://latex.codecogs.com/png.latex?P_x(y)"> (처치 <img src="https://latex.codecogs.com/png.latex?X">가 결과 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 효과)를 식별(Identify)하는 것입니다.</p>
<section id="예시-시나리오-운동이-뇌졸중에-미치는-영향" class="level3">
<h3 class="anchored" data-anchor-id="예시-시나리오-운동이-뇌졸중에-미치는-영향">2.1 예시 시나리오: 운동이 뇌졸중에 미치는 영향</h3>
<p>강의 자료에서 제시된 구체적인 예시를 통해 문제를 정의해 봅시다. 우리는 <strong>운동(Exercise, <img src="https://latex.codecogs.com/png.latex?X">)</strong>이 <strong>뇌졸중(Stroke, <img src="https://latex.codecogs.com/png.latex?Y">)</strong>에 미치는 인과적 효과를 알고 싶습니다.</p>
<p>관련된 변수들은 다음과 같습니다: * <img src="https://latex.codecogs.com/png.latex?A">: 나이 (Age) * <img src="https://latex.codecogs.com/png.latex?X">: 운동 (Exercise) - <strong>처치 변수</strong> * <img src="https://latex.codecogs.com/png.latex?B">: BMI * <img src="https://latex.codecogs.com/png.latex?C">: 혈압 (Blood Pressure) * <img src="https://latex.codecogs.com/png.latex?Y">: 뇌졸중 (Stroke) - <strong>결과 변수</strong></p>
<p>이 변수들의 인과 관계는 아래의 Causal Graph(DAG)로 표현됩니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-06/images/causal_graph_stroke_exercise.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: Exercise &amp; Stroke Causal Graph. A(Age)는 X(Exercise)와 B(BMI)에 영향을 줌. X는 B에 영향을 줌. B는 C(Blood Pressure)에 영향을 줌. C는 Y(Stroke)에 영향을 줌. 점선(Dashed Line)은 X와 Y 사이에 관측되지 않은 교란 요인(Unobserved Confounder)이 존재함을 암시함.</figcaption>
</figure>
</div>
</section>
<section id="주어진-데이터의-한계" class="level3">
<h3 class="anchored" data-anchor-id="주어진-데이터의-한계">2.2 주어진 데이터의 한계</h3>
<p>우리는 <img src="https://latex.codecogs.com/png.latex?P(A,%20X,%20B,%20C,%20Y)"> 전체를 포함하는 데이터가 없습니다. 대신 두 가지의 불완전한 연구 결과만 가지고 있다고 가정해 봅시다.</p>
<ol type="1">
<li><strong>연구 1 (Experimental Study):</strong>
<ul>
<li>운동(<img src="https://latex.codecogs.com/png.latex?X">)에 대해 무작위 배정(Intervention)을 수행했습니다.</li>
<li>측정 변수: 나이(<img src="https://latex.codecogs.com/png.latex?A">), 혈압(<img src="https://latex.codecogs.com/png.latex?C">). (BMI와 뇌졸중은 측정하지 않음)</li>
<li>확보된 분포: <img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)"></li>
</ul></li>
<li><strong>연구 2 (Observational Study):</strong>
<ul>
<li>단순 관찰 연구입니다.</li>
<li>측정 변수: BMI(<img src="https://latex.codecogs.com/png.latex?B">), 혈압(<img src="https://latex.codecogs.com/png.latex?C">), 뇌졸중(<img src="https://latex.codecogs.com/png.latex?Y">). (나이와 운동 여부는 데이터에 없음)</li>
<li>확보된 분포: <img src="https://latex.codecogs.com/png.latex?P(B,%20C,%20Y)"></li>
</ul></li>
</ol>
<p><strong>Task:</strong> 이 두 가지 부분적인 분포 <img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)">와 <img src="https://latex.codecogs.com/png.latex?P(B,%20C,%20Y)">만을 사용하여, 타겟 인과 효과 <strong><img src="https://latex.codecogs.com/png.latex?P_X(Y)"></strong>를 계산할 수 있는가?</p>
</section>
</section>
<section id="mathematical-derivations" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-derivations">3. Mathematical Derivations</h2>
<p>이 문제는 단순히 데이터를 합치는(Merge) 것으로는 해결되지 않습니다. 공통 변수가 <img src="https://latex.codecogs.com/png.latex?C">밖에 없으며, 연구 1은 실험 데이터, 연구 2는 관찰 데이터라는 성격의 차이가 있기 때문입니다. 우리는 확률 분포의 분해(Factorization) 법칙과 인과 그래프의 구조를 이용해야 합니다.</p>
<section id="step-1-target-distribution의-정의" class="level3">
<h3 class="anchored" data-anchor-id="step-1-target-distribution의-정의">3.1 Step 1: Target Distribution의 정의</h3>
<p>우리가 구하고자 하는 것은 <img src="https://latex.codecogs.com/png.latex?X">에 개입했을 때 <img src="https://latex.codecogs.com/png.latex?Y">의 주변 확률 분포(Marginal Distribution)입니다. <img src="https://latex.codecogs.com/png.latex?P_x(y)%20=%20%5Csum_%7Ba,b,c%7D%20P_x(a,%20b,%20c,%20y)"></p>
</section>
<section id="step-2-c-component-factorization" class="level3">
<h3 class="anchored" data-anchor-id="step-2-c-component-factorization">3.2 Step 2: C-Component Factorization</h3>
<p>인과 그래프(Figure 1)의 구조에 따라 결합 확률 분포(Joint Distribution)를 분해해 봅시다. 특히 <img src="https://latex.codecogs.com/png.latex?do(X)"> 연산이 적용된 개입 분포(Interventional Distribution) <img src="https://latex.codecogs.com/png.latex?P_x(%5Ccdot)">를 고려합니다.</p>
<p>일반적인 베이지안 네트워크 분해 법칙과 <img src="https://latex.codecogs.com/png.latex?do">-calculus의 원리에 따라, <img src="https://latex.codecogs.com/png.latex?X">로 들어오는 화살표를 제거한 그래프에서의 분해를 생각할 수 있습니다. 강의 자료의 유도 과정(Slide 8-10)에 따르면, 이 시스템은 다음과 같은 형태로 분해(Factorization)될 수 있습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?P_x(a,%20b,%20c,%20y)%20=%20P(a)%20%5Ccdot%20P_%7Ba,x%7D(b)%20%5Ccdot%20P_b(c)%20%5Ccdot%20P_c(y)"></p>
<p>이 식의 각 항이 의미하는 바는 다음과 같습니다: * <img src="https://latex.codecogs.com/png.latex?P(a)">: 나이의 분포 (외생 변수). * <img src="https://latex.codecogs.com/png.latex?P_%7Ba,x%7D(b)">: <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때(또는 개입했을 때) <img src="https://latex.codecogs.com/png.latex?B">의 분포. * <img src="https://latex.codecogs.com/png.latex?P_b(c)">: <img src="https://latex.codecogs.com/png.latex?B">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?C">의 분포. * <img src="https://latex.codecogs.com/png.latex?P_c(y)">: <img src="https://latex.codecogs.com/png.latex?C">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?Y">의 분포.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Why this factorization?</strong> 이 분해는 그래프의 <strong>c-component</strong> 구조에 기반합니다. <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이에는 직접적인(또는 confounding에 의한) 경로가 존재하지만, <img src="https://latex.codecogs.com/png.latex?A%20%5Cto%20B%20%5Cto%20C%20%5Cto%20Y">와 같은 매개 경로들이 존재합니다. 이 식은 전체 효과를 각 메커니즘의 결합으로 쪼갠 것입니다.</p>
</div>
</div>
</section>
<section id="step-3-수식의-재구성-regrouping" class="level3">
<h3 class="anchored" data-anchor-id="step-3-수식의-재구성-regrouping">3.3 Step 3: 수식의 재구성 (Regrouping)</h3>
<p>이제 위에서 얻은 식을 우리가 가진 데이터 <img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)">와 <img src="https://latex.codecogs.com/png.latex?P(B,%20C,%20Y)">와 연결하기 위해 <img src="https://latex.codecogs.com/png.latex?%5Csum">의 순서를 조정하여 재구성합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AP_x(y)%20&amp;=%20%5Csum_%7Ba,b,c%7D%20P(a)%20P_%7Ba,x%7D(b)%20P_b(c)%20P_c(y)%20%5C%5C%0A&amp;=%20%5Csum_%7Ba,c%7D%20P(a)%20%5Cleft(%20%5Csum_%7Bb%7D%20P_%7Ba,x%7D(b)%20P_b(c)%20%5Cright)%20P_c(y)%0A%5Cend%7Baligned%7D%0A"></p>
<p>여기서 괄호 안의 부분 <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bb%7D%20P_%7Ba,x%7D(b)%20P_b(c)">를 살펴봅시다. 이는 <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?X">가 결정되었을 때, <img src="https://latex.codecogs.com/png.latex?B">를 거쳐 <img src="https://latex.codecogs.com/png.latex?C">가 결정되는 과정의 확률, 즉 <img src="https://latex.codecogs.com/png.latex?P_%7Ba,x%7D(c)">로 해석할 수 있습니다 (Chain Rule of Intervention).</p>
<p>따라서 식은 다음과 같이 간소화됩니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?P_x(y)%20=%20%5Csum_%7Ba,c%7D%20P(a)%20P_%7Ba,x%7D(c)%20P_c(y)"></p>
<p>이 식은 매우 중요한 의미를 가집니다. <strong>서로 다른 출처의 정보를 결합할 수 있는 연결고리</strong>가 되기 때문입니다.</p>
</section>
</section>
<section id="mapping-to-data-sources-the-puzzle" class="level2">
<h2 class="anchored" data-anchor-id="mapping-to-data-sources-the-puzzle">4. Mapping to Data Sources (The Puzzle)</h2>
<p>이제 최종 유도된 식의 각 구성 요소가 우리가 가진 두 개의 데이터셋 중 어디에서 올 수 있는지 확인해 봅시다.</p>
<p><img src="https://latex.codecogs.com/png.latex?P_x(y)%20=%20%5Csum_%7Ba,c%7D%20%5Cunderbrace%7BP(a)%20P_%7Ba,x%7D(c)%7D_%7B%5Ctext%7BSource%201%7D%7D%20%5Ccdot%20%5Cunderbrace%7BP_c(y)%7D_%7B%5Ctext%7BSource%202%7D%7D"></p>
<section id="source-1-experimental-study-p_xa-c" class="level3">
<h3 class="anchored" data-anchor-id="source-1-experimental-study-p_xa-c">4.1 Source 1: Experimental Study (<img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)">)</h3>
<p>첫 번째 데이터셋은 운동(<img src="https://latex.codecogs.com/png.latex?X">)에 개입한 실험 데이터이며 <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?C">를 관측합니다. 즉, <img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)"> 분포를 온전히 가지고 있습니다.</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?P(a)">: <img src="https://latex.codecogs.com/png.latex?X">는 무작위 배정되었으므로 <img src="https://latex.codecogs.com/png.latex?A">와 독립입니다. 따라서 <img src="https://latex.codecogs.com/png.latex?P_x(a)%20=%20P(a)">. 실험군 내의 나이 분포에서 얻을 수 있습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?P_%7Ba,x%7D(c)">: <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?X">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?C">의 분포입니다. 실험 데이터 <img src="https://latex.codecogs.com/png.latex?P_X(A,%20C)">에서 조건부 확률 등을 통해 도출할 수 있습니다. (엄밀히는 <img src="https://latex.codecogs.com/png.latex?P_x(c%7Ca)"> 형태)</li>
</ul>
<p>따라서 식의 앞부분 <img src="https://latex.codecogs.com/png.latex?P(a)P_%7Ba,x%7D(c)">는 <strong>연구 1</strong>에서 식별 가능합니다.</p>
</section>
<section id="source-2-observational-study-pb-c-y" class="level3">
<h3 class="anchored" data-anchor-id="source-2-observational-study-pb-c-y">4.2 Source 2: Observational Study (<img src="https://latex.codecogs.com/png.latex?P(B,%20C,%20Y)">)</h3>
<p>두 번째 데이터셋은 <img src="https://latex.codecogs.com/png.latex?B,%20C,%20Y">를 관측합니다. 우리가 필요한 마지막 조각은 <img src="https://latex.codecogs.com/png.latex?P_c(y)">입니다.</p>
<ul>
<li>그래프 구조상 <img src="https://latex.codecogs.com/png.latex?C">에서 <img src="https://latex.codecogs.com/png.latex?Y">로 가는 길목에 <img src="https://latex.codecogs.com/png.latex?B">가 교란 요인으로 작용하지 않고(앞단의 변수임), <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y"> 사이의 Confounding path는 존재하지만, <img src="https://latex.codecogs.com/png.latex?C"> 자체에 대한 개입(<img src="https://latex.codecogs.com/png.latex?P_c(y)">)을 고려할 때 <img src="https://latex.codecogs.com/png.latex?P(B,%20C,%20Y)"> 데이터 내에서 이를 추정할 수 있는 구조적 조건이 성립합니다.</li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?P_c(y)">는 <strong>연구 2</strong>의 관찰 데이터로부터 복원해낼 수 있습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L17/part-06/images/data_fusion_puzzle.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: The Puzzle of Data Fusion. 보라색 퍼즐 조각(<img src="https://latex.codecogs.com/png.latex?P_c(y)">, <img src="https://latex.codecogs.com/png.latex?P_%7Ba,x%7D(b)"> 등)과 초록색 퍼즐 조각(<img src="https://latex.codecogs.com/png.latex?P(a)">, <img src="https://latex.codecogs.com/png.latex?P_b(c)">)이 서로 다른 데이터셋에서 추출되어 하나의 완성된 그림(<img src="https://latex.codecogs.com/png.latex?P_x(y)">)을 맞추는 과정을 시각화함.</figcaption>
</figure>
</div>
</section>
<section id="final-formula" class="level3">
<h3 class="anchored" data-anchor-id="final-formula">4.3 Final Formula</h3>
<p>결과적으로 우리는 타겟 효과를 다음과 같이 계산할 수 있습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?P_x(y)%20=%20%5Csum_%7Ba,c%7D%20P_x(a,%20c)%20%5Ctimes%20P_c(y)"></p>
<p>(참고: 위 식은 직관적인 매핑을 보여주며, 실제 계산 시에는 <img src="https://latex.codecogs.com/png.latex?P_c(y)">를 <img src="https://latex.codecogs.com/png.latex?P(B,C,Y)">에서 도출하기 위한 추가적인 adjustment formula가 적용될 수 있습니다. 강의 자료 Slide 10 하단에는 다음과 같은 GID 공식이 제시되어 있습니다.)</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BGID%7D%20=%20%5Csum_%7Ba,c%7D%20P_%7Bx'%7D(a)%20P_x(c%7Ca)%20%5Cleft(%20%5Csum_b%20P(y%7Cb,c)P(b)%20%5Cright)"></p>
<p>이 공식은 각 데이터 소스에서 얻을 수 있는 확률값들의 곱과 합으로 타겟 인과 효과를 완벽하게 재구성합니다.</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">5. Conclusions</h2>
<p>이번 GID-PO 프레임워크가 시사하는 바는 단순히 수식을 푸는 것 이상입니다.</p>
<ol type="1">
<li><strong>Data Fusion의 이론적 토대:</strong> 서로 다른 변수를 측정하는 이질적인 데이터셋들도, 그 기저에 있는 <strong>인과 메커니즘(Causal Mechanism)</strong>을 공유한다면 논리적으로 결합될 수 있습니다.</li>
<li><strong>데이터 과학의 새로운 패러다임:</strong> 빅데이터 시대에는 “모든 것을 측정한 스몰 데이터”보다 “부분적으로 측정한 빅 데이터”가 더 흔합니다. GID-PO는 이러한 환경에서 선택 편향(Selection Bias), 교란 편향(Confounding Bias), 그리고 데이터 결측(Missing Data) 문제를 통합적으로 해결할 수 있는 충분조건과 알고리즘을 제공합니다.</li>
<li><strong>Parsimonious Representation:</strong> 인과 그래프는 현상을 설명하는 가장 간결하고(coarse and parsimonious) 효율적인 표현 방식이며, 이를 통해 데이터 통합의 복잡성을 관리할 수 있습니다.</li>
</ol>
<hr>
<section id="appendix-checklist" class="level3">
<h3 class="anchored" data-anchor-id="appendix-checklist"><strong>Appendix: Checklist</strong></h3>
<p>본 포스트 작성을 위해 검토한 강의 자료 체크리스트입니다.</p>
<ul class="task-list">
<li><label><input type="checkbox" checked=""><strong>GID-PO 개념 정의</strong>: 부분 관측(Partial Observation) 하에서의 식별 가능성 문제 정의 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Causal Graph &amp; Variables</strong>: Exercise-Stroke 예시의 변수(<img src="https://latex.codecogs.com/png.latex?A,%20X,%20B,%20C,%20Y">) 및 그래프 구조 반영 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Data Sources</strong>: 실험 연구(<img src="https://latex.codecogs.com/png.latex?P_X(A,C)">)와 관찰 연구(<img src="https://latex.codecogs.com/png.latex?P(B,C,Y)">)의 차이 명시 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Mathematical Derivation</strong>: <img src="https://latex.codecogs.com/png.latex?P_x(y)">의 분해 과정(Factorization) 및 단계별 유도(<img src="https://latex.codecogs.com/png.latex?%5Csum"> regrouping) 포함 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Data Mapping</strong>: 유도된 수식의 각 항이 어느 데이터셋에서 식별 가능한지 설명 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Visuals</strong>: 인과 그래프 및 퍼즐 비유 이미지에 대한 설명(Alt text) 포함 완료.</label></li>
<li><label><input type="checkbox" checked=""><strong>Conclusion</strong>: 인과 데이터 과학(Causal Data Science) 관점에서의 의의 서술 완료.</label></li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L17/part-06/</guid>
  <pubDate>Sat, 24 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 02. Causal Models and Graphs (Part 2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L02/part-02/</link>
  <description><![CDATA[ 





<section id="introduction-why-causal-models" class="level1">
<h1>1. Introduction: Why Causal Models?</h1>
<p>기존의 통계학이나 머신러닝의 추론(Inference)은 주로 <strong>결합 확률 분포(Joint Distribution)</strong> <img src="https://latex.codecogs.com/png.latex?P(v)">를 파악하는 데 집중합니다. [cite_start]“고객이 A를 샀을 때 B도 살 확률은?”(<img src="https://latex.codecogs.com/png.latex?P(B%7CA)">)과 같은 질문은 데이터의 상관관계(Association)만으로도 충분히 답할 수 있습니다. [cite: 2412-2419]</p>
<p>하지만 “가격을 두 배로 올리면 판매량은 어떻게 변할까?”와 같은 <strong>개입(Intervention)</strong>이나 반사실적(Counterfactual) 질문에 답하기 위해서는 데이터 그 자체(<img src="https://latex.codecogs.com/png.latex?P">)를 넘어, 데이터가 생성되는 <strong>현실의 메커니즘(Reality)</strong>을 이해해야 합니다. [cite_start]이를 위해 우리는 <strong>구조적 인과 모델(Structural Causal Model, SCM)</strong>이라는 새로운 언어를 배웁니다. [cite: 2421-2449]</p>
<section id="motivation-simpsons-paradox-example" class="level2">
<h2 class="anchored" data-anchor-id="motivation-simpsons-paradox-example">1.1 Motivation: Simpson’s Paradox Example</h2>
<p>[cite_start]강의에서는 의사결정의 어려움을 보여주기 위해 가상의 전염병과 치료제 시나리오를 제시합니다. [cite: 2236-2246]</p>
<ul>
<li><strong>상황</strong>: 특정 도시에 전염병이 돌고 있고, 치료제(Drug)가 있습니다.</li>
<li><strong>숨겨진 진실(Reality - Unknown to physicians)</strong>:
<ol type="1">
<li><strong>부유층(Rich)</strong>: 생활 환경이 좋아 약물 복용 여부와 상관없이 생존합니다.</li>
<li><strong>빈곤층(Poor)</strong>:
<ul>
<li>유전 인자(Gene)가 없는 경우: 자연 치유력이 없어 사망합니다.</li>
<li>유전 인자가 있는 경우: 약을 먹으면 알레르기 반응으로 사망하고, 안 먹으면 생존합니다.</li>
</ul></li>
<li><strong>현재 처방 관행</strong>: 약값이 비싸서 의사들은 부유층에게만 약을 처방합니다.</li>
</ol></li>
</ul>
<p>이 상황에서 데이터만 관측하면(<img src="https://latex.codecogs.com/png.latex?P(r,%20d,%20a)">), 약을 먹은 사람(주로 부유층)은 생존율이 높고, 안 먹은 사람(주로 빈곤층)은 생존율이 낮게 나옵니다. 머신러닝 모델은 “약을 먹는 것이 생존에 유리하다”고 잘못된 결론을 내릴 수 있습니다. [cite_start]하지만 실제 메커니즘(SCM)을 안다면, <strong>“누구에게도 약을 주지 말아야 한다(빈곤층에게는 치명적, 부유층에게는 무의미)”</strong>는 정반대의 결론에 도달합니다. [cite: 2340-2402]</p>
<p>즉, 데이터(<img src="https://latex.codecogs.com/png.latex?P">)는 현실(<img src="https://latex.codecogs.com/png.latex?M">)의 그림자일 뿐이며, 올바른 인과 추론을 위해서는 <img src="https://latex.codecogs.com/png.latex?M">을 모델링해야 합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-02/images/inference_paradigm.png" class="img-fluid figure-img"></p>
<figcaption>Figure: 기존 통계적 추론과 인과 추론의 패러다임 비교. 통계적 추론은 데이터 P 내에서의 성질 Q(P)를 찾지만, 인과 추론은 데이터 생성 모델 M을 통해 현실을 이해하고 P’를 추정한다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="structural-causal-model-scm" class="level1">
<h1>2. Structural Causal Model (SCM)</h1>
<p>인과 관계를 수학적으로 정의하기 위해 SCM을 도입합니다. SCM은 현실의 메커니즘을 <strong>결정론적 함수</strong>와 <strong>확률적 노이즈</strong>의 결합으로 표현합니다.</p>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">2.1 Definition</h2>
<p>[cite_start]SCM <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">은 4개의 요소 <img src="https://latex.codecogs.com/png.latex?%5Clangle%20V,%20U,%20F,%20P(U)%20%5Crangle">로 구성된 튜플입니다. [cite: 2548-2554]</p>
<ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?V%20=%20%5C%7BV_1,%20...,%20V_n%5C%7D"> (Endogenous Variables)</strong>: 모델 내부에서 결정되며, 우리가 관측할 수 있는 변수들입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?U%20=%20%5C%7BU_1,%20...,%20U_m%5C%7D"> (Exogenous Variables)</strong>: 모델 외부에서 결정되는 변수들로, 관측되지 않는 배경 요인(Background factors)이나 노이즈를 의미합니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?F%20=%20%5C%7Bf_1,%20...,%20f_n%5C%7D"> (Structural Functions)</strong>: 각 내생 변수 <img src="https://latex.codecogs.com/png.latex?V_i">가 어떻게 결정되는지를 정의하는 함수 집합입니다. <img src="https://latex.codecogs.com/png.latex?v_i%20%5Cleftarrow%20f_i(pa_i,%20u_i)"> 여기서 <img src="https://latex.codecogs.com/png.latex?pa_i%20%5Csubseteq%20V%20%5Csetminus%20%5C%7BV_i%5C%7D">는 <img src="https://latex.codecogs.com/png.latex?V_i">의 부모 변수(직접적인 원인)들이고, <img src="https://latex.codecogs.com/png.latex?u_i%20%5Csubseteq%20U">는 관련된 외생 변수입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?P(U)"></strong>: 외생 변수 <img src="https://latex.codecogs.com/png.latex?U">에 대한 확률 분포입니다.</li>
</ol>
</section>
<section id="properties-of-scm" class="level2">
<h2 class="anchored" data-anchor-id="properties-of-scm">2.2 Properties of SCM</h2>
<p>[cite_start]SCM은 다음과 같은 중요한 성질을 가집니다. [cite: 2572-2637]</p>
<ol type="1">
<li><strong>Induces <img src="https://latex.codecogs.com/png.latex?P(V)"></strong>: 외생 변수의 분포 <img src="https://latex.codecogs.com/png.latex?P(U)">와 함수 <img src="https://latex.codecogs.com/png.latex?F">를 통해 관측 변수들의 결합 확률 분포 <img src="https://latex.codecogs.com/png.latex?P(V)">가 결정됩니다.</li>
<li><strong>Induces Causal Diagram</strong>: 변수 간의 함수적 관계(<img src="https://latex.codecogs.com/png.latex?f_i">)를 통해 인과 그래프(DAG)를 그릴 수 있습니다.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-02/images/scm_conceptual.png" class="img-fluid figure-img"></p>
<figcaption>Figure: SCM의 개념도. 외생 변수 U가 확률 분포 P(U)를 따르고, 함수 F를 통해 내생 변수 V의 값을 결정하여 관측 데이터 분포 P(V)를 유도한다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="causal-diagrams-dags" class="level1">
<h1>3. Causal Diagrams (DAGs)</h1>
<p>SCM은 시각적으로 <strong>유향 비순환 그래프(DAG, Directed Acyclic Graph)</strong>로 표현됩니다. [cite_start]그래프 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BG%7D%20=%20%5Clangle%20V,%20E%20%5Crangle">는 다음 규칙에 따라 생성됩니다. [cite: 2683-2687]</p>
<ol type="1">
<li><strong>Nodes</strong>: 각 내생 변수 <img src="https://latex.codecogs.com/png.latex?V_i">를 노드로 합니다.</li>
<li><strong>Directed Edges (<img src="https://latex.codecogs.com/png.latex?%5Crightarrow">)</strong>: 함수 <img src="https://latex.codecogs.com/png.latex?f_i">에서 <img src="https://latex.codecogs.com/png.latex?V_j">가 <img src="https://latex.codecogs.com/png.latex?V_i">의 인자(<img src="https://latex.codecogs.com/png.latex?pa_i">)로 사용되면 <img src="https://latex.codecogs.com/png.latex?V_j%20%5Crightarrow%20V_i"> 엣지를 그립니다.</li>
<li><strong>Bidirected Edges (<img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow">)</strong>: 두 변수 <img src="https://latex.codecogs.com/png.latex?V_i,%20V_j">가 공통된 외생 변수(Common unobserved confounder)를 공유하거나, 그들의 외생 변수 <img src="https://latex.codecogs.com/png.latex?U_i,%20U_j">가 서로 종속적(Correlated)일 때 점선 양방향 화살표로 연결합니다.</li>
</ol>
<hr>
</section>
<section id="markovian-factorization" class="level1">
<h1>4. Markovian Factorization</h1>
<p>SCM의 가장 강력한 점 중 하나는 복잡한 결합 확률 분포를 간단한 조건부 확률의 곱으로 분해할 수 있다는 것입니다. 이를 <strong>Markovian Factorization</strong> 또는 <strong>Bayesian Factorization</strong>이라고 합니다.</p>
<section id="markovian-condition" class="level2">
<h2 class="anchored" data-anchor-id="markovian-condition">4.1 Markovian Condition</h2>
<p>[cite_start]만약 모든 외생 변수 <img src="https://latex.codecogs.com/png.latex?U_i">들이 서로 <strong>독립(Jointly Independent)</strong>이라면, 즉 그래프에 양방향 엣지(<img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow">)가 하나도 없다면, 이 모델을 <strong>Markovian</strong>이라고 합니다. [cite: 2988-2991]</p>
</section>
<section id="mathematical-derivation" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-derivation">4.2 Mathematical Derivation</h2>
<p>[cite_start]Markovian 가정 하에서 결합 확률 분포 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)">가 어떻게 분해되는지 단계별로 유도해 보겠습니다. [cite: 2992-2993]</p>
<p><strong>Step 1: Law of Total Probability</strong> 모든 변수 <img src="https://latex.codecogs.com/png.latex?V">의 결합 확률은 외생 변수 <img src="https://latex.codecogs.com/png.latex?U">를 포함한 전체 확률에서 <img src="https://latex.codecogs.com/png.latex?U">를 합(Summing out)하여 얻습니다. SCM에서 <img src="https://latex.codecogs.com/png.latex?v_i">는 <img src="https://latex.codecogs.com/png.latex?pa_i">와 <img src="https://latex.codecogs.com/png.latex?u_i">에 의해 결정되므로(<img src="https://latex.codecogs.com/png.latex?P(v_i%7Cpa_i,%20u_i)">는 0 또는 1), 다음과 같이 쓸 수 있습니다. <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20P(%5Cmathbf%7Bu%7D)%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20pa_i,%20u_i)"></p>
<p><strong>Step 2: Independence of Exogenous Variables</strong> Markovian 가정에 의해 <img src="https://latex.codecogs.com/png.latex?U">들이 서로 독립이므로, <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bu%7D)%20=%20%5Cprod%20P(u_i)">가 됩니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20pa_i,%20u_i)%20P(u_i)"></p>
<p><strong>Step 3: Independence of <img src="https://latex.codecogs.com/png.latex?U_i"> and <img src="https://latex.codecogs.com/png.latex?Pa_i"></strong> 외생 변수 <img src="https://latex.codecogs.com/png.latex?U_i">는 시스템 외부에서 결정되므로 내생 변수인 부모 <img src="https://latex.codecogs.com/png.latex?Pa_i">와 독립입니다. 따라서 <img src="https://latex.codecogs.com/png.latex?P(u_i)%20=%20P(u_i%20%5Cmid%20pa_i)">로 쓸 수 있습니다. 이를 식에 대입하고, 확률의 곱셈 법칙(<img src="https://latex.codecogs.com/png.latex?P(A%7CB)P(B)%20=%20P(A,B)">)을 적용합니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20pa_i,%20u_i)%20P(u_i%20%5Cmid%20pa_i)"> <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i,%20u_i%20%5Cmid%20pa_i)"></p>
<p><strong>Step 4: Commutativity of Sum and Product</strong> 각 항은 자신에게 해당되는 <img src="https://latex.codecogs.com/png.latex?u_i">에만 의존하므로, 전체 합(<img src="https://latex.codecogs.com/png.latex?%5Csum_%7B%5Cmathbf%7Bu%7D%7D">)을 개별 합(<img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bu_i%7D">)의 곱으로 바꿀 수 있습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20%5Cleft(%20%5Csum_%7Bu_i%7D%20P(v_i,%20u_i%20%5Cmid%20pa_i)%20%5Cright)"></p>
<p><strong>Step 5: Marginalization (Final Result)</strong> 괄호 안의 식은 결합 확률에서 <img src="https://latex.codecogs.com/png.latex?u_i">를 마지널라이즈(Marginalize)한 것과 같으므로 최종적으로 다음 식이 성립합니다. <img src="https://latex.codecogs.com/png.latex?%5Cboxed%7BP(%5Cmathbf%7Bv%7D)%20=%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20pa_i)%7D"></p>
<p>이 결과는 관측 불가능한 <img src="https://latex.codecogs.com/png.latex?U">를 모르더라도, <strong>오직 부모-자식 간의 관계(Local Information)만으로 전체 시스템의 분포를 설명할 수 있음</strong>을 의미합니다.</p>
<hr>
</section>
</section>
<section id="conditional-independence-d-separation" class="level1">
<h1>5. Conditional Independence &amp; d-separation</h1>
<p>그래프 구조는 변수들 간의 조건부 독립성(Conditional Independence) 정보를 담고 있습니다. [cite_start]이를 파악하기 위해 세 가지 기본 구조(Triplets)를 이해해야 합니다. [cite: 3014-3191]</p>
<section id="the-three-basic-structures-triplets" class="level2">
<h2 class="anchored" data-anchor-id="the-three-basic-structures-triplets">5.1 The Three Basic Structures (Triplets)</h2>
<section id="chain-causal-chain" class="level3">
<h3 class="anchored" data-anchor-id="chain-causal-chain">1. Chain (Causal Chain)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Z%20%5Crightarrow%20Y"></li>
<li><strong>해석</strong>: <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Z">를 유발하고, <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?Y">를 유발합니다. (예: 공부 습관 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 수능 점수 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 대학 합격)</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 종속적입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 영향은 <img src="https://latex.codecogs.com/png.latex?Z">에 의해 차단(Blocked)되므로 <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%5Cmid%20Z"></strong> (독립)입니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-02/images/causal_chain.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Causal Chain 구조. 중간 변수 Z를 관측하면 X와 Y의 정보 흐름이 차단되어 독립이 된다.</figcaption>
</figure>
</div>
</section>
<section id="fork-common-cause" class="level3">
<h3 class="anchored" data-anchor-id="fork-common-cause">2. Fork (Common Cause)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?X%20%5Cleftarrow%20Z%20%5Crightarrow%20Y"></li>
<li><strong>해석</strong>: <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">의 공통 원인입니다. (예: 비 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 교통체증, 비 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 우산 사용)</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: 공통 원인에 의해 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 상관관계를 가집니다(Spurious Correlation).</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: 공통 원인을 통제했으므로 <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%5Cmid%20Z"></strong> (독립)입니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-02/images/common_cause.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Common Cause (Fork) 구조. 공통 원인 Z를 통제하면 X와 Y 사이의 허위 상관관계가 사라진다.</figcaption>
</figure>
</div>
</section>
<section id="collider-common-effect" class="level3">
<h3 class="anchored" data-anchor-id="collider-common-effect">3. Collider (Common Effect)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Z%20%5Cleftarrow%20Y"></li>
<li><strong>해석</strong>: 서로 독립인 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">가 공통 결과 <img src="https://latex.codecogs.com/png.latex?Z">를 유발합니다. (예: 감기 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 결석, 휴일 <img src="https://latex.codecogs.com/png.latex?%5Cto"> 결석)</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 서로 독립입니다. (<img src="https://latex.codecogs.com/png.latex?P(X,Y)%20=%20P(X)P(Y)">)</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cnot%5Cperp%20Y%20%5Cmid%20Z"></strong> (종속)이 됩니다. 이를 <strong>“Explaining Away”</strong> 현상이라고 합니다. 결석(<img src="https://latex.codecogs.com/png.latex?Z=1">)했는데 휴일이 아니라면(<img src="https://latex.codecogs.com/png.latex?Y=0">), 감기일 확률(<img src="https://latex.codecogs.com/png.latex?X=1">)이 높아지기 때문입니다.</li>
<li><strong>주의</strong>: Collider 본인뿐만 아니라 <strong>Collider의 자손(Descendant)</strong>을 관측해도 경로가 열립니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-02/images/common_effect.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Common Effect (Collider) 구조. 두 독립적인 원인이 공통 결과 Z를 조건부로 알게 되었을 때 종속적으로 변하는 Explaining Away 현상을 보여준다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="d-separation-rule" class="level2">
<h2 class="anchored" data-anchor-id="d-separation-rule">5.2 d-separation Rule</h2>
<p>[cite_start]이 세 가지 규칙을 일반화하여 그래프상의 두 노드 <img src="https://latex.codecogs.com/png.latex?X,%20Y">가 조건부 집합 <img src="https://latex.codecogs.com/png.latex?Z">에 대해 독립인지 판별하는 규칙을 <strong>d-separation</strong>이라고 합니다. [cite: 3224-3228]</p>
<ul>
<li>두 노드 사이의 <strong>모든 경로</strong>가 차단(Blocked)되면 d-separated 되었다고 합니다.</li>
<li><strong>경로가 차단되는 조건</strong>:
<ol type="1">
<li>경로 상에 <img src="https://latex.codecogs.com/png.latex?Z">에 포함된 Chain이나 Fork 노드가 있을 때.</li>
<li>경로 상에 Collider가 있으면서, 그 Collider와 자손들이 하나도 <img src="https://latex.codecogs.com/png.latex?Z">에 포함되지 않았을 때.</li>
</ol></li>
</ul>
<hr>
</section>
</section>
<section id="implementation-topological-order" class="level1">
<h1>6. Implementation: Topological Order</h1>
<p>DAG에서 부모가 항상 자식보다 먼저 오도록 노드를 정렬하는 것을 위상 정렬(Topological Order)이라고 합니다. [cite_start]이는 인과 추론 알고리즘 구현의 기초가 됩니다. [cite: 3231-3250]</p>
<p>다음은 Python을 이용한 위상 정렬 구현 예시입니다.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> collections <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> deque</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> topological_order(G):</span>
<span id="cb1-4">    order <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> deque()</span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 모든 노드의 진입 차수(in-degree) 계산 (부모의 수)</span></span>
<span id="cb1-6">    in_degrees <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {V: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(G.Pa(V)) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> V <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> G.Vs}</span>
<span id="cb1-7">    </span>
<span id="cb1-8">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">while</span> in_degrees:</span>
<span id="cb1-9">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 부모가 없는(in-degree=0) 소스 노드 찾기</span></span>
<span id="cb1-10">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> v, v_deg <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> in_degrees.items():</span>
<span id="cb1-11">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> v_deg <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb1-12">                <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb1-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb1-14">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 루프가 break 없이 끝났다면 사이클이 존재한다는 의미 (DAG가 아님)</span></span>
<span id="cb1-15">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">raise</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">ValueError</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cyclic'</span>)</span>
<span id="cb1-16">        </span>
<span id="cb1-17">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 소스 노드를 순서에 추가하고 그래프에서 제거 개념 적용</span></span>
<span id="cb1-18">        order.append(v)</span>
<span id="cb1-19">        <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">del</span> in_degrees[v]</span>
<span id="cb1-20">        </span>
<span id="cb1-21">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 해당 노드의 자식들의 진입 차수 감소</span></span>
<span id="cb1-22">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> G.Ch(v):</span>
<span id="cb1-23">            in_degrees[c] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-24">            </span>
<span id="cb1-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> order</span></code></pre></div></div>
<hr>
</section>
<section id="summary" class="level1">
<h1>7. Summary</h1>
<p>이번 포스트에서는 인과 추론의 기초가 되는 SCM과 인과 그래프에 대해 알아보았습니다.</p>
<ol type="1">
<li><strong>SCM</strong>: 현실의 메커니즘을 변수, 외생 변수, 함수적 관계로 정의하는 모델 ().</li>
<li><strong>Markovian Factorization</strong>: 외생 변수의 독립성을 가정하면, 결합 확률을 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Cprod%20P(v_i%20%7C%20pa_i)">로 분해할 수 있습니다.</li>
<li><strong>d-separation</strong>: Chain, Fork, Collider 구조를 통해 변수 간의 조건부 독립성을 파악할 수 있으며, 특히 Collider가 관측될 때 경로가 열린다는 점에 주의해야 합니다.</li>
</ol>
<hr>
<section id="checklist-for-content-verification" class="level3">
<h3 class="anchored" data-anchor-id="checklist-for-content-verification">Checklist for Content Verification</h3>
<ul class="task-list">
<li><label><input type="checkbox" checked=""><strong>Motivation</strong>: Simpson’s Paradox 예시와 Data vs Reality의 차이 설명 포함.</label></li>
<li><label><input type="checkbox" checked=""><strong>SCM Definition</strong>: 4-tuple 정의 및 구성 요소 설명 포함.</label></li>
<li><label><input type="checkbox" checked=""><strong>Mathematical Derivation</strong>: Markovian Factorization의 단계별 유도 과정 LaTeX 수식으로 포함.</label></li>
<li><label><input type="checkbox" checked=""><strong>Basic Structures</strong>: Chain, Fork, Collider의 구조 및 독립성 조건 설명 포함.</label></li>
<li><label><input type="checkbox" checked=""><strong>d-separation</strong>: 경로 차단 조건 및 규칙 설명 포함.</label></li>
<li><label><input type="checkbox" checked=""><strong>Algorithm</strong>: 위상 정렬 알고리즘 및 Python 코드 포함.</label></li>
</ul>
<pre><code></code></pre>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L02/part-02/</guid>
  <pubDate>Thu, 22 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 02. Causal Models and Graphs (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/lecture/L02/part-01/</link>
  <description><![CDATA[ 





<section id="introduction-from-statistics-to-causality" class="level1">
<h1>1. Introduction: From Statistics to Causality</h1>
<p>전통적인 통계학이나 머신러닝의 추론(Inference) 패러다임은 데이터의 <strong>결합 확률 분포(Joint Distribution)</strong> <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)">를 찾아내는 것에 집중합니다. 예를 들어, “상품 A를 산 고객이 상품 B도 살 확률은 얼마인가?”(<img src="https://latex.codecogs.com/png.latex?P(B%7CA)">)와 같은 질문은 관측된 데이터의 패턴(Association)만으로 충분히 답할 수 있습니다.</p>
<p>하지만 현실의 문제 해결은 종종 <strong>“만약 우리가 X를 변화시킨다면, Y는 어떻게 변할까?”</strong>라는 질문을 던집니다. * 가격을 두 배로 올리면 판매량은 어떻게 될까? * 흡연을 금지하면 암 발병률은 낮아질까?</p>
<p>이러한 질문은 데이터 자체(<img src="https://latex.codecogs.com/png.latex?P">)가 아니라 데이터가 생성되는 <strong>현실의 메커니즘(Reality)</strong>에 대한 이해를 요구합니다. 이번 포스트에서는 인과 추론의 핵심 언어인 <strong>구조적 인과 모델(Structural Causal Model, SCM)</strong>과 이를 시각화한 <strong>인과 그래프(Causal Graph)</strong>에 대해 다룹니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/stats_vs_causal_paradigm.png" class="img-fluid figure-img"></p>
<figcaption>Figure: 통계적 추론과 인과적 추론의 차이. 통계적 추론은 데이터 <img src="https://latex.codecogs.com/png.latex?P"> 내에서의 성질 <img src="https://latex.codecogs.com/png.latex?Q(P)">를 찾지만, 인과 추론은 데이터 생성 모델 <img src="https://latex.codecogs.com/png.latex?M">을 통해 현실의 메커니즘을 이해하고, 개입 후의 분포 <img src="https://latex.codecogs.com/png.latex?P'">를 추정하려 한다.</figcaption>
</figure>
</div>
<section id="motivation-simpsons-paradox-example" class="level2">
<h2 class="anchored" data-anchor-id="motivation-simpsons-paradox-example">1.1 Motivation: Simpson’s Paradox Example</h2>
<p>왜 데이터만으로는 충분하지 않을까요? 강의 자료에 제시된 ‘약물 투여와 생존율’ 예시를 봅시다.</p>
<ul>
<li><strong>상황</strong>: 특정 도시에 전염병이 돌고 있고, 치료제(Drug)가 있습니다.</li>
<li><strong>숨겨진 진실(Reality)</strong>:
<ol type="1">
<li>부유층(Rich)은 약물 복용 여부와 상관없이 생존합니다 (좋은 생활 환경).</li>
<li>빈곤층(Poor)은 약물을 복용하면 알레르기 반응으로 사망하고, 복용하지 않으면 생존합니다(자연 면역).</li>
<li>의사들은 부유층에게만 주로 약을 처방합니다(비용 문제).</li>
</ol></li>
</ul>
<p>이 경우 데이터만 보면 “약물을 복용한 집단(대부분 부유층)”의 생존율이 높게 나타납니다. 알고리즘은 “약을 먹어라”라고 추천할 것입니다. 하지만 실제 메커니즘(빈곤층에게는 치명적)을 안다면 빈곤층에게 약을 주면 안 된다는 정반대의 결론에 도달해야 합니다. 즉, <strong>데이터 생성 과정(Data Generating Process)</strong>을 모델링하지 않으면 잘못된 의사결정을 내리게 됩니다.</p>
<hr>
</section>
</section>
<section id="structural-causal-model-scm" class="level1">
<h1>2. Structural Causal Model (SCM)</h1>
<p>인과 관계를 수학적으로 엄밀하게 정의하기 위해 <strong>구조적 인과 모델(SCM)</strong>을 도입합니다.</p>
<section id="definition" class="level2">
<h2 class="anchored" data-anchor-id="definition">2.1 Definition</h2>
<p>SCM <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">은 다음 4가지 요소의 튜플 <img src="https://latex.codecogs.com/png.latex?%5Clangle%20V,%20U,%20F,%20P(U)%20%5Crangle">로 정의됩니다.</p>
<ol type="1">
<li><strong><img src="https://latex.codecogs.com/png.latex?V%20=%20%5C%7BV_1,%20...,%20V_n%5C%7D"></strong>: <strong>내생 변수(Endogenous variables)</strong>. 우리가 관측할 수 있는 변수들입니다. (예: 흡연 여부, 폐암 발병 여부)</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?U%20=%20%5C%7BU_1,%20...,%20U_m%5C%7D"></strong>: <strong>외생 변수(Exogenous variables)</strong>. 모델 내부의 다른 변수에 의해 설명되지 않는, 시스템 외부에서 결정되는 변수들입니다. (예: 유전적 요인, 미관측 환경 요인)</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?F%20=%20%5C%7Bf_1,%20...,%20f_n%5C%7D"></strong>: <strong>구조적 함수(Structural functions)</strong>. 각 내생 변수 <img src="https://latex.codecogs.com/png.latex?V_i">가 어떻게 결정되는지를 나타내는 함수입니다. <img src="https://latex.codecogs.com/png.latex?v_i%20%5Cleftarrow%20f_i(pa_i,%20u_i)"> 여기서 <img src="https://latex.codecogs.com/png.latex?pa_i%20%5Csubseteq%20V%20%5Csetminus%20%5C%7BV_i%5C%7D">는 <img src="https://latex.codecogs.com/png.latex?V_i">의 <strong>부모(Parents)</strong> 변수 집합이고, <img src="https://latex.codecogs.com/png.latex?u_i%20%5Csubseteq%20U">는 관련된 외생 변수입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?P(U)"></strong>: 외생 변수 <img src="https://latex.codecogs.com/png.latex?U">에 대한 확률 분포입니다.</li>
</ol>
<blockquote class="blockquote">
<p><strong>Key Idea</strong>: SCM에서 자연(Nature)은 결정론적(Deterministic) 함수 <img src="https://latex.codecogs.com/png.latex?F">와 확률적(Probabilistic) 노이즈 <img src="https://latex.codecogs.com/png.latex?P(U)">의 결합으로 세상을 정의합니다.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/scm_conceptual_diagram.png" class="img-fluid figure-img"></p>
<figcaption>Figure: SCM의 개념적 도식. 외생 변수 U가 확률 분포 P(U)를 따르고, 함수 F를 통해 내생 변수 V의 값을 결정하여 관측 데이터 분포 P(V)를 유도한다.</figcaption>
</figure>
</div>
</section>
<section id="scm-induces-a-distribution-pv" class="level2">
<h2 class="anchored" data-anchor-id="scm-induces-a-distribution-pv">2.2 SCM Induces a Distribution <img src="https://latex.codecogs.com/png.latex?P(V)"></h2>
<p>SCM은 단순히 변수 간의 관계만 정의하는 것이 아니라, 관측 가능한 변수 <img src="https://latex.codecogs.com/png.latex?V">의 결합 확률 분포 <img src="https://latex.codecogs.com/png.latex?P(V)">를 유도(Induce)합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Csum_%7B%5Cmathbf%7Bu%7D%20:%20Y(%5Cmathbf%7Bu%7D)%20=%20%5Cmathbf%7Bv%7D%7D%20P(%5Cmathbf%7Bu%7D)"></p>
<p>즉, 우리가 관측하는 데이터의 분포는 <strong>외생 변수의 불확실성(<img src="https://latex.codecogs.com/png.latex?P(U)">)이 함수 <img src="https://latex.codecogs.com/png.latex?F">를 통과하여 내생 변수 <img src="https://latex.codecogs.com/png.latex?V">로 전파된 결과</strong>입니다.</p>
<hr>
</section>
</section>
<section id="causal-diagrams-graphical-models" class="level1">
<h1>3. Causal Diagrams (Graphical Models)</h1>
<p>SCM은 수식으로 정의되지만, 이를 직관적으로 이해하고 분석하기 위해 <strong>유향 비순환 그래프(DAG, Directed Acyclic Graph)</strong> 형태인 인과 그래프로 표현할 수 있습니다.</p>
<section id="construction-rules" class="level2">
<h2 class="anchored" data-anchor-id="construction-rules">3.1 Construction Rules</h2>
<p>SCM <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D">으로부터 인과 그래프 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BG%7D">를 그리는 규칙은 다음과 같습니다.</p>
<ol type="1">
<li><strong>Nodes</strong>: 각 내생 변수 <img src="https://latex.codecogs.com/png.latex?V_i">를 노드(Vertex)로 그립니다.</li>
<li><strong>Directed Edges (<img src="https://latex.codecogs.com/png.latex?%5Crightarrow">)</strong>: 함수 <img src="https://latex.codecogs.com/png.latex?f_i">에서 <img src="https://latex.codecogs.com/png.latex?V_j">가 <img src="https://latex.codecogs.com/png.latex?V_i">의 입력(<img src="https://latex.codecogs.com/png.latex?pa_i">)으로 사용된다면, <img src="https://latex.codecogs.com/png.latex?V_j%20%5Cto%20V_i"> 화살표를 그립니다.</li>
<li><strong>Bidirected Edges (<img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow">)</strong>: 두 변수 <img src="https://latex.codecogs.com/png.latex?V_i,%20V_j">에 영향을 주는 외생 변수 <img src="https://latex.codecogs.com/png.latex?U_i,%20U_j">가 서로 상관관계가 있거나(Correlated), 같은 외생 변수를 공유한다면 점선 양방향 화살표로 연결합니다. 이는 <strong>미관측 교란 요인(Unobserved Confounder)</strong>의 존재를 의미합니다.</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/scm_to_dag_example.png" class="img-fluid figure-img"></p>
<figcaption>Figure: SCM에서 인과 그래프로의 변환 예시. (좌) 수식으로 표현된 SCM, (우) 이에 대응하는 DAG. 외생 변수 U는 보통 그래프에서 생략되거나 점선으로 표현된다.</figcaption>
</figure>
</div>
</section>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">3.2 Terminology</h2>
<ul>
<li><strong>Parents (<img src="https://latex.codecogs.com/png.latex?Pa_i">)</strong>: <img src="https://latex.codecogs.com/png.latex?V_i">로 직접 화살표를 보내는 변수들.</li>
<li><strong>Children (<img src="https://latex.codecogs.com/png.latex?Ch_i">)</strong>: <img src="https://latex.codecogs.com/png.latex?V_i">로부터 직접 화살표를 받는 변수들.</li>
<li><strong>Ancestors / Descendants</strong>: 화살표를 따라 거슬러 올라가거나 내려갈 수 있는 변수들.</li>
</ul>
<hr>
</section>
</section>
<section id="markovian-factorization-detailed-derivation" class="level1">
<h1>4. Markovian Factorization (Detailed Derivation)</h1>
<p>이 포스트의 핵심 파트입니다. SCM과 그래프 구조를 이용하여 복잡한 결합 확률 분포 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)">를 어떻게 간단한 조건부 확률들의 곱으로 분해할 수 있는지 증명합니다.</p>
<section id="the-markovian-condition" class="level2">
<h2 class="anchored" data-anchor-id="the-markovian-condition">4.1 The Markovian Condition</h2>
<p>만약 외생 변수들 <img src="https://latex.codecogs.com/png.latex?U">가 서로 <strong>독립(Jointly Independent)</strong>이라면, 즉 그래프 상에 양방향 엣지(<img src="https://latex.codecogs.com/png.latex?%5Cleftrightarrow">)가 하나도 없다면, 이 모델을 <strong>Markovian</strong>이라고 부릅니다.</p>
</section>
<section id="derivation-of-bayesian-factorization" class="level2">
<h2 class="anchored" data-anchor-id="derivation-of-bayesian-factorization">4.2 Derivation of Bayesian Factorization</h2>
<p>우리의 목표는 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)">를 <img src="https://latex.codecogs.com/png.latex?%5Cprod%20P(v_i%20%7C%20pa_i)"> 형태로 만드는 것입니다. 이를 <strong>Bayesian Factorization</strong>이라 합니다.</p>
<p><strong>Step 1: Law of Total Probability</strong> 모든 변수 <img src="https://latex.codecogs.com/png.latex?V">의 결합 확률은 외생 변수 <img src="https://latex.codecogs.com/png.latex?U">를 포함한 결합 확률에서 <img src="https://latex.codecogs.com/png.latex?U">를 합(Summing out)하여 얻을 수 있습니다. <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20P(%5Cmathbf%7Bv%7D,%20%5Cmathbf%7Bu%7D)"></p>
<p>SCM에서 <img src="https://latex.codecogs.com/png.latex?V">는 <img src="https://latex.codecogs.com/png.latex?Pa">와 <img src="https://latex.codecogs.com/png.latex?U">에 의해 결정되므로(<img src="https://latex.codecogs.com/png.latex?v_i%20=%20f_i(pa_i,%20u_i)">), <img src="https://latex.codecogs.com/png.latex?P(v_i%20%7C%20pa_i,%20u_i)">는 결정론적입니다(0 또는 1). 이를 이용하여 식을 전개하면: <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20P(%5Cmathbf%7Bu%7D)%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i,%20%5Cmathbf%7Bu%7D_i)"></p>
<p><strong>Step 2: Independence of Exogenous Variables</strong> Markovian 가정에 의해 <img src="https://latex.codecogs.com/png.latex?U">들이 서로 독립이므로, <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bu%7D)%20=%20%5Cprod%20P(%5Cmathbf%7Bu%7D_i)">가 성립합니다. 이를 대입합니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cleft(%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(%5Cmathbf%7Bu%7D_i)%20%5Cright)%20%5Cleft(%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i,%20%5Cmathbf%7Bu%7D_i)%20%5Cright)"></p>
<p><strong>Step 3: Independence of <img src="https://latex.codecogs.com/png.latex?U_i"> and <img src="https://latex.codecogs.com/png.latex?Pa_i"></strong> 외생 변수 <img src="https://latex.codecogs.com/png.latex?U_i">는 시스템 외부에서 결정되므로, 내생 변수인 부모 <img src="https://latex.codecogs.com/png.latex?Pa_i">와는 독립입니다. 따라서 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bu%7D_i)%20=%20P(%5Cmathbf%7Bu%7D_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)">로 쓸 수 있습니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i,%20%5Cmathbf%7Bu%7D_i)%20P(%5Cmathbf%7Bu%7D_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)"></p>
<p><strong>Step 4: Merging Conditional Probabilities</strong> 곱셈 법칙 <img src="https://latex.codecogs.com/png.latex?P(A%7CB)P(B)%20=%20P(A,B)">를 적용하여 항을 합칩니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Csum_%7B%5Cmathbf%7Bu%7D%7D%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i,%20%5Cmathbf%7Bu%7D_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)"></p>
<p><strong>Step 5: Rearranging Sum and Product</strong> 전체 외생 변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D">에 대한 합(<img src="https://latex.codecogs.com/png.latex?%5Csum_%7B%5Cmathbf%7Bu%7D%7D">)을 개별 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D_i">에 대한 합으로 분리하여 곱셈 기호 안으로 넣습니다. 각 항은 해당되는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D_i">에만 의존하기 때문에 가능합니다. <img src="https://latex.codecogs.com/png.latex?=%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20%5Cleft(%20%5Csum_%7B%5Cmathbf%7Bu%7D_i%7D%20P(v_i,%20%5Cmathbf%7Bu%7D_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)%20%5Cright)"></p>
<p><strong>Step 6: Marginalization</strong> 괄호 안의 식 <img src="https://latex.codecogs.com/png.latex?%5Csum_%7B%5Cmathbf%7Bu%7D_i%7D%20P(v_i,%20%5Cmathbf%7Bu%7D_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)">는 결합 확률에서 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D_i">를 덜어내는(Marginalize) 과정이므로 <img src="https://latex.codecogs.com/png.latex?P(v_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)">가 됩니다.</p>
<p><strong>Final Result:</strong> <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Cprod_%7BV_i%20%5Cin%20V%7D%20P(v_i%20%5Cmid%20%5Cmathbf%7Bpa%7D_i)"></p>
<p>이 결과는 매우 강력합니다. 우리가 관측할 수 없는 <img src="https://latex.codecogs.com/png.latex?U">를 모르더라도, <strong>오직 관측 가능한 데이터 내에서 부모-자식 간의 조건부 확률만 알면 전체 분포를 알 수 있다</strong>는 것을 의미하기 때문입니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/markovian_factorization_derivation.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Markovian Factorization 유도 과정 요약. 독립성 가정과 확률의 연쇄 법칙을 통해 복잡한 결합 확률이 조건부 확률의 곱으로 분해됨을 보여준다.</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="conditional-independence-d-separation" class="level1">
<h1>5. Conditional Independence &amp; d-separation</h1>
<p>그래프 구조는 변수들 사이의 조건부 독립(Conditional Independence, CI) 정보를 담고 있습니다. 이를 파악하기 위해 3가지 기본 구조(Triplets)를 이해해야 합니다.</p>
<section id="the-three-basic-structures-triplets" class="level2">
<h2 class="anchored" data-anchor-id="the-three-basic-structures-triplets">5.1 The Three Basic Structures (Triplets)</h2>
<section id="chain-x-rightarrow-z-rightarrow-y" class="level3">
<h3 class="anchored" data-anchor-id="chain-x-rightarrow-z-rightarrow-y">1. Chain (<img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Z%20%5Crightarrow%20Y">)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Z">에 영향을 주고, <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 줍니다.</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 종속입니다 (정보가 흐름).</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: <img src="https://latex.codecogs.com/png.latex?X">가 <img src="https://latex.codecogs.com/png.latex?Y">에 미치는 영향은 이미 <img src="https://latex.codecogs.com/png.latex?Z">에 의해 설명되었으므로, <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%5Cmid%20Z"> (독립)</strong>입니다. <img src="https://latex.codecogs.com/png.latex?Z">가 정보를 차단(Block)합니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/causal_chain.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Causal Chain 구조와 독립성. 중간 매개변수 Z를 조건부로 알게 되면 X와 Y 사이의 정보 흐름이 차단되어 독립이 된다.</figcaption>
</figure>
</div>
</section>
<section id="fork-common-cause-x-leftarrow-z-rightarrow-y" class="level3">
<h3 class="anchored" data-anchor-id="fork-common-cause-x-leftarrow-z-rightarrow-y">2. Fork / Common Cause (<img src="https://latex.codecogs.com/png.latex?X%20%5Cleftarrow%20Z%20%5Crightarrow%20Y">)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">의 공통 원인입니다. (예: <img src="https://latex.codecogs.com/png.latex?Z">=날씨, <img src="https://latex.codecogs.com/png.latex?X">=교통체증, <img src="https://latex.codecogs.com/png.latex?Y">=우산사용)</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 종속입니다 (상관관계 발생).</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: 공통 원인을 통제했으므로 <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cperp%20Y%20%5Cmid%20Z"> (독립)</strong>입니다.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/common_cause.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Common Cause 구조와 독립성. 공통 원인 Z를 통제하면 X와 Y 사이의 허위 상관관계(Spurious Correlation)가 사라져 독립이 된다.</figcaption>
</figure>
</div>
</section>
<section id="collider-common-effect-x-rightarrow-z-leftarrow-y" class="level3">
<h3 class="anchored" data-anchor-id="collider-common-effect-x-rightarrow-z-leftarrow-y">3. Collider / Common Effect (<img src="https://latex.codecogs.com/png.latex?X%20%5Crightarrow%20Z%20%5Cleftarrow%20Y">)</h3>
<ul>
<li><strong>구조</strong>: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">가 동시에 <img src="https://latex.codecogs.com/png.latex?Z">에 영향을 줍니다. (예: <img src="https://latex.codecogs.com/png.latex?X">=감기, <img src="https://latex.codecogs.com/png.latex?Y">=휴일, <img src="https://latex.codecogs.com/png.latex?Z">=결석)</li>
<li><strong>독립성</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">를 모를 때: <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 <strong>독립</strong>입니다 (서로 관계없는 사건).</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z">를 알 때 (Given <img src="https://latex.codecogs.com/png.latex?Z">)</strong>: <strong><img src="https://latex.codecogs.com/png.latex?X%20%5Cnot%5Cperp%20Y%20%5Cmid%20Z"> (종속)</strong>이 됩니다.</li>
</ul></li>
<li><strong>Explaining Away</strong>: 결석(<img src="https://latex.codecogs.com/png.latex?Z=1">)했다는 사실을 알 때, 휴일이 아니라면(<img src="https://latex.codecogs.com/png.latex?Y=0">), 아플 확률(<img src="https://latex.codecogs.com/png.latex?X=1">)이 높아집니다. 즉, 결과를 알면 원인들 사이에 상관관계가 생깁니다. <strong>Collider는 관측될 때 경로를 엽니다(Open).</strong></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/collider_structure.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Common Effect (Collider) 구조와 Explaining Away 현상. 두 독립적인 원인이 공통 결과 Z를 조건부로 알게 되었을 때 종속적으로 변하는 현상을 설명한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="d-separation" class="level2">
<h2 class="anchored" data-anchor-id="d-separation">5.2 d-separation</h2>
<p>이 세 가지 규칙을 일반화한 것이 <strong>d-separation</strong>입니다. 그래프 상의 두 노드 <img src="https://latex.codecogs.com/png.latex?X,%20Y"> 사이의 모든 경로가 관측된 변수 집합 <img src="https://latex.codecogs.com/png.latex?Z">에 의해 차단(Blocked)된다면, <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Y">는 <img src="https://latex.codecogs.com/png.latex?Z">에 대해 조건부 독립입니다.</p>
<ul>
<li>경로가 차단되는 경우:
<ol type="1">
<li>경로 상의 Chain이나 Fork 노드가 <img src="https://latex.codecogs.com/png.latex?Z">에 포함될 때.</li>
<li>경로 상의 <strong>Collider 노드와 그 자손들이 <img src="https://latex.codecogs.com/png.latex?Z">에 포함되지 않을 때</strong>.</li>
</ol></li>
</ul>
</section>
<section id="food-for-thought-quiz" class="level2">
<h2 class="anchored" data-anchor-id="food-for-thought-quiz">5.3 Food for Thought (Quiz)</h2>
<p>아래 그래프를 보고 독립성을 판별해 봅시다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/lecture/L02/part-01/images/food_for_thought_graph.png" class="img-fluid figure-img"></p>
<figcaption>Figure: d-separation 연습을 위한 복합 그래프 예시. A, B, C, D 노드와 실선/점선 엣지가 섞여 있어 다양한 경로의 독립성을 테스트한다.</figcaption>
</figure>
</div>
<ol type="1">
<li><strong>Is <img src="https://latex.codecogs.com/png.latex?A%20%5Cperp%20D">? (No)</strong>
<ul>
<li>경로: <img src="https://latex.codecogs.com/png.latex?A%20%5Cleftrightarrow%20B%20%5Crightarrow%20D">.</li>
<li><img src="https://latex.codecogs.com/png.latex?B">는 Chain/Fork 역할을 하지만 관측되지 않았으므로 경로가 열려 있습니다.</li>
</ul></li>
<li><strong>Is <img src="https://latex.codecogs.com/png.latex?A%20%5Cperp%20C">? (Yes)</strong>
<ul>
<li>경로: <img src="https://latex.codecogs.com/png.latex?A%20%5Cleftrightarrow%20B%20%5Cleftarrow%20C">.</li>
<li><img src="https://latex.codecogs.com/png.latex?B">는 Collider입니다. 관측되지 않았으므로 경로는 <strong>차단</strong>되어 있습니다.</li>
</ul></li>
<li><strong>Is <img src="https://latex.codecogs.com/png.latex?A%20%5Cperp%20C%20%5Cmid%20D">? (No)</strong>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?D">는 Collider <img src="https://latex.codecogs.com/png.latex?B">의 자손(Descendant)입니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?D">를 관측하면 <img src="https://latex.codecogs.com/png.latex?B">가 열리게 되어 경로가 연결됩니다.</li>
</ul></li>
<li><strong>Is <img src="https://latex.codecogs.com/png.latex?D%20%5Cperp%20C%20%5Cmid%20B">? (No)</strong>
<ul>
<li>경로 1: <img src="https://latex.codecogs.com/png.latex?C%20%5Crightarrow%20B%20%5Crightarrow%20D">. <img src="https://latex.codecogs.com/png.latex?B">를 알면 차단됩니다.</li>
<li>경로 2: <img src="https://latex.codecogs.com/png.latex?C%20%5Cleftrightarrow%20D"> (Backdoor path, <img src="https://latex.codecogs.com/png.latex?C%20%5Cleftarrow%20U%20%5Crightarrow%20D">). 이 경로는 <img src="https://latex.codecogs.com/png.latex?B">와 무관하게 열려 있습니다.</li>
<li>하나라도 열린 경로가 있으므로 종속입니다.</li>
</ul></li>
</ol>
<hr>
</section>
</section>
<section id="summary" class="level1">
<h1>6. Summary</h1>
<p>이번 포스트에서는 인과 추론의 기초가 되는 SCM과 인과 그래프에 대해 알아보았습니다.</p>
<ol type="1">
<li><strong>SCM</strong>: 현실의 메커니즘을 변수, 외생 변수, 함수적 관계로 정의하는 모델입니다.</li>
<li><strong>Markovian Factorization</strong>: 외생 변수의 독립성을 가정하면, 결합 확률 분포를 <img src="https://latex.codecogs.com/png.latex?P(%5Cmathbf%7Bv%7D)%20=%20%5Cprod%20P(v_i%20%7C%20pa_i)">로 분해할 수 있습니다.</li>
<li><strong>d-separation</strong>: 그래프 구조(Chain, Fork, Collider)를 통해 변수 간의 조건부 독립성을 파악할 수 있으며, 특히 Collider가 관측될 때 경로가 열린다는 점(Explaining Away)이 중요합니다.</li>
</ol>
<p>다음 시간에는 이 구조 위에서 실제로 <strong>개입(Intervention)</strong>을 했을 때 어떤 일이 벌어지는지(Pearl’s Causal Hierarchy의 2단계)에 대해 다루겠습니다.</p>
<hr>
<section id="checklist-for-content-verification" class="level3">
<h3 class="anchored" data-anchor-id="checklist-for-content-verification"><strong>Checklist for Content Verification</strong></h3>
<ul class="task-list">
<li><label><input type="checkbox" checked=""><strong>Motivation</strong>: Simpson’s Paradox (Drug example) included? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>SCM Definition</strong>: 4-tuple and definition of components included? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>Induced Distribution</strong>: How SCM induces <img src="https://latex.codecogs.com/png.latex?P(v)"> described? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>Causal Diagrams</strong>: Construction rules (Nodes, Edges) included? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>Markovian Factorization</strong>: Detailed mathematical derivation included? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>Conditional Independence</strong>: 3 Basic structures (Chain, Fork, Collider) explained? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>d-separation</strong>: General rule and specific quiz (Food for thought) analysis included? (Yes)</label></li>
<li><label><input type="checkbox" checked=""><strong>Algebra Review</strong>: Sum of products logic implicitly handled in the derivation section? (Yes)</label></li>
</ul>
<p>```</p>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/lecture/L02/part-01/</guid>
  <pubDate>Thu, 22 Jan 2026 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
