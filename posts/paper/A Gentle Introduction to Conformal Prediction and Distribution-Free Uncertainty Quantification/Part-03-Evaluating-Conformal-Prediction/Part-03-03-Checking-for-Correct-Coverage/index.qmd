---
title: "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 3.3)"
subtitle: "3.3. Checking for Correct Coverage"
author: "유성현"
date: "2026-01-16"
categories: [Paper Review]
format:
  html:
    toc: true
    number-sections: false
    code-fold: show
---

# Introduction

Conformal Prediction(CP)을 구현했다면, 가장 먼저 해야 할 일은 **"이게 정말 작동하는가?"**를 확인하는 것입니다.
즉, 우리가 설정한 목표 커버리지(예: 90%)가 실제 테스트 데이터에서도 지켜지는지 검증해야 합니다.

하지만 단순히 한 번의 테스트 셋 결과만 보고 "90.1%니까 성공!"이라고 단정 짓기는 어렵습니다.
데이터의 무작위성 때문에 우연히 잘 나왔을 수도, 우연히 못 나왔을 수도 있기 때문입니다.
따라서 우리는 **여러 번의 실험(Trials)**을 통해 커버리지 분포를 확인해야 합니다.

# Methodology: Repeated Experiments

가장 확실한 검증 방법은 $R$번의 독립적인 실험을 수행하는 것입니다.
각 실험 $j=1, \dots, R$마다 새로운 Calibration Set과 Validation Set을 준비하고, 다음 과정을 반복합니다:

1.  Calibration 수행 $\rightarrow$ $\hat{q}_j$ 계산
2.  Validation Set에 대해 예측 집합 구성 $\mathcal{C}_j$
3.  **경험적 커버리지(Empirical Coverage)** $C_j$ 계산:

$$
C_j = \frac{1}{n_{val}} \sum_{i=1}^{n_{val}} \mathbb{I} \{ Y_{i,j}^{(val)} \in \mathcal{C}_j(X_{i,j}^{(val)}) \}
$$

이렇게 얻은 $R$개의 커버리지 값들 $C_1, \dots, C_R$의 평균 $\overline{C}$는 이론적으로 $1-\alpha$에 매우 근접해야 합니다.

$$
\overline{C} = \frac{1}{R} \sum_{j=1}^{R} C_j \approx 1 - \alpha
$$

또한, $C_j$들의 히스토그램을 그렸을 때 $1-\alpha$를 중심으로 종 모양(Bell-curve) 분포를 보여야 합니다.

# The Practical Challenge: Limited Data

현실적인 문제는 **"매번 새로운 데이터를 어디서 구하는가?"**입니다.
우리가 가진 데이터는 유한(총 $n_{total} = n + n_{val}$)하므로, $R$번이나 새로운 데이터를 수집할 수는 없습니다.

따라서 우리는 **Resampling (Random Split)** 방식을 사용합니다.
전체 데이터를 무작위로 섞어서 Calibration/Validation 셋으로 나누는 과정을 $R$번 반복하는 것입니다.

## Efficiency Trick: Score Caching

하지만 $R$번(예: 100번)이나 모델을 다시 학습시키거나 추론(Inference)을 돌리는 것은 계산 비용이 매우 큽니다.
여기서 중요한 팁은 **Conformal Score를 미리 계산해두는 것(Caching)**입니다.

CP 알고리즘은 **Score값($s_i$)들의 순위**에만 의존합니다. 데이터가 어느 셋(Calibration vs Validation)에 속하느냐에 따라 역할만 달라질 뿐, 각 데이터 포인트의 Score 값 자체는 변하지 않습니다.

따라서 다음과 같이 효율적으로 검증할 수 있습니다:

1.  **Pre-computation**: 전체 데이터에 대해 Score를 미리 한 번만 계산합니다.
2.  **Shuffle & Split**: 계산된 Score 배열만 무작위로 섞어서 나눕니다.
3.  **Evaluate**: 나누어진 Score들로 Quantile을 구하고 커버리지를 계산합니다.

이 방식을 사용하면 딥러닝 모델을 매번 돌릴 필요가 없어 검증 속도가 수백 배 빨라집니다.

# Implementation

Python 코드로 이를 구현하면 다음과 같습니다.

![Figure 12: Score Caching을 이용한 효율적인 커버리지 검증 코드. 전체 Score를 미리 계산해두고(`get_scores`), 반복문 안에서는 단순히 배열을 섞고(`shuffle`) 자르는 연산만 수행한다.](images/figure12_coverage_check_code.png)

### Code Explanation
1.  **Load/Compute Scores**: `get_scores(X, Y)`를 통해 모든 데이터의 Score를 계산하고 저장합니다.
2.  **Loop $R$ times**:
    * `np.random.shuffle(scores)`: Score들을 섞습니다.
    * `calib, val = scores[:n], scores[n:]`: $n$개는 Calibration용, 나머지는 검증용으로 나눕니다.
    * `qhat`: Calibration Score들로 Quantile을 계산합니다.
    * `mean()`: Validation Score들이 `qhat`보다 작은 비율(Coverage)을 계산합니다.
3.  **Check**: `coverages.mean()`이 $1-\alpha$와 비슷한지 확인하고, 히스토그램을 그립니다.

# Interpretation of Results

검증 결과 커버리지가 정확히 90.00%가 나오지 않더라도 당황할 필요는 없습니다.
$n$ (Calibration 크기), $n_{val}$ (Validation 크기), $R$ (반복 횟수)이 모두 유한하기 때문에 **약간의 변동(Benign Fluctuations)**은 자연스러운 현상입니다.

* **정상**: 평균이 0.89 ~ 0.91 사이이며 히스토그램이 0.9 근처에 모여 있음.
* **비정상**: 평균이 0.80처럼 현저히 낮거나, 히스토그램이 한쪽으로 크게 치우침 $\rightarrow$ 구현 오류(버그) 혹은 데이터 분포의 문제(i.i.d. 위반 등)를 의심해야 합니다.

이 진단 과정을 통과했다면, 여러분의 Conformal Predictor는 통계적으로 신뢰할 수 있는 상태입니다.

---
**Next Step**: 지금까지는 표준적인 환경에서의 CP를 다루었습니다. 다음 포스트부터는 **Section 4. Extensions of Conformal Prediction**으로 넘어가서, 데이터 불균형, 시계열, 분포 변화 등 더 복잡하고 현실적인 문제들을 해결하는 방법을 알아보겠습니다. 첫 번째로 **Group-Balanced Conformal Prediction**을 다룹니다.