---
title: "[Causal Inference] 08. Partial Identification (Part 1)"
description: "Partial Identification Problem: Natural Bounds"
author: "유성현"
date: "2026-01-11"
categories: [Causal Inference]
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
    latex: true
---

# 개요 (Overview)

* 이전까지 우리는 인과 효과 $P(y|do(x))$를 관측 데이터 $P(v)$로부터 정확한 하나의 값(Point estimate)으로 계산할 수 있는지, 즉 **식별 가능성(Identifiability)**을 따졌습니다.

* 하지만 현실의 많은 문제에서는 그래프 구조상 식별이 불가능한 경우(Non-identifiable)가 많습니다. 

* 이때 우리는 포기하는 대신 **"그렇다면 인과 효과가 존재할 수 있는 범위(Bound)라도 알 수 없을까?"**라는 질문을 던지게 됩니다. 

* 이것이 바로 **부분 식별(Partial Identification)** 문제입니다.

![Figure 1: Identification Process Flowchart. 식별 엔진이 'No'를 반환했을 때, 구간 [a, b]를 찾는 과정으로 넘어가는 흐름도.](./images/partial_id_flowchart.png)

* Note: 식별 불가능(No) 판정이 났을 때, 우리는 0과 1 사이의 어딘가에 존재하는 구간 $[a(y;x), b(y;x)]$를 찾게 됩니다. *

---

# 부분 식별 문제 (The Partial Identification Problem)

* 인과 효과가 부분 식별 가능하다(Partially-identifiable)는 것은 관측 분포 $P(V)$를 통해 타겟 $P(y|do(x))$가 특정 구간 $[a, b]$ 안에 있음을 알 수 있고, 이 구간이 $[0, 1]$보다 더 좁은 정보($[a, b] \subset [0, 1]$)를 제공할 때를 말합니다.

  * **Point Identification:** 구간의 너비가 0인 특수한 경우 ($a=b$).

  * **Partial Identification:** 구간의 너비가 0보다는 크지만, 아무 정보가 없는 $[0, 1]$보다는 좁은 경우.

---

# Natural Bounds

* 가장 기본적이면서도 중요한 **Natural Bounds**를 살펴보겠습니다. 

* 이는 아무런 추가 가정 없이, 관측 데이터 $P(x, y)$만 주어졌을 때 $P(y|do(x))$가 가질 수 있는 최악과 최선의 범위를 의미합니다.

## 문제 설정

다음과 같이 $X$와 $Y$ 사이에 비관측 교란 변수 $U$가 존재하는 가장 일반적인 그래프를 가정해 봅시다.

![Figure 2: Basic Confounded Graph. X와 Y가 U에 의해 교란된 구조.](./images/natural_bound_graph.png)

* 우리의 목표는 $P(x, y)$가 주어졌을 때, $P(y|do(x))$의 범위를 구하는 것입니다.

## Theorem

* 관측 분포 $P(x, y)$가 주어졌을 때, 인과 효과 $P(y|do(x))$는 다음 구간 안에 존재합니다.

$$
P(y, x) \le P(y|do(x)) \le P(y, x) + 1 - P(x)
$$

* **하한 (Lower Bound):** $a(y;x) = P(y, x)$
* **상한 (Upper Bound):** $b(y;x) = P(y, x) + 1 - P(x)$

  * **직관적 해석**
    * 이 수식은 데이터를 통해 **"확실히 아는 것"**과 **"모르는 것"**을 구분하는 것으로 이해할 수 있습니다.
    * 우리가 $do(x)$를 했을 때, 원래 자연스럽게 $X=x$를 선택했던 사람들은 관측 데이터 $P(y, x)$와 똑같이 행동할 것입니다. 이는 최소한 보장되는 비율입니다 (**하한**).
    * 반면, 원래 $X \ne x$였던 사람들(처치를 받지 않은 비율 $P(x')$)이 강제로 처치를 받았을 때 어떻게 행동할지는 데이터로 알 수 없습니다.
    * **상한의 논리:** "처치 받지 않은 비율($P(x')$)에 속하는 사람들이 강제로 처치를 받았을 때, **모두가 $Y=y$로 변화한다(성공한다)**"고 가장 긍정적으로 가정하면 최댓값이 됩니다.

---

## 유도 과정 (Derivation)

* 왜 저런 범위가 나오는지 수식으로 살펴봅시다. 구조적 인과 모델(SCM)에서 $P(y|do(x))$는 다음과 같이 정의됩니다.

$$
P(y|do(x)) = \sum_u P(y|x, u)P(u)
$$

* 확률의 성질을 이용하여 $P(u)$를 두 부분으로 나눌 수 있습니다: $P(u) = P(u, x) + \{P(u) - P(u, x)\}$. 이를 식에 대입하면:

$$
\begin{aligned}
P(y|do(x)) &= \sum_u P(y|x, u) [P(u, x) + \{P(u) - P(u, x)\}] \\
&= \underbrace{\sum_u P(y|x, u)P(u, x)}_{\text{Part A}} + \underbrace{\sum_u P(y|x, u)\{P(u) - P(u, x)\}}_{\text{Part B}}
\end{aligned}
$$

* 여기서 **Part A**를 정리하면:

$$
\text{Part A} = \sum_u P(y|x, u)P(x|u)P(u) = \sum_u P(y, x, u) = P(y, x)
$$

* 즉, Part A는 우리가 관측 가능한 데이터 $P(y, x)$와 같습니다. 이제 **Part B** 때문에 범위가 생깁니다.

### 하한 (Lower Bound) 유도

* $P(y|x, u)$는 확률이므로 $0 \le P(y|x, u) \le 1$. 

* $P(u) = \sum_x P(u, x)$이므로 $P(u) \ge P(u, x)$. 

$$
\begin{aligned}
P(y|do(x)) &= \sum_u P(y|x, u)P(u, x) + \sum_u P(y|x, u)\{P(u) - P(u, x)\} \\
&= P(y,x) + \sum_u P(y|x, u)\{P(u) - P(u, x)\} \\
&\ge P(y, x) + 0 = P(y, x) \\
\end{aligned}
$$

### 상한 (Upper Bound) 유도

* $P(y|x, u)$는 확률이므로 $0 \le P(y|x, u) \le 1$. 

**Part B**의 조건부 확률이 모두 1일 때 전체 값은 최대가 됩니다.
$$
\begin{aligned}
P(y|do(x)) &= \sum_u P(y|x, u)P(u, x) + \sum_u P(y|x, u)\{P(u) - P(u, x)\} \\
&= P(y,x) + \sum_u P(y|x, u)\{P(u) - P(u, x)\} \\
&\le P(y, x) + \sum_u \{P(u) - P(u, x)\} \\
&= P(y, x) + 1 - P(x)
\end{aligned}
$$

---

## 범위의 타당성 증명 (Tightness)

* 구한 범위 $[P(y, x), P(y, x) + 1 - P(x)]$가 **Tight하다**는 것은, 추가적인 가정 없이는 이 범위를 더 좁힐 수 없음을 의미합니다. 

* 이를 증명하기 위해, 관측 데이터 $P(x, y)$와 완벽히 일치하면서(Compatible), 인과 효과가 각각 하한값과 상한값을 갖는 두 개의 가상 모델($M^{(1)}, M^{(2)}$)을 만들어낼 수 있음을 보이면 됩니다.

### 모델 설계 전략

* 두 모델 모두 $U \sim P(u)$와 $X \leftarrow f_X(u)$는 동일하게 두고, $Y$를 결정하는 함수 $f_Y(x, u)$만 다르게 설정합니다.

$$
f_Y(x, u) = 
\begin{cases} 
f_Y^{\text{observation}}(x, u) & \text{if } x = f_X(u) \quad \text{(관측된 경우)} \\
f_Y^{\text{counterfactual}}(x, u) & \text{if } x \ne f_X(u) \quad \text{(반사실적 경우)}
\end{cases}
$$

### Case 분류

* 우리는 상황을 두 가지 케이스로 나눌 수 있습니다.

  * **Case 1 (Observation):** $x = f_X(u)$. 
    * 즉, 개인이 **원래 $x$를 선택하려던 경우**입니다. 
    * 이 경우 모델은 관측 데이터와 일치해야 하므로 선택의 여지가 없습니다.
  
  * **Case 2 (Counterfactual):** $x \ne f_X(u)$. 
    * 즉, **원래 다른 것을 하려던 사람에게 억지로 $x$를 시킬 때**입니다. 
    * 이 부분은 데이터로 관측되지 않으므로, 우리가 임의로 0(실패) 또는 1(성공)을 부여하여 범위를 만듭니다.

### 두 모델의 구성

1.  **모델 $\mathcal{M}_x^{(1)}$ (하한 모델):**
    * Case 2일 때 무조건 **$Y=0$**을 출력하게 설정합니다.
    * 인과 효과: $P^{(1)}(y=1|do(x)) = P(y=1, x) + 0 = \text{Lower Bound}$

2.  **모델 $\mathcal{M}_x^{(2)}$ (상한 모델):**
    * Case 2일 때 무조건 **$Y=1$**을 출력하게 설정합니다.
    * 인과 효과: $P^{(2)}(y=1|do(x)) = P(y=1, x) + 1- P(x) = \text{Upper Bound}$

* 두 모델 모두 관측 상황(Case 1)에서는 동일하게 작동하므로 $P(x, y)$ 분포는 같습니다. 
* 하지만 $do(x)$ 개입 시(Case 2), $U$와 $X$가 일치할 필요가 없으므로 $\mathcal{M}_x^{(1)}$와 $\mathcal{M}_x^{(2)}$가 $Y = 1$ 일 때, 다른 확률을 가질 수 있습니다.

$$
\begin{aligned}
P^{(i)}(Y = 1|do(x)) &= \sum_{u} P^{(i)}(Y = 1|do(x), u)P(u|do(x)) \\
&= \sum_{u} P^{(i)}(Y = 1|x, u)P(u) && \because \text{Rule 2, Rule 3}\\
&= P^{(i)}(Y = 1|x, f_X(U) = x)P(f_X(U) = x) \\
&\quad + P^{(i)}(Y = 1|x, f_X(U) \neq x)P(f_X(U) \neq x) \\
&= P^{(i)}(Y = 1|x)P(x) + \mathbf{1}[i = 2]\{1 - P(x)\} \\
&= P^{(i)}(x, Y = 1) + \mathbf{1}[i = 2](1 - P(x))
\end{aligned}
$$

$$
\begin{aligned}
P^{(1)}(Y = 1|do(x)) &= a(y;x) = P(y = 1, x) \quad \text{while} \\
P^{(2)}(Y = 1|do(x)) &= b(y;x) = P(y = 1, x) + 1 - P(x)
\end{aligned}
$$

# 결론
* Partial Identification은 식별 불가능한 상황에서 우리가 데이터로부터 얻을 수 있는 정보의 한계를 명확히 해줍니다. 
* Natural Bounds는 어떠한 가정 없이도 얻을 수 있는 "진실의 범위"이며, 이 범위가 너무 넓다면 추가적인 가정을 통해 범위를 좁혀나가는 과정이 필요합니다.