---
title: "[Causal Inference] 03. Identification of Causal Effects"
description: "관측 데이터로부터 인과 효과를 계산할 수 있는가? (SCM, Truncated Factorization, Adjustment Formula)"
author: "유성현"
date: "2026-01-23"
categories: [Causal Inference]
format:
  html:
    toc: true
    number-sections: false
    code-fold: show
    math: true
---

> [cite_start]**Note**: 본 포스트는 서울대학교 GSDS 이상학 교수님의 "Identification of Causal Effects" 강의 자료를 바탕으로 작성되었습니다. [cite: 1, 2]

# 1. Introduction: The Challenge of Causal Inference

인과 추론(Causal Inference)의 핵심 목표는 정적인 데이터(Static conditions)로부터 **변화(Change)**를 예측하는 것입니다.

[cite_start]우리가 흔히 접하는 관측 데이터(Observational Data)는 특정 체제(Regime) 하에서의 인구 집단 분포를 설명할 뿐, 시스템에 변화가 가해졌을 때 인구 집단이 어떻게 반응할지는 말해주지 않습니다[cite: 27, 28].

[cite_start]예를 들어, "흡연을 금지시킨다면 암 환자가 줄어들까?"라는 질문은 $P(Cancer|Smoking)$이라는 관측된 조건부 확률(Association)이 아니라, $P(Cancer|do(Smoking=no))$라는 개입(Intervention) 후의 확률을 묻는 것입니다[cite: 24, 25].

이 포스트에서는 **구조적 인과 모형(Structural Causal Model, SCM)**을 사용하여 관측 데이터($P$)와 인과 그래프($G$)가 주어졌을 때, 인과 효과($P(y|do(x))$)를 식별(Identify)해내는 과정을 수학적으로 다룹니다.

---

# 2. Structural Causal Model & Intervention

## 2.1. Real World vs. Hypothetical World

인과 효과를 정의하기 위해 우리는 두 개의 세계를 비교해야 합니다.

1.  **Real World (Observational World):** 변수들이 자연스러운 인과 구조에 따라 값을 갖는 세계. 결합 확률 분포 $P(z, x, w, y)$로 표현됩니다.
2.  **Hyphetical World (Interventional World):** 우리가 변수 $X$를 강제로 특정 값 $x$로 고정($do(X=x)$)했을 때의 세계. [cite_start]분포 $P(z, w, y | do(x))$로 표현됩니다 [cite: 33-44].

![Figure: Real world vs. Hypothetical world. 왼쪽(Real world)에서는 Z가 X에 영향을 주지만, 오른쪽(Hyphetical world)에서는 X에 대한 개입(do(X))으로 인해 Z에서 X로 가는 화살표가 끊어진(Mutilated) 것을 볼 수 있다.](./images/real_vs_hypothetical_world.png)

위 그림에서 볼 수 있듯이, 개입 $do(X=x)$는 모델 내에서 $X$를 결정하는 모든 방정식(화살표)을 삭제하고, $X=x$라는 상수로 대체하는 연산입니다. [cite_start]이를 통해 $Z$(교란 변수)가 $X$에 미치는 영향을 차단합니다 [cite: 52-71].

## 2.2. Causal Effect의 정의

수학적으로, $X$가 $Y$에 미치는 **인과 효과(Causal Effect)** $P(y|do(x))$는 다음과 같이 정의됩니다.

$$P(y|do(x)) = P_x(y)$$

여기서 $P_x(y)$는 서브모델(Submodel) $\mathcal{M}_x$에서 $Y=y$일 확률을 의미합니다. [cite_start]$\mathcal{M}_x$는 원래 모델 $\mathcal{M}$에서 $X$에 관련된 모든 방정식을 삭제하고 $X=x$를 대입하여 얻은 모델입니다 [cite: 96-98].

---

# 3. Computing Causal Effects: The Sprinkler Example

[cite_start]이론적인 정의를 넘어, 실제 관측 데이터로 이를 어떻게 계산하는지 "스프링클러 예제"를 통해 살펴보겠습니다[cite: 125].

## 3.1. 시나리오 및 그래프

다음과 같은 변수와 인과 관계가 있다고 가정합니다.
* **Season ($Sn$):** 계절 (모든 변수의 근원)
* **Sprinkler ($Sp$):** 스프링클러 가동 여부 (계절에 영향 받음)
* **Rain ($Rn$):** 비 옴 여부 (계절에 영향 받음)
* **Wet ($Wt$):** 땅이 젖음 (스프링클러와 비에 영향 받음)
* **Slippery ($Sl$):** 미끄러움 (땅이 젖음에 영향 받음)

전체 결합 확률 분포(Markovian factorization)는 다음과 같습니다:
$$P(v) = P(Sn)P(Sp|Sn)P(Rn|Sn)P(Wt|Sp, Rn)P(Sl|Wt)$$
[cite_start][cite: 127]

![Figure: Sprinkler Example Causal Graph. Season이 Sprinkler와 Rain의 공통 원인(Confounder)으로 작용하고 있으며, Sprinkler와 Rain은 Wet의 원인이 된다.](./images/sprinkler_graph.png)

## 3.2. Query 1: Observation ($P(Wt|Sp=on)$)

우리가 단순히 "스프링클러가 켜진 것을 목격했을 때($Sp=on$)", 땅이 젖어 있을 확률은 조건부 확률의 정의에 따라 다음과 같습니다.

$$
Q_1 = P(Wt | Sp=\text{on}) = \frac{\sum_{sn, rn} P(sn) \mathbf{P(Sp=\text{on}|sn)} P(rn|sn) P(wt|Sp=\text{on}, rn)}{\sum_{sn} \mathbf{P(Sp=\text{on}|sn)} P(sn)}
$$
[cite_start][cite: 148]

* **해석:** 여기서 $P(Sp=\text{on}|sn)$ 항이 살아있습니다. 즉, 계절에 따라 스프링클러를 켜는 경향성(Selection Bias)이 결과에 반영됩니다.

## 3.3. Query 2: Intervention ($P(Wt|do(Sp=on))$)

이제 우리가 "스프링클러를 강제로 켰을 때($do(Sp=on)$)", 땅이 젖어 있을 확률을 구해보겠습니다.
개입이 일어나면 $Sn \rightarrow Sp$의 화살표가 끊어지므로, $Sp$는 더 이상 $Sn$의 함수가 아닙니다. 따라서 분해 식에서 **$P(Sp|Sn)$ 항이 제거(Truncated)** 됩니다.

$$
Q_2 = P(Wt | do(Sp=\text{on})) = \sum_{sn, rn} P(sn) P(rn|sn) P(wt|Sp=\text{on}, rn)
$$
[cite_start][cite: 165]

* **중요한 차이:** 관측 식($Q_1$)과 달리, $Q_2$에서는 $P(Sp|Sn)$ 항이 사라졌습니다. 대신 자연적인 계절의 분포 $P(Sn)$과 비의 분포 $P(Rn|Sn)$에 따라 가중 평균을 구하게 됩니다.
* [cite_start]이것은 관측 분포와 개입 분포 사이의 변환을 **재가중(Re-weighting) 과정**으로 해석할 수 있음을 보여줍니다[cite: 211].

---

# 4. Truncated Factorization Formula

위 예제에서 본 원리를 일반화하면 **Truncated Factorization Formula (절단된 분해 공식)**를 얻을 수 있습니다. 이것은 마르코프 모형(Markovian Model)에서 인과 효과를 계산하는 가장 기본적인 정리입니다.

## 4.1. Theorem (Manipulation Theorem)

마르코프 모형 $M$에서 개입 $do(X=x)$에 의해 생성된 확률 분포는 다음과 같이 주어집니다.

$$P(v \setminus x | do(x)) = \prod_{V_i \in V \setminus X} P(v_i | pa_i)$$
[cite_start][cite: 173]

즉, 전체 결합 확률 $P(v) = \prod P(v_i|pa_i)$에서 개입된 변수 $X$에 해당하는 항 $P(x|pa_x)$만 제거한 형태입니다.

## 4.2. Re-weighting 관점

이 식은 관측 데이터의 분포 $P(v)$를 이용해 다음과 같이 다시 쓸 수 있습니다.

$$
P(v \setminus x | do(x)) = \frac{P(v)}{\prod_{X \in X} P(x|pa_X)}
$$

만약 $X$가 단일 변수라면(Singleton), 이는 다음과 같이 표현됩니다.

$$
P(v \setminus x | do(x)) = \frac{P(v)}{P(x|pa_X)} = P(v'' | x, pa_X)P(pa_X)
$$
(여기서 $V'' = V \setminus Pa_X \setminus \{X\}$) [cite_start][cite: 200, 209].

이 공식은 인과 추론 문제를 "어떻게 $P(x|pa_x)$(Propensity Score)로 관측 데이터를 역가중(Inverse weighting)할 것인가"의 문제로 연결해 줍니다.

---

# 5. The Identification Problem

이제 근본적인 질문을 던져봅시다. **"우리는 언제 인과 효과를 구할 수 있는가?"**

## 5.1. 식별 가능성 (Identifiability) 정의

[cite_start]인과 효과 $P(y|do(x))$가 인과 그래프 $G$로부터 **식별 가능하다(Identifiable)**는 것은, 관측 가능한 변수들의 확률 분포 $P(v)$ ($P(v)>0$)만으로 $P(y|do(x))$를 유일하게(Uniquely) 계산해낼 수 있다는 뜻입니다[cite: 219].

$$P^{M_1}(v) = P^{M_2}(v) \implies P^{M_1}(y|do(x)) = P^{M_2}(y|do(x))$$

![Figure: Identifiability Venn Diagram. 관측 분포 P(v)와 그래프 G를 공유하는 모든 모델(M1, M2)이 동일한 인과 효과 P(y|do(x))를 내놓는다면, 그 효과는 식별 가능하다. 만약 그렇지 않다면(빨간 교집합 영역), 식별 불가능하다.](./images/identifiability_venn.png)

[cite_start]즉, 데이터($P(v)$)와 가정($G$)이 같다면, 내부 파라미터가 달라도 결론($Q$)은 같아야 한다는 것입니다 [cite: 302-304].

---

# 6. Identification in Markovian Models

모든 관련 변수가 관측된(Hidden variable이 없는) 마르코프 모형에서는 인과 효과가 **항상 식별 가능**합니다.

## 6.1. General Theorem

[cite_start]모든 변수 $V$가 측정된 마르코프 모형의 인과 그래프 $G$가 주어졌을 때, 임의의 집합 $X, Y$에 대한 인과 효과 $P(y|do(x))$는 식별 가능하며, 다음 두 단계로 계산됩니다 [cite: 361-364].

1.  **Truncated Factorization:** 전체 시스템의 개입 후 분포를 구합니다.
    $$P(v' | do(x)) = \prod_{V_i \in V \setminus X} P(v_i | pa_i)$$
2.  **Marginalization (Hence step):** 관심 있는 결과 변수 $Y$를 제외한 나머지 변수($V' \setminus Y$)를 합(Summation)하여 제거합니다.
    $$P(y | do(x)) = \sum_{V' \setminus Y} \prod_{V_i \in V \setminus X} P(v_i | pa_i)$$

---

# 7. Adjustment Formulas (Backdoor Adjustment)

위의 일반 정리를 실전에서 자주 쓰이는 형태로 정리한 것이 **조정 공식(Adjustment Formula)**입니다.

## 7.1. Adjustment by Direct Parents (Singleton)

[cite_start]단일 변수 $X$에 대해, 그 부모 변수들 $Pa_X$가 모두 관측되었다면, 인과 효과는 다음과 같이 부모 변수를 조정(Conditioning)하여 계산할 수 있습니다 [cite: 368-369].

$$
P(y|do(x)) = \sum_{pa_X} P(y|x, pa_X)P(pa_X)
$$

이 공식은 "원인의 직접적인 원인(Parents)"을 통제하면 교란 요인을 차단할 수 있다는 직관을 수식화한 것입니다.

## 7.2. Adjustment by Direct Parents (Set of Treatments) - Advanced

만약 $X$가 단일 변수가 아니라 변수들의 집합 $X = \{X_1, ..., X_k\}$라면 어떻게 될까요?
[cite_start]변수들이 위상학적 순서(Topological order)로 정렬되어 있다고 가정할 때, 다음 조건이 만족되면 일반화된 조정 공식을 사용할 수 있습니다 [cite: 374-379].

**Theorem:**
만약 모든 $i < j$에 대해, $Pa_{X_j} \setminus (X_{<j} \cup Pa_{X_{<j}}^-)$가 $X_i$의 자손(Descendant)이 아니라면:

$$P(y|do(x)) = \sum_{pa_X^-} P(y|x, pa_X^-)P(pa_X^-)$$
(여기서 $Pa_X^- = Pa_X \setminus X$)

[cite_start]**Proof Sketch (by Induction):** [cite: 382-401]

이 증명은 $P(x_{\le i} | pa_{X_{\le i}}^-)$가 $\prod_{X \in X_{\le i}} P(x|pa_X)$와 같음을 보이는 귀납법을 사용합니다.
1.  **Base case:** $i=1$일 때 성립함은 자명합니다.
2.  **Hypothesis:** $i-1$까지 성립한다고 가정하고, $i$번째 단계에서 조건부 독립(d-separation)과 연쇄 법칙(Chain rule)을 사용하여 식을 전개합니다.
3.  핵심은 $P(pa_{X_i}' | x_{<i}, pa_{X_{<i}}^-)$ 항이 $P(pa_{X_i}' | pa_{X_{<i}}^-)$로 단순화되는 과정에 있으며, 이는 가정된 그래프 구조(자손이 아님) 덕분에 성립합니다.

[cite_start]이 정리는 복잡한 다중 처치(Multiple Treatments) 상황에서도 부모 변수들을 적절히 조정하면 인과 효과를 식별할 수 있음을 보장합니다[cite: 414].

---

# 8. Handling Latent Variables: Latent Season Example

마지막으로, 만약 중요한 교란 변수가 관측되지 않았다면(Latent) 어떻게 될까요? [cite_start]앞선 스프링클러 예제에서 **Season이 관측 불가능한 잠재 변수**라고 가정해 봅시다[cite: 419].

우리는 $Q_2 = P(Wt | do(Sp=on))$을 계산하고 싶습니다. 원래 식은 다음과 같았습니다.

$$Q_2 = \sum_{sn, rn} P(sn)P(rn|sn)P(wt|Sp=on, rn)$$

여기서 $P(sn)$과 $P(rn|sn)$은 관측 불가능한 $sn$을 포함하고 있어 계산이 불가능해 보입니다. 하지만 수식을 정리해보면 놀라운 결과를 얻습니다.

$$
\begin{aligned}
Q_2 &= \sum_{rn} P(wt|Sp=on, rn) \sum_{sn} P(sn)P(rn|sn) \\
&= \sum_{rn} P(wt|Sp=on, rn) \sum_{sn} P(rn, sn) \\
&= \sum_{rn} P(wt|Sp=on, rn) P(rn)
\end{aligned}
$$
[cite_start][cite: 462-465]

**결론:**
최종 식 $\sum_{rn} P(wt|Sp=on, rn) P(rn)$에는 관측 불가능한 $Season$이 사라졌습니다!
이는 $Season$을 몰라도, $Rain$과 $Sprinkler$만 관측하면 인과 효과를 계산할 수 있음을 의미합니다.

이 예시는 모든 교란 변수를 통제할 수 없는 상황에서도, 그래프 구조와 확률 법칙을 잘 활용하면 인과 효과가 **식별 가능(Identifiable)**할 수 있음을 시사합니다.

---

# Summary

1.  **인과 효과의 식별:** 관측 데이터($P(v)$)와 인과 그래프($G$)를 통해 개입 효과($P(y|do(x))$)를 유일하게 결정하는 과정입니다.
2.  **Truncated Factorization:** $P(v \setminus x | do(x)) = \prod_{V_i \in V \setminus X} P(v_i | pa_i)$. 모든 마르코프 모형의 기초가 되는 공식입니다.
3.  **Adjustment Formula:** $P(y|do(x)) = \sum_{pa_X} P(y|x, pa_X)P(pa_X)$. 원인의 부모 변수를 조정하여 인과 효과를 계산하는 실용적인 공식입니다.
4.  **잠재 변수:** 변수가 숨겨져 있어도(Latent), 구조에 따라 인과 효과가 식별 가능할 수 있습니다.

---

### Check List: Coverage & Verification

* **SCM & Intervention Definition:** $\checkmark$ (Sec 2)
* **The Challenge (Static vs Change):** $\checkmark$ (Sec 1)
* **Sprinkler Example (Obs vs Intervention):** $\checkmark$ (Sec 3)
* **Truncated Factorization Theory:** $\checkmark$ (Sec 4)
* **Identifiability Definition (Venn Diagram):** $\checkmark$ (Sec 5)
* **Identification in Markovian Models (Theorem):** $\checkmark$ (Sec 6)
* **Adjustment Formula (Singleton):** $\checkmark$ (Sec 7.1)
* **Adjustment Formula (Set of Treatments - Proof included):** $\checkmark$ (Sec 7.2)
* **Latent Variable Case (Derivation included):** $\checkmark$ (Sec 8)

**누락된 내용:** 없음. 강의 자료의 모든 핵심 정리와 예제를 포함하였으며, 특히 Optional로 표시된 다중 처치(Set of Treatments) 증명 과정까지 상세히 기술하였습니다.