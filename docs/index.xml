<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>shsha0110.github.io</title>
<link>https://shsha0110.github.io/</link>
<atom:link href="https://shsha0110.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>A blog built with Quarto</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sat, 17 Jan 2026 15:00:00 GMT</lastBuildDate>
<item>
  <title>[Causal Inference] 13. IV (Part 3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/causal-inference-13-part-03/</link>
  <description><![CDATA[ 





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>사회과학 데이터, 특히 범죄학이나 경제학 데이터를 다루다 보면 선형(Linear) 모델로는 설명하기 힘든 복잡한 인과관계를 마주하게 됩니다.</p>
<p>전통적인 <strong>2단계 최소제곱법(2SLS)</strong>은 강력한 도구이지만, 다음과 같은 한계가 있습니다. 1. 처치(Treatment)와 결과(Outcome)의 관계를 <strong>선형</strong>으로 가정합니다. 2. 공변량(Covariate)과 처치 변수 간의 복잡한 <strong>상호작용</strong>을 포착하기 어렵습니다.</p>
<p>이번 포스트에서는 Hartford et al.(2017)이 제안한 <strong>Deep IV</strong> 방법론을 소개하고, 가격(<img src="https://latex.codecogs.com/png.latex?P">)과 판매량(<img src="https://latex.codecogs.com/png.latex?Y">)의 비선형적 관계를 시뮬레이션 데이터를 통해 추정해보겠습니다.</p>
<section id="the-problem-formulation" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-formulation">The Problem Formulation</h3>
<p>우리가 관심 있는 인과 모델은 다음과 같습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?Y%20=%20g(P,%20X)%20+%20%5Cepsilon"></p>
<p>여기서: * <img src="https://latex.codecogs.com/png.latex?Y">: 결과 변수 (예: 판매량, 범죄율) * <img src="https://latex.codecogs.com/png.latex?P">: 처치 변수 (예: 가격, 경찰 순찰 빈도) - <strong>내생성(Endogeneity) 존재</strong> (<img src="https://latex.codecogs.com/png.latex?E%5B%5Cepsilon%7CP%5D%20%5Cneq%200">) * <img src="https://latex.codecogs.com/png.latex?X">: 관측 가능한 공변량 (예: 성수기 여부, 지역 특성) * <img src="https://latex.codecogs.com/png.latex?Z">: 도구 변수 (예: 원자재 가격, 예산) - 외생성 만족</p>
<p>Deep IV는 이 문제를 풀기 위해 <strong>두 단계의 신경망</strong>을 사용합니다.</p>
<ol type="1">
<li><strong>Stage 1 (Treatment Network):</strong> 도구 변수를 이용해 처치 변수의 조건부 분포 <img src="https://latex.codecogs.com/png.latex?F(P%7CZ,%20X)">를 학습합니다.</li>
<li><strong>Stage 2 (Outcome Network):</strong> 1단계에서 추정된 분포를 적분(Integrate out)하여 인과 함수 <img src="https://latex.codecogs.com/png.latex?g(P,%20X)">를 학습합니다.</li>
</ol>
<hr>
</section>
</section>
<section id="data-generation" class="level2">
<h2 class="anchored" data-anchor-id="data-generation">2. Data Generation</h2>
<p>먼저 내생성과 비선형성이 존재하는 가상의 데이터를 생성합니다. 상황은 다음과 같습니다. * <strong>가격(<img src="https://latex.codecogs.com/png.latex?P">)</strong>이 오르면 <strong>판매량(<img src="https://latex.codecogs.com/png.latex?Y">)</strong>은 줄어듭니다. * 하지만 <strong>성수기(<img src="https://latex.codecogs.com/png.latex?X">)</strong>에는 가격도 비싸고 판매량도 많아, 단순 회귀 시 양의 상관관계(편향)가 관찰됩니다. * 실제 인과 효과는 특정 가격 이상에서 급격히 판매량이 떨어지는 <strong>S자 곡선(Sigmoid)</strong> 형태입니다.</p>
<div id="data-generation" class="cell" data-execution_count="1">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.optim <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> optim</span>
<span id="cb1-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.distributions <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> D</span>
<span id="cb1-8"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn.functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb1-9"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LinearRegression</span>
<span id="cb1-10"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sklearn.preprocessing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> StandardScaler</span></code></pre></div></div>
</details>
</div>
<div id="2c402ad1" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate_data(n, seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>):</span>
<span id="cb2-2">    np.random.seed(seed)</span>
<span id="cb2-3">    </span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 외생 변수 생성</span></span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X: 공변량 (성수기 여부)</span></span>
<span id="cb2-6">    x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n) </span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Z: 도구변수 (연료비)</span></span>
<span id="cb2-8">    z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.uniform(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n) </span>
<span id="cb2-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># E: 교란 변수</span></span>
<span id="cb2-10">    e <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n)   </span>
<span id="cb2-11"></span>
<span id="cb2-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 처치 변수 (P) 생성</span></span>
<span id="cb2-13">    p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> z) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> e) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n)</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. 결과 변수 (Y) 생성</span></span>
<span id="cb2-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># True Structural Function: g(p, x)</span></span>
<span id="cb2-17">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> true_structural_function(p_val, x_val):</span>
<span id="cb2-18">        threshold <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X에 따라 임계값이 변함 (Heterogeneity)</span></span>
<span id="cb2-19">        base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">150</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.exp(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (p_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> threshold)))</span>
<span id="cb2-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val)</span>
<span id="cb2-21">        </span>
<span id="cb2-22">    y_structural <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> true_structural_function(p, x)</span>
<span id="cb2-23">    y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> y_structural <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> e) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.random.normal(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, n)</span>
<span id="cb2-24"></span>
<span id="cb2-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (z.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), x.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), p.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), y.reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), true_structural_function)</span></code></pre></div></div>
</details>
</div>
<div id="d1fe5ae9" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 데이터 생성</span></span>
<span id="cb3-2">Z_data, X_data, P_data, Y_data, true_func <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generate_data(n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb3-3"></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Scaler 선언</span></span>
<span id="cb3-5">scaler_z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb3-6">scaler_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb3-7">scaler_p <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb3-8">scaler_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> StandardScaler()</span>
<span id="cb3-9"></span>
<span id="cb3-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Fitting &amp; Transform</span></span>
<span id="cb3-11">Z_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_z.fit_transform(Z_data)</span>
<span id="cb3-12">X_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_x.fit_transform(X_data)</span>
<span id="cb3-13">P_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_p.fit_transform(P_data)</span>
<span id="cb3-14">Y_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_y.fit_transform(Y_data)</span>
<span id="cb3-15"></span>
<span id="cb3-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 텐서 변환</span></span>
<span id="cb3-17">Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(Z_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb3-18">X <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(X_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb3-19">P <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(P_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb3-20">Y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(Y_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span></code></pre></div></div>
</details>
</div>
<div id="abe4b2b0" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 스타일 설정</span></span>
<span id="cb4-2">sns.set_style(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"whitegrid"</span>)</span>
<span id="cb4-3">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'figure.figsize'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">18</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>)</span>
<span id="cb4-4">plt.rcParams[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'font.size'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span></span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 데이터 1차원 변환</span></span>
<span id="cb4-7">z_flat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Z_data.flatten()</span>
<span id="cb4-8">x_flat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> X_data.flatten()</span>
<span id="cb4-9">p_flat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> P_data.flatten()</span>
<span id="cb4-10">y_flat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Y_data.flatten()</span>
<span id="cb4-11"></span>
<span id="cb4-12">fig, axes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb4-13">fig.suptitle(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'DeepIV Simulation: Non-linear Causal Inference'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">22</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb4-14"></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-16"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (A) First Stage Strength: 도구 변수(Z) -&gt; 처치(P)</span></span>
<span id="cb4-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-18">sns.regplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>z_flat, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>p_flat, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], </span>
<span id="cb4-19">            scatter_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'alpha'</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'navy'</span>}, line_kws<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'color'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>})</span>
<span id="cb4-20">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(A) First Stage Relevance: Instrument(Z) -&gt; Treatment(P)'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb4-21">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instrument Z (Fuel Cost)'</span>)</span>
<span id="cb4-22">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Treatment P (Price)'</span>)</span>
<span id="cb4-23">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].text(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>, <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Corr(Z, P): </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>np<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>corrcoef(z_flat, p_flat)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Strong Relevance"</span>, </span>
<span id="cb4-24">                transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].transAxes, bbox<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>))</span>
<span id="cb4-25"></span>
<span id="cb4-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (B) Confounding Bias: 가격(P) vs 판매량(Y) (관측 데이터)</span></span>
<span id="cb4-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X(성수기 여부)가 P와 Y 모두를 증가시키는 교란(Confounding) 현상 시각화</span></span>
<span id="cb4-30">scatter <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].scatter(p_flat, y_flat, c<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>x_flat, cmap<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'coolwarm'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, s<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span>
<span id="cb4-31">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(B) Confounding Bias: Observed P vs Y'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb4-32">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Price P'</span>)</span>
<span id="cb4-33">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sales Y'</span>)</span>
<span id="cb4-34">cbar <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.colorbar(scatter, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb4-35">cbar.set_label(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Confounder X (Seasonality)'</span>, rotation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">270</span>, labelpad<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>)</span>
<span id="cb4-36">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].text(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Endogeneity Present:</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">High X causes High P &amp; High Y"</span>, </span>
<span id="cb4-37">                transform<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].transAxes, bbox<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'white'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>))</span>
<span id="cb4-38"></span>
<span id="cb4-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (C) Covariate Distribution: 공변량(X) -&gt; 가격(P)</span></span>
<span id="cb4-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-42">sns.kdeplot(x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>p_flat, hue<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(x_flat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>), fill<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, ax<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], palette<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'coolwarm'</span>)</span>
<span id="cb4-43">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(C) P Distribution by Seasonality (X)'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb4-44">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Treatment P (Price)'</span>)</span>
<span id="cb4-45">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].legend([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'High Season (X&gt;0.5)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Low Season (X&lt;=0.5)'</span>])</span>
<span id="cb4-46"></span>
<span id="cb4-47"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># (D) The "True" Causal Curve (Ground Truth)</span></span>
<span id="cb4-49"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ---------------------------------------------------------</span></span>
<span id="cb4-50"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_true_effect_consistent(p_input, x_val):</span>
<span id="cb4-51">    threshold <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val)</span>
<span id="cb4-52">    base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">150</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.exp(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (p_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> threshold)))</span>
<span id="cb4-53">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val)</span>
<span id="cb4-54"></span>
<span id="cb4-55">p_range <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(p_flat.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(), p_flat.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>)</span>
<span id="cb4-56"></span>
<span id="cb4-57">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].scatter(p_flat, y_flat, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Observed Samples'</span>)</span>
<span id="cb4-58"></span>
<span id="cb4-59"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 시나리오별 True Curve 그리기</span></span>
<span id="cb4-60"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># X=0.1 (비수기), X=0.5 (평균), X=0.9 (성수기)</span></span>
<span id="cb4-61">lines_x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]</span>
<span id="cb4-62">colors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'blue'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'red'</span>]</span>
<span id="cb4-63">labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Low Season (X=0.1)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Avg Season (X=0.5)'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'High Season (X=0.9)'</span>]</span>
<span id="cb4-64"></span>
<span id="cb4-65"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> lx, c, lbl <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(lines_x, colors, labels):</span>
<span id="cb4-66">    y_true <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_true_effect_consistent(p_range, lx)</span>
<span id="cb4-67">    axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].plot(p_range, y_true, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>c, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.5</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f'True: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>lbl<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb4-68"></span>
<span id="cb4-69">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'(D) Ground Truth: Heterogeneous S-Curves'</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">14</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb4-70">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Treatment P (Price)'</span>)</span>
<span id="cb4-71">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].set_ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Outcome Y (Sales)'</span>)</span>
<span id="cb4-72">axes[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].legend(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'upper right'</span>, frameon<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, framealpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb4-73"></span>
<span id="cb4-74">plt.tight_layout()</span>
<span id="cb4-75">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/causal-inference-13-part-03/index_files/figure-html/cell-5-output-1.png" width="1712" height="1130" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<hr>
</section>
<section id="benchmark-linear-2sls" class="level2">
<h2 class="anchored" data-anchor-id="benchmark-linear-2sls">3. Benchmark: Linear 2SLS</h2>
<p>비교를 위해 전통적인 Linear 2SLS를 먼저 수행합니다. <code>scikit-learn</code>을 사용하여 2단계 회귀분석을 진행합니다.</p>
<ol type="1">
<li>Stage 1: Predict using</li>
<li>Stage 2: Regress on</li>
</ol>
<div id="linear-2sls" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Linear 2SLS Implementation</span></span>
<span id="cb5-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Stage 1] P ~ Z + X</span></span>
<span id="cb5-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 도구변수(Z)와 공변량(X)를 사용하여 내생변수(P)를 예측.</span></span>
<span id="cb5-7">ZX_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate((Z_data, X_data), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-8">stage1_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression()</span>
<span id="cb5-9">stage1_model.fit(ZX_data, P_data)</span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># P_hat (Hat P): 내생성이 제거된 P의 부분</span></span>
<span id="cb5-12">P_hat <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stage1_model.predict(ZX_data)</span>
<span id="cb5-13"></span>
<span id="cb5-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Stage 2] Y ~ P_hat + X</span></span>
<span id="cb5-15"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 예측된 처치(P_hat)와 공변량(X)를 사용하여 결과(Y)를 예측.</span></span>
<span id="cb5-16">PX_hat_data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate((P_hat, X_data), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb5-17">stage2_model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LinearRegression()</span>
<span id="cb5-18">stage2_model.fit(PX_hat_data, Y_data)</span>
<span id="cb5-19"></span>
<span id="cb5-20"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Linear 2SLS Training Complete."</span>)</span>
<span id="cb5-21"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Estimated Causal Effect (Coefficient of P): </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>stage2_model<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>coef_[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Linear 2SLS Training Complete.
Estimated Causal Effect (Coefficient of P): -3.1744</code></pre>
</div>
</div>
<hr>
</section>
<section id="deep-iv-implementation" class="level2">
<h2 class="anchored" data-anchor-id="deep-iv-implementation">4. Deep IV Implementation</h2>
<section id="stage-1-mixture-density-network-mdn" class="level3">
<h3 class="anchored" data-anchor-id="stage-1-mixture-density-network-mdn">Stage 1: Mixture Density Network (MDN)</h3>
<p>Deep IV의 첫 단계는 의 값을 하나로 예측하는 것이 아니라, 의 <strong>조건부 확률 분포</strong>를 추정하는 것입니다. 이를 위해 가우시안 혼합 모델(Gaussian Mixture Model)을 출력하는 신경망을 사용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Chat%7BF%7D(P%7CZ,X)%20=%20%5Csum_%7Bk=1%7D%5E%7BK%7D%20%5Cpi_k(Z,X)%20%5Cmathcal%7BN%7D(%5Cmu_k(Z,X),%20%5Csigma_k%5E2(Z,X))%20"></p>
<div id="deep-iv-stage1" class="cell" data-execution_count="6">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> FirstStageMDN(nn.Module):</span>
<span id="cb7-2">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, input_dim, num_gaussians<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>):</span>
<span id="cb7-3">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb7-4">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.shared_layer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb7-5">            nn.Linear(input_dim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>),</span>
<span id="cb7-6">            nn.ReLU(),</span>
<span id="cb7-7">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>),</span>
<span id="cb7-8">            nn.ReLU()</span>
<span id="cb7-9">        )</span>
<span id="cb7-10">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.pi_head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, num_gaussians) </span>
<span id="cb7-11">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mu_head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, num_gaussians) </span>
<span id="cb7-12">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.sigma_head <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">64</span>, num_gaussians) </span>
<span id="cb7-13"></span>
<span id="cb7-14">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, z):</span>
<span id="cb7-15">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 입력: 공변량(X)와 도구변수(Z)를 결합</span></span>
<span id="cb7-16">        inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([x, z], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-17">        features <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.shared_layer(inputs)</span>
<span id="cb7-18">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Pi (혼합 비율): Softmax로 합이 1이 되도록 함</span></span>
<span id="cb7-19">        pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.softmax(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.pi_head(features), dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-20">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Mu (평균)</span></span>
<span id="cb7-21">        mu <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.mu_head(features)</span>
<span id="cb7-22">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Sigma (표준편차)</span></span>
<span id="cb7-23">        sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.softplus(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.sigma_head(features)) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-5</span></span>
<span id="cb7-24">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> pi, mu, sigma</span>
<span id="cb7-25"></span>
<span id="cb7-26">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> sample(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x, z, n_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb7-27">        pi, mu, sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.forward(x, z)    </span>
<span id="cb7-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># GMM(Gaussian Mixture Model) 객체 생성</span></span>
<span id="cb7-29">        mix <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.Categorical(probs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pi)</span>
<span id="cb7-30">        comp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.Normal(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mu, scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sigma)</span>
<span id="cb7-31">        gmm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.MixtureSameFamily(mix, comp)</span>
<span id="cb7-32">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 샘플링 수행 (Batch, n_samples)</span></span>
<span id="cb7-33">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> n_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:</span>
<span id="cb7-34">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> gmm.sample().unsqueeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-35">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb7-36">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> gmm.sample((n_samples,)).permute(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb7-37"></span>
<span id="cb7-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 손실 함수 (Negative Log Likelihood)</span></span>
<span id="cb7-39"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> first_stage_loss_fn(pi, mu, sigma, p):</span>
<span id="cb7-40">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 각 가우시안 분포에서의 확률 밀도 계산</span></span>
<span id="cb7-41">    m <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> D.Normal(loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mu, scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sigma)</span>
<span id="cb7-42">    log_probs_component <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> m.log_prob(p)</span>
<span id="cb7-43">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. 혼합 비율(pi) 반영 (Log-Sum-Exp Trick)</span></span>
<span id="cb7-44">    weighted_log_probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.log(pi <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-8</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> log_probs_component</span>
<span id="cb7-45">    log_likelihood <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.logsumexp(weighted_log_probs, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb7-46">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. NLL 최소화</span></span>
<span id="cb7-47">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>torch.mean(log_likelihood)</span></code></pre></div></div>
</details>
</div>
</section>
<section id="stage-2-outcome-network" class="level3">
<h3 class="anchored" data-anchor-id="stage-2-outcome-network">Stage 2: Outcome Network</h3>
<p>두 번째 단계는 인과 함수 <img src="https://latex.codecogs.com/png.latex?g(P,%20X)">를 근사하는 신경망입니다. 손실 함수는 1단계 분포에서 샘플링한 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BP%7D">를 사용하여 계산된 기댓값과 실제 의 차이를 최소화합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20L%20=%20E%20%5Cleft%5B%20%5Cleft(%20Y%20-%20%5Cint%20g(%5Chat%7Bp%7D,%20X)%20d%5Chat%7BF%7D(%5Chat%7Bp%7D%7CZ,X)%20%5Cright)%5E2%20%5Cright%5D%20"></p>
<div id="deep-iv-stage2" class="cell" data-execution_count="7">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2단계: Causal Function h (Outcome Network)</span></span>
<span id="cb8-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> SecondStageH(nn.Module):</span>
<span id="cb8-3">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x_dim, p_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, output_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>):</span>
<span id="cb8-4">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb8-5">        input_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> p_dim</span>
<span id="cb8-6">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Sequential(</span>
<span id="cb8-7">            nn.Linear(input_dim, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),</span>
<span id="cb8-8">            nn.ReLU(),</span>
<span id="cb8-9">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),</span>
<span id="cb8-10">            nn.ReLU(),</span>
<span id="cb8-11">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>),</span>
<span id="cb8-12">            nn.ReLU(),</span>
<span id="cb8-13">            nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">128</span>, output_dim)</span>
<span id="cb8-14">        )</span>
<span id="cb8-15">        </span>
<span id="cb8-16">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, p, x):</span>
<span id="cb8-17">        inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.cat([p, x], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) </span>
<span id="cb8-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.net(inputs)</span>
<span id="cb8-19"></span>
<span id="cb8-20"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> second_stage_loss_fn(model_h, pi, mu, sigma, x, y, num_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span>):</span>
<span id="cb8-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mu shape: (Batch, K)</span></span>
<span id="cb8-22">    batch_size, n_components <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu.shape</span>
<span id="cb8-23">    x_dim <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb8-24">    </span>
<span id="cb8-25">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Step 1] Stratified Sampling (벡터화 구현)</span></span>
<span id="cb8-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 모든 컴포넌트(K)와 샘플(S)을 한꺼번에 처리하기 위해 차원 확장</span></span>
<span id="cb8-27">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># eps: (Batch, S, K)</span></span>
<span id="cb8-28">    eps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.randn(batch_size, num_samples, n_components, device<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mu.device)</span>
<span id="cb8-29">    </span>
<span id="cb8-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># mu, sigma 확장: (Batch, 1, K) -&gt; 브로드캐스팅 -&gt; (Batch, S, K)</span></span>
<span id="cb8-31">    mu_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mu.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-32">    sigma_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sigma.unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-33">    </span>
<span id="cb8-34">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Reparameterization Trick</span></span>
<span id="cb8-35">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p_samples: (Batch, S, K) -&gt; 모든 가우시안 성분별로 S개씩 샘플링</span></span>
<span id="cb8-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 주의: 1단계 모델의 그래디언트는 필요 없으므로 detach() 권장 (일반적인 2SLS)</span></span>
<span id="cb8-37">    p_samples <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (mu_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> sigma_exp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> eps).detach()</span>
<span id="cb8-38">    </span>
<span id="cb8-39">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Step 2] X 확장</span></span>
<span id="cb8-40">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># x: (Batch, x_dim) -&gt; (Batch, S, K, x_dim)</span></span>
<span id="cb8-41">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p_samples와 짝을 맞추기 위해 차원을 늘립니다.</span></span>
<span id="cb8-42">    x_expanded <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.view(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, x_dim).expand(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, num_samples, n_components, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-43">    </span>
<span id="cb8-44">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># p_samples 차원 맞추기: (Batch, S, K) -&gt; (Batch, S, K, 1)</span></span>
<span id="cb8-45">    p_samples_reshaped <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> p_samples.unsqueeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-46">    </span>
<span id="cb8-47">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Step 3] Forward Pass</span></span>
<span id="cb8-48">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 입력 shape: (Batch, S, K, dim) -&gt; model_h는 마지막 dim만 신경 씀</span></span>
<span id="cb8-49">    h_out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_h(p_samples_reshaped, x_expanded) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 결과: (Batch, S, K, 1)</span></span>
<span id="cb8-50">    </span>
<span id="cb8-51">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Step 4] 가중 평균 (Expectation 계산)</span></span>
<span id="cb8-52">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4-1. 각 성분(K)에 대한 가중치(pi) 적용</span></span>
<span id="cb8-53">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># pi: (Batch, K) -&gt; (Batch, 1, K, 1)</span></span>
<span id="cb8-54">    pi_expanded <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pi.view(batch_size, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, n_components, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-55">    </span>
<span id="cb8-56">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 성분별 기댓값 합산 (Sum over K)</span></span>
<span id="cb8-57">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># h_weighted_sum: (Batch, S, 1)</span></span>
<span id="cb8-58">    h_weighted_sum <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(h_out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> pi_expanded, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb8-59">    </span>
<span id="cb8-60">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4-2. 샘플링(S)에 대한 평균 (Mean over S)</span></span>
<span id="cb8-61">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># y_pred_expectation: (Batch, 1)</span></span>
<span id="cb8-62">    y_pred_expectation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mean(h_weighted_sum, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb8-63">    </span>
<span id="cb8-64">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Step 5] MSE Loss</span></span>
<span id="cb8-65">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># y: (Batch, 1)</span></span>
<span id="cb8-66">    loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mean((y.view(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> y_pred_expectation) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb8-67">    </span>
<span id="cb8-68">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> loss</span></code></pre></div></div>
</details>
</div>
</section>
<section id="training-loop" class="level3">
<h3 class="anchored" data-anchor-id="training-loop">Training Loop</h3>
<div id="training" class="cell" data-execution_count="8">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb9-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. Training</span></span>
<span id="cb9-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb9-4"></span>
<span id="cb9-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Stage 1] Treatment Network Training (Z + X -&gt; P distribution)</span></span>
<span id="cb9-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-8">treatment_net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> FirstStageMDN(</span>
<span id="cb9-9">    input_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], </span>
<span id="cb9-10">    num_gaussians<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span></span>
<span id="cb9-11">)</span>
<span id="cb9-12"></span>
<span id="cb9-13">opt1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.Adam(treatment_net.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>)</span>
<span id="cb9-14">epochs_stage1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb9-15"></span>
<span id="cb9-16"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Starting Stage 1 Training (Epochs: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epochs_stage1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)..."</span>)</span>
<span id="cb9-17"></span>
<span id="cb9-18">treatment_net.train()</span>
<span id="cb9-19"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs_stage1):</span>
<span id="cb9-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Forward Pass</span></span>
<span id="cb9-21">    pi, mu, sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> treatment_net(X, Z)</span>
<span id="cb9-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Loss Calculation (Negative Log Likelihood)</span></span>
<span id="cb9-23">    loss1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> first_stage_loss_fn(pi, mu, sigma, P)</span>
<span id="cb9-24">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Optimization</span></span>
<span id="cb9-25">    opt1.zero_grad()</span>
<span id="cb9-26">    loss1.backward()</span>
<span id="cb9-27">    opt1.step()</span>
<span id="cb9-28">    </span>
<span id="cb9-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb9-30">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"[Stage 1] Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epochs_stage1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">] | Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> | Avg Sigma: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sigma<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>mean()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>item()<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-31"></span>
<span id="cb9-32"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Transition] Freeze Stage 1</span></span>
<span id="cb9-34"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-35">treatment_net.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb9-36"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> param <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> treatment_net.parameters():</span>
<span id="cb9-37">    param.requires_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb9-38"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Stage 1 Freezed."</span>)</span>
<span id="cb9-39"></span>
<span id="cb9-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># [Stage 2] Outcome Network Training (Resampled P + X -&gt; Y)</span></span>
<span id="cb9-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># -----------------------------------------------------</span></span>
<span id="cb9-43">outcome_net <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SecondStageH(</span>
<span id="cb9-44">    x_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X.shape[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], </span>
<span id="cb9-45">    p_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, </span>
<span id="cb9-46">    output_dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb9-47">)</span>
<span id="cb9-48"></span>
<span id="cb9-49">opt2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> optim.Adam(outcome_net.parameters(), lr<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-4</span>)</span>
<span id="cb9-50">epochs_stage2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb9-51"></span>
<span id="cb9-52"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">Starting Stage 2 Training (Epochs: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epochs_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)..."</span>)</span>
<span id="cb9-53"></span>
<span id="cb9-54">outcome_net.train()</span>
<span id="cb9-55"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> epoch <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(epochs_stage2):</span>
<span id="cb9-56">    total_loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-57"></span>
<span id="cb9-58">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb9-59">        pi, mu, sigma <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> treatment_net(X, Z)</span>
<span id="cb9-60">    </span>
<span id="cb9-61">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2단계 Loss 계산</span></span>
<span id="cb9-62">    loss2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> second_stage_loss_fn(</span>
<span id="cb9-63">        model_h<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>outcome_net, </span>
<span id="cb9-64">        pi<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pi, </span>
<span id="cb9-65">        mu<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>mu, </span>
<span id="cb9-66">        sigma<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sigma, </span>
<span id="cb9-67">        x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>X, </span>
<span id="cb9-68">        y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Y, </span>
<span id="cb9-69">        num_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span> </span>
<span id="cb9-70">    )</span>
<span id="cb9-71">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Optimization</span></span>
<span id="cb9-72">    opt2.zero_grad()</span>
<span id="cb9-73">    loss2.backward()</span>
<span id="cb9-74">    opt2.step()</span>
<span id="cb9-75"></span>
<span id="cb9-76">    </span>
<span id="cb9-77">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> (epoch <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb9-78">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"[Stage 2] Epoch [</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epoch<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>epochs_stage2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">] | Loss: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>loss2<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.4f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb9-79">   </span>
<span id="cb9-80"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Deep IV Training Complete."</span>)</span></code></pre></div></div>
</details>
</div>
<hr>
</section>
</section>
<section id="result-visualization" class="level2">
<h2 class="anchored" data-anchor-id="result-visualization">5. Result Visualization</h2>
<p>학습된 모델이 실제 인과 효과 곡선(Ground Truth)을 얼마나 잘 복원했는지 확인합니다. 테스트는 <strong>성수기와 비성수기의 중간()</strong> 조건을 가정합니다.</p>
<div id="cell-visualization" class="cell" data-execution_count="9">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb10-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. Visualization &amp; Evaluation</span></span>
<span id="cb10-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ==========================================</span></span>
<span id="cb10-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. 테스트 데이터 생성</span></span>
<span id="cb10-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-7">p_min, p_max <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> P_data.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>(), P_data.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>()</span>
<span id="cb10-8">p_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.linspace(p_min, p_max, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">200</span>).reshape(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-9"></span>
<span id="cb10-10">fixed_x_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb10-11">x_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.full_like(p_test, fixed_x_val)</span>
<span id="cb10-12"></span>
<span id="cb10-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> true_structural_function(p_val, x_val):</span>
<span id="cb10-14">    threshold <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">35</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">40</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val)</span>
<span id="cb10-15">    base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">150</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> np.exp(<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> (p_val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> threshold)))</span>
<span id="cb10-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> base_effect <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> (<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> x_val)</span>
<span id="cb10-17">        </span>
<span id="cb10-18">true_y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> true_structural_function(p_test, x_test)</span>
<span id="cb10-19"></span>
<span id="cb10-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Linear 2SLS 예측</span></span>
<span id="cb10-22"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-23">px_test_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate((p_test, x_test), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb10-24">linear_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> stage2_model.predict(px_test_linear)</span>
<span id="cb10-25"></span>
<span id="cb10-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. DeepIV 예측</span></span>
<span id="cb10-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-29">outcome_net.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()</span>
<span id="cb10-30"></span>
<span id="cb10-31">p_test_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_p.transform(p_test)</span>
<span id="cb10-32">x_test_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_x.transform(x_test)</span>
<span id="cb10-33">p_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(p_test_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb10-34">x_tensor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.tensor(x_test_scaled, dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.float32)</span>
<span id="cb10-35"></span>
<span id="cb10-36"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb10-37">    y_pred_scaled <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outcome_net(p_tensor, x_tensor)    </span>
<span id="cb10-38">    deep_pred <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scaler_y.inverse_transform(y_pred_scaled.numpy())</span>
<span id="cb10-39"></span>
<span id="cb10-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-41"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. 최종 시각화</span></span>
<span id="cb10-42"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ===========================================================</span></span>
<span id="cb10-43">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>))</span>
<span id="cb10-44"></span>
<span id="cb10-45"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 배경: 관측 데이터</span></span>
<span id="cb10-46">plt.scatter(P_data, Y_data, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'gray'</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, s<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Observed Data'</span>)</span>
<span id="cb10-47"></span>
<span id="cb10-48"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 1. Ground Truth (검은 실선)</span></span>
<span id="cb10-49">plt.plot(p_test, true_y, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'k-'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ground Truth'</span>)</span>
<span id="cb10-50"></span>
<span id="cb10-51"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2. Linear 2SLS (빨간 점선)</span></span>
<span id="cb10-52">plt.plot(p_test, linear_pred, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'r--'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.5</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Linear 2SLS'</span>)</span>
<span id="cb10-53"></span>
<span id="cb10-54"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Deep IV (파란 실선)</span></span>
<span id="cb10-55">plt.plot(p_test, deep_pred, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b-'</span>, linewidth<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.5</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Deep IV'</span>)</span>
<span id="cb10-56"></span>
<span id="cb10-57">plt.title(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Causal Effect Estimation: Non-linear Pricing (at Seasonality X=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>fixed_x_val<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">)"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, weight<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bold'</span>)</span>
<span id="cb10-58">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Treatment: Ticket Price (P)"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>)</span>
<span id="cb10-59">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Outcome: Sales (Y)"</span>, fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>)</span>
<span id="cb10-60">plt.legend(fontsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">12</span>, loc<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'upper right'</span>, framealpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>)</span>
<span id="cb10-61">plt.grid(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, linestyle<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'--'</span>)</span>
<span id="cb10-62">plt.tight_layout()</span>
<span id="cb10-63">plt.show()</span></code></pre></div></div>
</details>
<div class="cell-output cell-output-display">
<div id="visualization" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/causal-inference-13-part-03/index_files/figure-html/visualization-output-1.png" width="1138" height="755" class="figure-img"></p>
<figcaption>Comparison of Causal Effect Estimation</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>결과 그래프에서 볼 수 있듯이:</p>
<ol type="1">
<li><strong>Linear 2SLS</strong>는 데이터의 비선형성을 무시하고 단순한 직선으로 효과를 추정하여, 가격 임계값 근처에서의 급격한 수요 변화를 포착하지 못합니다.</li>
<li><strong>Deep IV</strong>는 실제 인과 곡선(Ground Truth)인 S자 형태를 매우 정확하게 복원해냈습니다.</li>
</ol>
<p>이는 사회과학 연구에서 비선형성이 강하게 의심될 때, 딥러닝 기반의 인과추론 방법론이 강력한 대안이 될 수 있음을 시사합니다.</p>
<pre><code></code></pre>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/causal-inference-13-part-03/</guid>
  <pubDate>Sat, 17 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 2.1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-01-Classification-wiht-Adaptive-Prediction-Sets/</link>
  <description><![CDATA[ 





<section id="introduction-why-adaptive" class="level1">
<h1>Introduction: Why Adaptive?</h1>
<ul>
<li><p>이전 포스트(Section 1)에서 다룬 기본적인 Conformal Prediction 방법은 단순하고 강력하지만 한 가지 단점이 있습니다.</p></li>
<li><p>기존 방식(Naive method)은 단순히 <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Chat%7Bf%7D(x)_y">를 점수로 사용하기 때문에, 모든 클래스에 대해 고정된 임계값(Threshold)을 적용하는 경향이 있습니다.</p></li>
<li><p>이로 인해 다음과 같은 문제가 발생합니다:</p>
<ul>
<li><ol type="1">
<li><strong>Hard Inputs (어려운 이미지)</strong>: 모델이 헷갈려하는 경우에도 예측 집합이 충분히 커지지 않아 정답을 놓칠 수 있습니다 (Under-coverage).</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Easy Inputs (쉬운 이미지)</strong>: 모델이 확신하는 경우에도 예측 집합이 불필요하게 클 수 있습니다 (Over-coverage).</li>
</ol></li>
</ul></li>
<li><p>우리는 입력 이미지의 난이도에 따라 <strong>어려우면 집합을 크게, 쉬우면 집합을 작게</strong> 만드는 <strong>Adaptive Prediction Sets (APS)</strong> 기법을 도입하여 이 문제를 해결할 것입니다.</p></li>
</ul>
</section>
<section id="the-intuition-water-filling-approach" class="level1">
<h1>The Intuition: “Water-filling” Approach</h1>
<ul>
<li>APS의 핵심 아이디어는 직관적입니다.</li>
<li>만약 모델의 예측 확률 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">가 완벽하다면, 우리는 확률이 높은 클래스부터 순서대로 골라 담으면서 그 <strong>확률의 합(Cumulative Sum)이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> (예: 90%)를 넘기는 순간</strong> 멈추면 됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csum_%7Bj=1%7D%5E%7Bk%7D%20%5Chat%7Bf%7D(x)_%7B%5Cpi_j(x)%7D%20%5Cge%201%20-%20%5Calpha%0A"></p>
<ul>
<li><p>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">는 확률이 높은 순서대로 정렬된 클래스의 순열(Permutation)입니다.</p></li>
<li><p>이 방식은 “컵에 물(확률)을 <img src="https://latex.codecogs.com/png.latex?90%5C%25">가 찰 때까지 붓는 것”과 유사합니다.</p></li>
<li><p><strong>쉬운 문제</strong>: 확률 분포가 뾰족(Peaked)하므로, 1~2개만 담아도 금방 <img src="https://latex.codecogs.com/png.latex?90%5C%25">가 찹니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>Small Set</strong></p></li>
<li><p><strong>어려운 문제</strong>: 확률 분포가 평평(Flat)하므로, 여러 개를 담아야 <img src="https://latex.codecogs.com/png.latex?90%5C%25">가 찹니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>Large Set</strong></p></li>
<li><p>하지만 실제 모델의 확률 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">는 완벽하지 않으므로(Overconfident/Underconfident), 단순히 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">에서 끊으면 커버리지를 보장할 수 없습니다.</p></li>
<li><p>따라서 <strong>Conformal Prediction을 사용하여 이 “멈추는 지점(Threshold)”을 보정</strong>해야 합니다.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-01-Classification-wiht-Adaptive-Prediction-Sets/images/figure4_aps_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure 4: Adaptive Prediction Sets (APS) 알고리즘의 시각화. 확률이 높은 클래스(Fox squirrel, Gray fox…)부터 순서대로 누적 합을 구하고, 그 합이 보정된 분위수(Quantile)를 넘을 때까지 클래스를 포함시킨다.</figcaption>
</figure>
</div>
</section>
<section id="mathematical-formulation" class="level1">
<h1>Mathematical Formulation</h1>
<ul>
<li>이제 이를 수학적으로 엄밀하게 정의해보겠습니다.</li>
</ul>
<section id="defining-the-score-function" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-score-function">1. Defining the Score Function</h2>
<ul>
<li><p>APS를 위한 Score Function <img src="https://latex.codecogs.com/png.latex?s(x,%20y)">는 <strong>“정답 클래스 <img src="https://latex.codecogs.com/png.latex?y">를 포함시키기 위해, 확률 상위 몇 번째 클래스까지 내려가야 하는가?”</strong>를 누적 확률로 나타냅니다.</p></li>
<li><p>먼저, 입력 <img src="https://latex.codecogs.com/png.latex?x">에 대해 모델이 예측한 확률을 내림차순으로 정렬하는 순열 함수 <img src="https://latex.codecogs.com/png.latex?%5Cpi(x)">를 정의합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Chat%7Bf%7D(x)_%7B%5Cpi_1(x)%7D%20%5Cge%20%5Chat%7Bf%7D(x)_%7B%5Cpi_2(x)%7D%20%5Cge%20%5Cdots%20%5Cge%20%5Chat%7Bf%7D(x)_%7B%5Cpi_K(x)%7D%20"></p></li>
<li><p>이때, 정답 클래스 <img src="https://latex.codecogs.com/png.latex?y">가 정렬된 순서상 <img src="https://latex.codecogs.com/png.latex?k">번째에 위치한다고 가정합시다 (<img src="https://latex.codecogs.com/png.latex?y%20=%20%5Cpi_k(x)">).</p></li>
<li><p>Score <img src="https://latex.codecogs.com/png.latex?s(x,%20y)">는 <img src="https://latex.codecogs.com/png.latex?y">까지의 누적 확률 질량(Cumulative Probability Mass)으로 정의됩니다:</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0As(x,y)%20=%20%5Csum_%7Bj=1%7D%5E%7Bk%7D%20%5Chat%7Bf%7D(x)_%7B%5Cpi_j(x)%7D%0A"></p>
<ul>
<li><strong>의미</strong>: “모델이 가장 가능성 높다고 생각하는 것부터 정답 <img src="https://latex.codecogs.com/png.latex?y">가 나올 때까지 확률을 다 더한 값”입니다.
<ul>
<li>만약 모델이 정답을 1순위로 예측했다면, <img src="https://latex.codecogs.com/png.latex?s(x,y)">는 작을 것입니다.</li>
<li>만약 모델이 정답을 하위권으로 예측했다면, <img src="https://latex.codecogs.com/png.latex?s(x,y)">는 1에 가까워질 것입니다.</li>
</ul></li>
</ul>
</section>
<section id="calibration-finding-hatq" class="level2">
<h2 class="anchored" data-anchor-id="calibration-finding-hatq">2. Calibration (Finding <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">)</h2>
<ul>
<li>이제 Calibration 데이터셋 <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cdots,%20(X_n,%20Y_n)">에 대해 위 점수들을 계산합니다.</li>
<li>그리고 다음 식을 만족하는 분위수(Quantile) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 찾습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20;%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20%5Cright)%0A"></p>
<ul>
<li>이 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">는 “정답을 포함하기 위해 누적 확률을 어디까지 허용해야 하는가?”에 대한 통계적 임계값입니다.</li>
</ul>
</section>
<section id="constructing-the-prediction-set" class="level2">
<h2 class="anchored" data-anchor-id="constructing-the-prediction-set">3. Constructing the Prediction Set</h2>
<ul>
<li>새로운 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">에 대해 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X_%7Btest%7D)">는 누적 확률이 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 넘어서는 지점까지의 모든 클래스를 포함하여 구성됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5C%7B%20%5Cpi_1(x),%20%5Cdots,%20%5Cpi_k(x)%20%5C%7D%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?k">는 다음을 만족하는 가장 작은 정수입니다: <img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsup%7D%20%5Cleft%5C%7B%20k'%20:%20%5Csum_%7Bj=1%7D%5E%7Bk'%7D%20%5Chat%7Bf%7D(x)_%7B%5Cpi_j(x)%7D%20%3C%20%5Chat%7Bq%7D%20%5Cright%5C%7D%20+%201%0A"></li>
<li>즉, 누적 합이 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 초과하는 순간까지 포함합니다.</li>
</ul>
</section>
</section>
<section id="implementation-steps" class="level1">
<h1>Implementation Steps</h1>
<ul>
<li>Python 코드로 구현할 때의 핵심 로직은 다음과 같습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-01-Classification-wiht-Adaptive-Prediction-Sets/images/figure5_aps_python_code.png" class="img-fluid figure-img"></p>
<figcaption>Figure 5: Adaptive Prediction Sets 구현을 위한 Python 코드 예시. <code>argsort</code>를 통해 정렬하고 <code>cumsum</code>을 통해 누적 확률을 계산하는 과정이 포함되어 있다.</figcaption>
</figure>
</div>
<ol type="1">
<li><strong>Softmax &amp; Sort</strong>: 모델 출력값(Softmax)을 구하고 <code>argsort</code>를 이용해 내림차순 정렬합니다.</li>
<li><strong>Cumulative Sum</strong>: 정렬된 확률값들의 누적 합(<code>cumsum</code>)을 계산합니다.</li>
<li><strong>Calculate Scores (Calibration)</strong>: 정답 라벨 위치에서의 누적 합을 가져와 <img src="https://latex.codecogs.com/png.latex?s_i">를 구하고, Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산합니다.</li>
<li><strong>Prediction (Test)</strong>: 테스트 데이터의 누적 합이 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">보다 작거나 같은 클래스들을 선택합니다. (엄밀하게는 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 넘는 첫 번째 클래스까지 포함해야 함)</li>
</ol>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<ul>
<li><strong>Adaptive Prediction Sets (APS)</strong>는 불확실성을 더 지능적으로 다루는 방법입니다.</li>
<li>단순히 모델의 Softmax 값 하나만 보는 것이 아니라, <strong>전체 확률 분포의 형상(Shape)</strong>을 고려합니다.</li>
<li>그 결과, <strong>쉬운 샘플에는 작은 집합</strong>을, <strong>어려운 샘플에는 큰 집합</strong>을 할당하여 사용자가 모델의 신뢰도를 직관적으로 파악할 수 있게 해줍니다.</li>
<li>이 모든 과정에서도 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">라는 통계적 커버리지는 엄격하게 보장됩니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 다음 포스트에서는 연속적인 값을 예측하는 회귀(Regression) 문제에서 불확실성 구간을 구하는 <strong>Conformalized Quantile Regression</strong>에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-01-Classification-wiht-Adaptive-Prediction-Sets/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 2.2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-02-Conformalized-Quantile-Regression/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li><p>이전 포스트에서는 분류(Classification) 문제에 대한 Conformal Prediction을 다루었습니다.</p></li>
<li><p>이번에는 <strong>연속적인 값(Continuous Output)을 예측하는 회귀(Regression)</strong> 문제로 넘어가 보겠습니다.</p></li>
<li><p>회귀 문제에서 우리의 목표는 입력 <img src="https://latex.codecogs.com/png.latex?x">에 대해 단순히 하나의 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D">를 내놓는 것이 아니라, 정답 <img src="https://latex.codecogs.com/png.latex?y">가 포함될 확률이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> (예: 90%)인 <strong>예측 구간(Prediction Interval)</strong>을 생성하는 것입니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5B%5Ctext%7BLower%20Bound%7D,%20%5Ctext%7BUpper%20Bound%7D%5D%0A"></p>
<ul>
<li>이를 위해 가장 효과적인 베이스 모델 중 하나인 <strong>Quantile Regression</strong>을 사용하고, CP를 통해 이를 보정하는 <strong>Conformalized Quantile Regression (CQR)</strong> 기법을 알아보겠습니다.</li>
</ul>
</section>
<section id="base-model-quantile-regression" class="level1">
<h1>Base Model: Quantile Regression</h1>
<section id="concept" class="level2">
<h2 class="anchored" data-anchor-id="concept">Concept</h2>
<ul>
<li><p>일반적인 회귀 모델은 평균(Mean)을 예측(MSE Loss 사용)하지만, <strong>Quantile Regression</strong>은 조건부 분포의 특정 분위수(Quantile)를 예측합니다.</p></li>
<li><p>90%의 커버리지를 목표로 한다면, 우리는 양쪽 꼬리에서 5%씩을 제외한 구간을 알고 싶을 것입니다. 즉, 다음 두 가지 분위수를 학습합니다:</p>
<ul>
<li><strong>Lower Quantile</strong>: <img src="https://latex.codecogs.com/png.latex?%5Calpha/2%20=%200.05"> (5% 지점)</li>
<li><strong>Upper Quantile</strong>: <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Calpha/2%20=%200.95"> (95% 지점)</li>
</ul></li>
<li><p>모델이 완벽하다면, 정답 <img src="https://latex.codecogs.com/png.latex?y">는 90%의 확률로 이 구간 <img src="https://latex.codecogs.com/png.latex?%5B%5Chat%7Bt%7D_%7B%5Calpha/2%7D(x),%20%5Chat%7Bt%7D_%7B1-%5Calpha/2%7D(x)%5D"> 사이에 존재해야 합니다.</p></li>
</ul>
</section>
<section id="quantile-loss-pinball-loss" class="level2">
<h2 class="anchored" data-anchor-id="quantile-loss-pinball-loss">Quantile Loss (Pinball Loss)</h2>
<ul>
<li>이러한 분위수를 학습하기 위해 <strong>Quantile Loss (Pinball Loss)</strong>를 사용합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AL_%7B%5Cgamma%7D(%5Chat%7Bt%7D_%7B%5Cgamma%7D,%20y)%20=%20(y%20-%20%5Chat%7Bt%7D_%7B%5Cgamma%7D)%5Cgamma%20%5Cmathbb%7B1%7D%5C%7By%20%3E%20%5Chat%7Bt%7D_%7B%5Cgamma%7D%5C%7D%20+%20(%5Chat%7Bt%7D_%7B%5Cgamma%7D%20-%20y)(1-%5Cgamma)%5Cmathbb%7B1%7D%5C%7By%20%5Cle%20%5Chat%7Bt%7D_%7B%5Cgamma%7D%5C%7D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma">: 타겟 분위수 (예: 0.05 또는 0.95)</li>
<li>이 손실 함수를 사용하여 뉴럴 네트워크 등 어떤 모델이든 특정 분위수를 예측하도록 학습시킬 수 있습니다.</li>
</ul>
</section>
</section>
<section id="the-problem-why-conformalize" class="level1">
<h1>The Problem: Why Conformalize?</h1>
<ul>
<li><p>Quantile Regression으로 구한 구간 <img src="https://latex.codecogs.com/png.latex?%5B%5Chat%7Bt%7D_%7B0.05%7D(x),%20%5Chat%7Bt%7D_%7B0.95%7D(x)%5D">는 꽤 훌륭한 불확실성 추정치입니다.</p></li>
<li><p>하지만 문제는 <strong>“Finite Sample”에서 90% 커버리지를 보장하지 못한다</strong>는 점입니다.</p></li>
<li><p>모델이 과적합(Overfitting)되거나 학습이 덜 되면, 실제 정답이 이 구간을 벗어나는 비율이 10%보다 훨씬 클 수도, 작을 수도 있습니다.</p></li>
<li><p>따라서 우리는 Conformal Prediction을 사용하여 이 구간을 <strong>보정(Calibrate)</strong>해야 합니다.</p></li>
</ul>
</section>
<section id="conformalized-quantile-regression-algorithm" class="level1">
<h1>Conformalized Quantile Regression Algorithm</h1>
<ul>
<li>CQR의 핵심 아이디어는 학습된 분위수 구간을 <strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">만큼 늘리거나 줄여서</strong> 엄밀한 커버리지를 맞추는 것입니다.</li>
</ul>
<section id="step-1-define-score-function" class="level2">
<h2 class="anchored" data-anchor-id="step-1-define-score-function">Step 1: Define Score Function</h2>
<ul>
<li>Calibration 데이터 <img src="https://latex.codecogs.com/png.latex?(X_i,%20Y_i)">에 대해, 정답 <img src="https://latex.codecogs.com/png.latex?Y_i">가 학습된 구간 <img src="https://latex.codecogs.com/png.latex?%5B%5Chat%7Bt%7D_%7B%5Calpha/2%7D(X_i),%20%5Chat%7Bt%7D_%7B1-%5Calpha/2%7D(X_i)%5D"> 밖으로 얼마나 나갔는지를 측정하는 Score를 정의합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0As(x,%20y)%20=%20%5Cmax%20%5C%7B%20%5Chat%7Bt%7D_%7B%5Calpha/2%7D(x)%20-%20y,%20%5Cquad%20y%20-%20%5Chat%7Bt%7D_%7B1-%5Calpha/2%7D(x)%20%5C%7D%0A"></p>
<ul>
<li>이 식의 의미를 직관적으로 살펴봅시다:
<ul>
<li><strong>Case 1: <img src="https://latex.codecogs.com/png.latex?y">가 구간 안에 있을 때</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_%7B%5Ctext%7Blower%7D%7D%20%3C%20y%20%3C%20%5Chat%7Bt%7D_%7B%5Ctext%7Bupper%7D%7D"> 이므로, 두 항 모두 음수가 됩니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?s(x,%20y)%20%3C%200"> (즉, 안전함)</li>
</ul></li>
<li><strong>Case 2: <img src="https://latex.codecogs.com/png.latex?y">가 구간 밖(위쪽)에 있을 때</strong>:
<ul>
<li><img src="https://latex.codecogs.com/png.latex?y%20%3E%20%5Chat%7Bt%7D_%7B%5Ctext%7Bupper%7D%7D"> 이므로 <img src="https://latex.codecogs.com/png.latex?y%20-%20%5Chat%7Bt%7D_%7B%5Ctext%7Bupper%7D%7D"> 가 양수가 됩니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?s(x,%20y)%20%3E%200"> (즉, 위험함/에러)</li>
</ul></li>
</ul></li>
<li>즉, 점수 <img src="https://latex.codecogs.com/png.latex?s">가 클수록 정답이 예측 구간을 크게 벗어났음을 의미합니다.</li>
</ul>
</section>
<section id="step-2-calibration-get-hatq" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibration-get-hatq">Step 2: Calibration (Get <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">)</h2>
<ul>
<li>계산된 점수들 <img src="https://latex.codecogs.com/png.latex?s_1,%20%5Cdots,%20s_n">의 분포에서 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 찾습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20;%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20%5Cright)%0A"></p>
<ul>
<li>만약 모델이 불확실성을 과소평가했다면(구간이 너무 좁으면), 많은 <img src="https://latex.codecogs.com/png.latex?y">가 구간 밖에 있을 것이고 <img src="https://latex.codecogs.com/png.latex?s">값들이 커져서 <strong>양수의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"></strong>가 나옵니다.</li>
<li>만약 모델이 불확실성을 과대평가했다면(구간이 너무 넓으면), <img src="https://latex.codecogs.com/png.latex?s">값들이 대부분 음수일 것이고 <strong>음수의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"></strong>가 나옵니다.</li>
</ul>
</section>
<section id="step-3-construct-prediction-interval" class="level2">
<h2 class="anchored" data-anchor-id="step-3-construct-prediction-interval">Step 3: Construct Prediction Interval</h2>
<ul>
<li>최종적으로 새로운 입력 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">에 대한 예측 구간을 생성할 때, 원래 구간을 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">만큼 조정합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5B%5Chat%7Bt%7D_%7B%5Calpha/2%7D(X_%7Btest%7D)%20-%20%5Chat%7Bq%7D,%20%5Cquad%20%5Chat%7Bt%7D_%7B1-%5Calpha/2%7D(X_%7Btest%7D)%20+%20%5Chat%7Bq%7D%5D%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%20%3E%200">: 원래 구간이 너무 좁았으므로, 양쪽으로 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">만큼 <strong>넓힙니다</strong>.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%20%3C%200">: 원래 구간이 너무 넓었으므로, 양쪽으로 <img src="https://latex.codecogs.com/png.latex?%7C%5Chat%7Bq%7D%7C">만큼 <strong>좁힙니다</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-02-Conformalized-Quantile-Regression/images/figure6_cqr_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure 6: CQR 알고리즘의 시각화. 원래의 Quantile Regression 구간(파란색 영역)을 Calibration을 통해 얻은 상수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">만큼 확장하거나 축소하여 최종 Prediction Set을 생성한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<p>Python으로 CQR을 구현하는 것은 매우 간단합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-02-Conformalized-Quantile-Regression/images/figure7_cqr_python_code.png" class="img-fluid figure-img"></p>
<figcaption>Figure 7: Conformalized Quantile Regression 구현을 위한 Python 코드. Score를 계산하고 Quantile을 구하여 최종 구간을 조정하는 과정이 몇 줄의 코드로 구현된다.</figcaption>
</figure>
</div>
<ul>
<li><ol type="1">
<li><strong>Get Scores</strong>: Calibration 데이터에 대해 <code>max(lower - y, y - upper)</code>를 계산합니다.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Get Quantile</strong>: 위 점수들의 <img src="https://latex.codecogs.com/png.latex?(1-%5Calpha)"> 분위수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산합니다.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Deploy</strong>: 새로운 데이터의 Lower/Upper 예측값에 각각 <img src="https://latex.codecogs.com/png.latex?-%5Chat%7Bq%7D,%20+%5Chat%7Bq%7D">를 더해줍니다.</li>
</ol></li>
</ul>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<ul>
<li><p><strong>Conformalized Quantile Regression (CQR)</strong>은 회귀 문제에서 가장 널리 사용되는 최신 기법 중 하나입니다.</p></li>
<li><p><strong>Adaptivity</strong>: 입력값 <img src="https://latex.codecogs.com/png.latex?x">에 따라 구간의 길이(불확실성 크기)가 달라지는 Quantile Regression의 장점을 그대로 가집니다. (어려운 입력은 구간이 넓고, 쉬운 입력은 구간이 좁음)</p></li>
<li><p><strong>Validity</strong>: Conformal Prediction을 통해 통계적 커버리지를 엄밀하게 보장합니다.</p></li>
<li><p>이 방법은 단순한 MSE 기반 회귀보다 훨씬 풍부한 정보를 제공하며, 의료나 금융과 같이 리스크 관리가 중요한 분야에서 필수적인 도구입니다.</p></li>
</ul>
<hr>
<p><strong>Next Step</strong>: 다음 포스트에서는 회귀 문제에서 Quantile Regression을 사용하기 어려운 경우(예: 단순히 평균과 분산만 예측하는 경우)에 사용할 수 있는 <strong>Conformalizing Scalar Uncertainty Estimates</strong>에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-02-Conformalized-Quantile-Regression/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 2.3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-03-Conformalizing-Scalar-Uncertainty-Estimates/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li><p>이전 포스트에서 다룬 <strong>Conformalized Quantile Regression (CQR)</strong>은 매우 강력하지만, 두 개의 Quantile을 학습시켜야 한다는 조건이 있습니다.</p></li>
<li><p>하지만 실무에서는 종종 <strong>평균(<img src="https://latex.codecogs.com/png.latex?%5Cmu">)과 분산(<img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2">)</strong>만을 예측하는 더 단순한 모델을 사용하거나, 혹은 모델 앙상블의 분산 등을 불확실성 지표로 삼기도 합니다.</p></li>
<li><p>이번 포스트에서는 이러한 <strong>단일 스칼라 불확실성 지표(Scalar Uncertainty Estimate)</strong>를 활용하여 통계적으로 유효한 예측 구간을 생성하는 방법을 알아봅니다.</p></li>
</ul>
</section>
<section id="heuristic-uncertainty-the-estimated-standard-deviation" class="level1">
<h1>Heuristic Uncertainty: The Estimated Standard Deviation</h1>
<p>가장 흔한 시나리오는 데이터가 정규분포(Gaussian)를 따른다고 가정하고 모델을 학습시키는 것입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Y%20%7C%20X%20=%20x%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu(x),%20%5Csigma(x))%20"></p>
<ul>
<li><p>딥러닝 프레임워크(PyTorch 등)에서는 <code>GaussianNLLLoss</code>와 같은 손실 함수를 제공하여, 모델이 평균 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">와 불확실성 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D(x)">를 동시에 학습하도록 돕습니다.</p></li>
<li><p>하지만 실제 데이터는 <strong>정규분포를 따르지 않는 경우가 대부분</strong>입니다.</p></li>
<li><p>따라서 모델이 예측한 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D(x)">를 그대로 사용하여 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)%20%5Cpm%201.96%5Chat%7B%5Csigma%7D(x)">와 같은 구간을 만들면, 실제로는 95% 커버리지를 보장할 수 없습니다.</p></li>
<li><p>우리는 Conformal Prediction을 사용하여 이 부정확한(Heuristic) <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Csigma%7D(x)">를 <strong>보정(Calibrate)</strong>할 것입니다.</p></li>
</ul>
</section>
<section id="generalizing-uncertainty-scalars" class="level1">
<h1>Generalizing Uncertainty Scalars</h1>
<ul>
<li><p>이 방법은 비단 표준편차뿐만 아니라, <strong>“값이 클수록 불확실하다”</strong>는 의미를 가진 어떤 함수 <img src="https://latex.codecogs.com/png.latex?u(x)">에도 적용 가능합니다.</p></li>
<li><p>논문에서는 <img src="https://latex.codecogs.com/png.latex?u(x)">로 사용할 수 있는 다양한 예시를 제시합니다:</p>
<ul>
<li><ol type="1">
<li><strong>Residual Prediction</strong>: <img src="https://latex.codecogs.com/png.latex?%7Cy%20-%20%5Chat%7Bf%7D(x)%7C">를 예측하는 별도의 모델 학습.</li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>Ensemble Variance</strong>: 여러 모델 예측값들의 분산 측정.</li>
</ol></li>
<li><ol start="3" type="1">
<li><strong>Dropout Variance</strong>: 추론 시 Dropout을 켜고 여러 번 예측했을 때의 분산.</li>
</ol></li>
<li><ol start="4" type="1">
<li><strong>Input Perturbation</strong>: 입력에 작은 노이즈를 주었을 때 출력의 변화량.</li>
</ol></li>
</ul></li>
<li><p>우리는 이 <img src="https://latex.codecogs.com/png.latex?u(x)">를 “불확실성의 크기(Magnitude)”로 간주하고, 이를 스케일링하는 방식(Multiplicative Correction)을 사용합니다.</p></li>
</ul>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<ul>
<li>알고리즘의 핵심은 <strong>“실제 에러가 불확실성 지표 <img src="https://latex.codecogs.com/png.latex?u(x)"> 대비 몇 배나 큰가?”</strong>를 측정하는 것입니다.</li>
</ul>
<section id="step-1-define-score-function" class="level2">
<h2 class="anchored" data-anchor-id="step-1-define-score-function">Step 1: Define Score Function</h2>
<ul>
<li>Calibration 데이터 <img src="https://latex.codecogs.com/png.latex?(X_i,%20Y_i)">에 대해 다음과 같은 Score를 정의합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0As(x,%20y)%20=%20%5Cfrac%7B%7Cy%20-%20%5Chat%7Bf%7D(x)%7C%7D%7Bu(x)%7D%0A"></p>
<ul>
<li><strong>분자 <img src="https://latex.codecogs.com/png.latex?%7Cy%20-%20%5Chat%7Bf%7D(x)%7C"></strong>: 실제 모델의 예측 오차(절대값)입니다.</li>
<li><strong>분모 <img src="https://latex.codecogs.com/png.latex?u(x)"></strong>: 모델이 스스로 추정한 불확실성입니다.</li>
<li><strong>의미</strong>: “모델이 예상한 불확실성 대비 실제 오차의 비율”입니다.
<ul>
<li>만약 모델이 불확실하다고 판단(<img src="https://latex.codecogs.com/png.latex?u(x)"> 큼)했는데 오차도 크다면, <img src="https://latex.codecogs.com/png.latex?s">는 적절한 값을 가집니다.</li>
<li>만약 모델이 확실하다고 판단(<img src="https://latex.codecogs.com/png.latex?u(x)"> 작음)했는데 오차가 크다면, <img src="https://latex.codecogs.com/png.latex?s">는 매우 커집니다 (Penalty).</li>
</ul></li>
</ul>
</section>
<section id="step-2-calibration-get-hatq" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibration-get-hatq">Step 2: Calibration (Get <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">)</h2>
<ul>
<li>계산된 점수들 <img src="https://latex.codecogs.com/png.latex?s_1,%20%5Cdots,%20s_n">에 대해 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수(Quantile) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 구합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20;%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20%5Cright)%0A"></p>
<ul>
<li>여기서 구해진 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">는 <strong>“불확실성 지표 <img src="https://latex.codecogs.com/png.latex?u(x)">에 곱해야 할 보정 계수(Multiplier)”</strong> 역할을 합니다.</li>
</ul>
</section>
<section id="step-3-construct-prediction-interval" class="level2">
<h2 class="anchored" data-anchor-id="step-3-construct-prediction-interval">Step 3: Construct Prediction Interval</h2>
<ul>
<li>새로운 입력 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">에 대한 예측 구간은 중심 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">에서 불확실성 지표 <img src="https://latex.codecogs.com/png.latex?u(x)">의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">배만큼 벌려준 구간이 됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5B%5Chat%7Bf%7D(x)%20-%20%5Chat%7Bq%7Du(x),%20%5Cquad%20%5Chat%7Bf%7D(x)%20+%20%5Chat%7Bq%7Du(x)%5D%0A"></p>
<section id="유도-과정-derivation-of-validity" class="level3">
<h3 class="anchored" data-anchor-id="유도-과정-derivation-of-validity">유도 과정 (Derivation of Validity)</h3>
<ul>
<li>이 구간이 왜 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 커버리지를 보장하는지 살펴보겠습니다.</li>
</ul>
<ol type="1">
<li>Calibration 단계에서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 구했으므로, 새로운 데이터에 대해 다음 확률이 성립합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Bs(X_%7Btest%7D,%20Y_%7Btest%7D)%20%5Cle%20%5Chat%7Bq%7D%5D%20%5Cge%201%20-%20%5Calpha%20"></li>
<li>Score <img src="https://latex.codecogs.com/png.latex?s">의 정의를 대입합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Cleft%5B%20%5Cfrac%7B%7CY_%7Btest%7D%20-%20%5Chat%7Bf%7D(X_%7Btest%7D)%7C%7D%7Bu(X_%7Btest%7D)%7D%20%5Cle%20%5Chat%7Bq%7D%20%5Cright%5D%20%5Cge%201%20-%20%5Calpha%20"></li>
<li>양변에 <img src="https://latex.codecogs.com/png.latex?u(X_%7Btest%7D)">를 곱합니다 (불확실성은 항상 양수이므로 부등호 유지). <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Cleft%5B%20%7CY_%7Btest%7D%20-%20%5Chat%7Bf%7D(X_%7Btest%7D)%7C%20%5Cle%20%5Chat%7Bq%7Du(X_%7Btest%7D)%20%5Cright%5D%20%5Cge%201%20-%20%5Calpha%20"></li>
<li>절대값을 풉니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Cleft%5B%20-%5Chat%7Bq%7Du(X_%7Btest%7D)%20%5Cle%20Y_%7Btest%7D%20-%20%5Chat%7Bf%7D(X_%7Btest%7D)%20%5Cle%20%5Chat%7Bq%7Du(X_%7Btest%7D)%20%5Cright%5D%20%5Cge%201%20-%20%5Calpha%20"></li>
<li><img src="https://latex.codecogs.com/png.latex?Y_%7Btest%7D">에 대해 정리하면 최종 구간이 도출됩니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Cleft%5B%20%5Chat%7Bf%7D(X_%7Btest%7D)%20-%20%5Chat%7Bq%7Du(X_%7Btest%7D)%20%5Cle%20Y_%7Btest%7D%20%5Cle%20%5Chat%7Bf%7D(X_%7Btest%7D)%20+%20%5Chat%7Bq%7Du(X_%7Btest%7D)%20%5Cright%5D%20%5Cge%201%20-%20%5Calpha%20"></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-03-Conformalizing-Scalar-Uncertainty-Estimates/images/figure8_scalar_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure 8: Uncertainty Scalar를 이용한 예측 구간 시각화. 중심 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">를 기준으로, 위아래로 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7Du(x)">만큼 벌어진 대칭적인 구간을 형성한다.</figcaption>
</figure>
</div>
</section>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<ul>
<li>이 방법은 구현이 매우 간단하며, PyTorch 등의 라이브러리와 쉽게 결합됩니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-03-Conformalizing-Scalar-Uncertainty-Estimates/images/figure9_scalar_python_code.png" class="img-fluid figure-img"></p>
<figcaption>Figure 9: Conformalized Uncertainty Scalars 구현을 위한 Python 코드. Score를 계산할 때 예측된 표준편차(model output의 두 번째 컬럼)로 나누어주는 것이 핵심이다.</figcaption>
</figure>
</div>
<ol type="1">
<li><strong>Get Scores</strong>: 에러의 절대값을 예측된 표준편차(또는 불확실성 지표)로 나눕니다.</li>
<li><strong>Get Quantile</strong>: 점수들의 분위수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산합니다.</li>
<li><strong>Deploy</strong>: 예측값 <img src="https://latex.codecogs.com/png.latex?%5Cpm%20(%5Ctext%7B%EB%B6%88%ED%99%95%EC%8B%A4%EC%84%B1%7D%20%5Ctimes%20%5Chat%7Bq%7D)">를 통해 구간을 생성합니다.</li>
</ol>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<ul>
<li>이 방법은 <strong>Symmetric(대칭적)</strong>인 구간을 생성한다는 특징이 있습니다.
<ul>
<li><strong>장점</strong>: 구현이 쉽고 직관적입니다. 이미 학습된 모델(Gaussian Output 등)을 그대로 재활용하기 좋습니다.</li>
<li><strong>단점</strong>: CQR과 달리 구간이 항상 예측값을 중심으로 대칭입니다. 실제 데이터 분포가 비대칭(Skewed)이라면 효율적이지 않을 수 있습니다. 또한, <img src="https://latex.codecogs.com/png.latex?u(x)">가 실제 분위수와 비례하지 않는다면 CQR보다 성능(구간의 평균 길이 등)이 떨어질 수 있습니다.</li>
</ul></li>
<li>따라서 가능하다면 <strong>Quantile Regression (CQR)</strong>을 사용하는 것이 더 좋지만, 상황이 여의치 않거나 빠른 배포가 필요할 때 이 방법은 훌륭한 대안이 됩니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 지금까지는 Black-box 모델의 불확실성을 다루었습니다. 다음 포스트에서는 이 기법들이 실제 복잡한 문제(Computer Vision 등)에서 어떻게 확장되는지 살펴보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-03-Conformalizing-Scalar-Uncertainty-Estimates/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 2.4)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-04-Conformalizing-Bayes/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Bayesian Neural Network와 같은 베이지안 모델들은 불확실성 정량화(Uncertainty Quantification) 분야에서 매우 매력적인 도구입니다. 이들은 사전 지식(Prior)을 반영할 수 있고, 예측의 결과물로 단일 값이 아닌 분포(Posterior Predictive Density)를 제공하기 때문입니다.</p>
<p>하지만 베이지안 모델에는 치명적인 약점이 있습니다. <strong>“모델의 가정(Prior, Likelihood function 등)이 완벽하게 맞아야만”</strong> 예측된 불확실성이 정확하다는 점입니다. 현실의 복잡한 데이터에서 이러한 가정이 완벽히 들어맞기는 어렵습니다.</p>
<p><strong>Conformalizing Bayes</strong>는 베이지안 모델의 정보량을 그대로 활용하되, <strong>Conformal Prediction을 통해 “가정이 틀렸더라도” 통계적 커버리지를 보장</strong>하는 강력한 방법론입니다.</p>
</section>
<section id="the-bayesian-ideal-vs.-reality" class="level1">
<h1>The Bayesian Ideal vs.&nbsp;Reality</h1>
<section id="ideal-scenario" class="level2">
<h2 class="anchored" data-anchor-id="ideal-scenario">Ideal Scenario</h2>
<p>만약 우리가 만든 베이지안 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(y%7Cx)"> (입력 <img src="https://latex.codecogs.com/png.latex?x">가 주어졌을 때 <img src="https://latex.codecogs.com/png.latex?y">의 사후 확률 밀도)가 완벽하다면, 최적의 예측 집합 <img src="https://latex.codecogs.com/png.latex?S(x)">는 단순히 밀도 함수(Density)가 높은 영역을 잘라내어 만들 수 있습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS(x)%20=%20%5C%7B%20y%20:%20%5Chat%7Bf%7D(y%7Cx)%20%3E%20t%20%5C%7D%0A"></p>
<p>여기서 임계값 <img src="https://latex.codecogs.com/png.latex?t">는 해당 영역의 적분값이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">가 되도록 설정합니다 (<img src="https://latex.codecogs.com/png.latex?%5Cint_%7By%20%5Cin%20S(x)%7D%20%5Chat%7Bf%7D(y%7Cx)%20dy%20=%201-%5Calpha">). 이를 <strong>Highest Posterior Density (HPD) Region</strong>이라고도 합니다.</p>
</section>
<section id="reality" class="level2">
<h2 class="anchored" data-anchor-id="reality">Reality</h2>
<p>하지만 우리는 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D">가 완벽하다고 보장할 수 없습니다. 따라서 위 방식으로 구한 집합은 실제로는 90%를 커버하지 못할 수도(Under-coverage), 너무 넓을 수도(Over-coverage) 있습니다.</p>
<p>우리는 베이지안 모델의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(y%7Cx)">를 <strong>“진짜 확률”이 아니라 “유용한 불확실성 점수(Heuristic Score)”로 간주</strong>하고, CP를 적용하여 올바른 임계값(Threshold)을 찾을 것입니다.</p>
</section>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<p>Conformalizing Bayes의 절차는 우리가 익숙한 CP의 흐름을 그대로 따릅니다.</p>
<section id="step-1-define-score-function" class="level2">
<h2 class="anchored" data-anchor-id="step-1-define-score-function">Step 1: Define Score Function</h2>
<p>우리는 모델이 예측한 <strong>사후 확률 밀도(Posterior Predictive Density)</strong>가 높을수록 에러가 작다(확실하다)고 봅니다. Conformal Score는 “불확실한 정도”를 나타내야 하므로, 밀도 함수의 <strong>음수(Negative)</strong>를 취합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0As(x,%20y)%20=%20-%20%5Chat%7Bf%7D(y%7Cx)%0A"></p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(y%7Cx)">가 높음 (모델이 정답을 확신함) <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> Score <img src="https://latex.codecogs.com/png.latex?s">는 매우 작은 음수.</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(y%7Cx)">가 낮음 (모델이 정답을 예측 못함) <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> Score <img src="https://latex.codecogs.com/png.latex?s">는 큰 값(0에 가까운 값 혹은 양수).</li>
</ul>
</section>
<section id="step-2-calibration-finding-hatq" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibration-finding-hatq">Step 2: Calibration (Finding <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">)</h2>
<p>Calibration 데이터 <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cdots,%20(X_n,%20Y_n)">에 대해 점수들을 계산하고, <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 찾습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20;%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20%5Cright)%0A"></p>
<p>여기서 구한 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">는 <strong>“밀도 함수를 어디서 잘라야(Thresholding) 하는가?”</strong>에 대한 보정된 기준선이 됩니다.</p>
</section>
<section id="step-3-construct-prediction-set" class="level2">
<h2 class="anchored" data-anchor-id="step-3-construct-prediction-set">Step 3: Construct Prediction Set</h2>
<p>새로운 입력 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">에 대해, Score가 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> 이하인(즉, 밀도가 <img src="https://latex.codecogs.com/png.latex?-%5Chat%7Bq%7D"> 이상인) 모든 <img src="https://latex.codecogs.com/png.latex?y">를 포함합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5C%7B%20y%20:%20s(x,%20y)%20%5Cle%20%5Chat%7Bq%7D%20%5C%7D%20=%20%5C%7B%20y%20:%20-%5Chat%7Bf%7D(y%7Cx)%20%5Cle%20%5Chat%7Bq%7D%20%5C%7D%0A"></p>
<p>이를 정리하면 최종 예측 집합은 다음과 같습니다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5C%7B%20y%20:%20%5Chat%7Bf%7D(y%7Cx)%20%5Cge%20-%5Chat%7Bq%7D%20%5C%7D%0A"></p>
<p>즉, <strong>보정된 임계값 <img src="https://latex.codecogs.com/png.latex?-%5Chat%7Bq%7D">보다 확률 밀도가 높은 모든 <img src="https://latex.codecogs.com/png.latex?y">의 집합(Superlevel Set)</strong>을 구성합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-04-Conformalizing-Bayes/images/figure9_bayes_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure 9: Conformalized Bayes 알고리즘 시각화. 사후 확률 밀도 함수(Posterior Predictive Density) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(y%7Cx)">를 Conformal Prediction으로 구한 임계값 <img src="https://latex.codecogs.com/png.latex?-%5Chat%7Bq%7D">에서 잘라(Slicing), 그 위의 영역을 예측 집합으로 삼는다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="mathematical-derivation-validity" class="level1">
<h1>Mathematical Derivation &amp; Validity</h1>
<p>이 집합이 왜 유효한지(Valid) 수학적으로 살펴보겠습니다.</p>
<ol type="1">
<li><p><strong>Calibration Guarantee</strong>: Calibration 단계에서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 구했으므로, 새로운 i.i.d. 샘플에 대해 다음이 성립합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5Bs(X_%7Btest%7D,%20Y_%7Btest%7D)%20%5Cle%20%5Chat%7Bq%7D%5D%20%5Cge%201%20-%20%5Calpha%20"></p></li>
<li><p><strong>Substitution</strong>: Score의 정의 <img src="https://latex.codecogs.com/png.latex?s(x,y)%20=%20-%5Chat%7Bf%7D(y%7Cx)">를 대입합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5B-%5Chat%7Bf%7D(Y_%7Btest%7D%20%7C%20X_%7Btest%7D)%20%5Cle%20%5Chat%7Bq%7D%5D%20%5Cge%201%20-%20%5Calpha%20"></p></li>
<li><p><strong>Inequality Rearrangement</strong>: 부등식의 양변에 -1을 곱하여 부등호 방향을 바꿉니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D%5B%5Chat%7Bf%7D(Y_%7Btest%7D%20%7C%20X_%7Btest%7D)%20%5Cge%20-%5Chat%7Bq%7D%5D%20%5Cge%201%20-%20%5Calpha%20"></p></li>
<li><p><strong>Conclusion</strong>: 따라서, 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5C%7B%20y%20:%20%5Chat%7Bf%7D(y%7CX_%7Btest%7D)%20%5Cge%20-%5Chat%7Bq%7D%20%5C%7D">는 정답 <img src="https://latex.codecogs.com/png.latex?Y_%7Btest%7D">를 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 확률로 포함합니다.</p></li>
</ol>
</section>
<section id="why-is-this-useful-bayes-optimality" class="level1">
<h1>Why is this useful? (Bayes Optimality)</h1>
<p>이 방법은 단순히 유효할(Valid) 뿐만 아니라, 특정 조건 하에서 <strong>효율적(Efficient)</strong>입니다. 논문에 따르면, 만약 원래의 베이지안 모델이 (비록 틀렸을지라도) 어느 정도 합리적이라면, 이 방법으로 생성된 예측 집합은 <strong><img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 커버리지를 만족하는 모든 예측 집합 중에서 평균 크기(Average Size)가 가장 작습니다 (Bayes Optimal).</strong></p>
<p>이는 Neyman-Pearson Lemma와 유사한 논리로, “가장 확률 밀도가 높은 곳부터 담는 것”이 주어진 확률 질량을 채우면서 집합의 크기(Volume)를 최소화하는 전략이기 때문입니다.</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<ul>
<li><strong>Conformalizing Bayes</strong>는 베이지안 모델의 확률 밀도 함수를 Score로 사용하여 CP를 적용하는 기법입니다.</li>
<li>결과적으로 <strong>“Superlevel Set of Density”</strong> 형태의 예측 집합을 얻게 됩니다.</li>
<li>이 방법은 베이지안 모델의 가정이 틀려도 <strong>Coverage를 보장</strong>하며, 동시에 베이지안 모델의 정보량을 활용하여 <strong>집합의 크기를 최적화</strong>할 수 있습니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 지금까지 Section 2를 통해 Classification, Regression, Bayesian Model 등 다양한 환경에서의 CP 적용법을 배웠습니다. 다음 포스트에서는 이러한 CP 알고리즘들이 제대로 작동하는지 검증하는 <strong>Section 3. Evaluating Conformal Prediction</strong>에 대해 다루겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-02-Examples-of-Conformal-Procedures/Part-02-04-Conformalizing-Bayes/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 3.1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-01-Evaluating-Adaptivity/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>지금까지 우리는 Conformal Prediction(CP)을 통해 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">의 커버리지를 보장하는 예측 집합을 만드는 법을 배웠습니다. 하지만 <strong>“평균적으로 90% 정답을 포함한다(Marginal Coverage)”</strong>는 사실만으로는 충분하지 않습니다.</p>
<p>예를 들어, 어떤 의사가 쉬운 환자에게는 100% 정확한 진단을 내리지만, 어려운 희귀병 환자에게는 0%의 정확도를 보인다면 어떨까요? 전체 평균으로는 90% 정확도일지 몰라도, 이는 좋은 시스템이라 할 수 없습니다.</p>
<p>좋은 CP 알고리즘은 <strong>쉬운 입력에는 작은 집합(Small Sets)</strong>을, <strong>어려운 입력에는 큰 집합(Large Sets)</strong>을 출력해야 합니다. 이를 <strong>Adaptivity(적응성)</strong>라고 합니다. 이번 포스트에서는 내 모델이 얼마나 “적응적(Adaptive)”인지 평가하는 구체적인 지표들을 알아봅니다.</p>
</section>
<section id="metric-1-set-size-distribution" class="level1">
<h1>Metric 1: Set Size Distribution</h1>
<p>가장 먼저 확인해야 할 지표는 예측 집합 크기(Set Size)의 분포입니다. 단순히 평균 크기만 보는 것이 아니라, <strong>히스토그램</strong>을 그려봐야 합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-01-Evaluating-Adaptivity/images/set_size_histogram.png" class="img-fluid figure-img"></p>
<figcaption>Placeholder: Histogram of Set Sizes. X축은 집합의 크기, Y축은 빈도수를 나타낸다. 분포가 넓게 퍼져 있을수록 다양한 난이도의 입력을 잘 구별하고 있다는 의미이다.</figcaption>
</figure>
</div>
<ol type="1">
<li><strong>Average Set Size</strong>: 작을수록 좋습니다. (단, <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 커버리지를 만족하는 전제하에)
<ul>
<li>평균이 너무 크다면? <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 모델 성능이 나쁘거나, Score Function이 효율적이지 않음을 의미합니다.</li>
</ul></li>
<li><strong>Spread of Set Sizes</strong>: 분포가 넓을수록(Dynamic Range가 클수록) 좋습니다.
<ul>
<li>분포가 넓다는 것은 모델이 “확실한 것(크기 1)”과 “불확실한 것(크기 5 이상)”을 잘 구분하고 있다는 뜻입니다.</li>
</ul></li>
</ol>
</section>
<section id="metric-2-conditional-coverage" class="level1">
<h1>Metric 2: Conditional Coverage</h1>
<p>우리가 궁극적으로 원하는 것은 모든 개별 입력 <img src="https://latex.codecogs.com/png.latex?X">에 대해 커버리지를 보장하는 <strong>Conditional Coverage</strong>입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D%5BY_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%7C%20X_%7Btest%7D%5D%20%5Cge%201%20-%20%5Calpha%0A"></p>
<p>하지만 현실적으로 모든 <img src="https://latex.codecogs.com/png.latex?X">값 하나하나에 대해 이를 검증하는 것은 불가능합니다(Impossible in general case). 대신 우리는 데이터를 의미 있는 <strong>그룹(Group)</strong>으로 나누어, 각 그룹별로 커버리지가 잘 지켜지는지 확인해야 합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-01-Evaluating-Adaptivity/images/figure10_conditional_coverage.png" class="img-fluid figure-img"></p>
<figcaption>Figure 10: Marginal Coverage vs Conditional Coverage 비교. (좌측) 커버리지가 지켜지지 않음. (중앙) Marginal Coverage는 만족하지만 특정 그룹(Group 2)에서 에러가 집중됨. (우측) Conditional Coverage는 모든 그룹에서 고르게 에러가 분산됨.</figcaption>
</figure>
</div>
<p>위 그림의 중앙(Marginal) 케이스를 봅시다. * <strong>Group 1 (Easy)</strong>: 100% 커버리지 (과잉) * <strong>Group 2 (Hard)</strong>: 80% 커버리지 (부족) * <strong>전체 평균</strong>: 90% 커버리지 (만족)</p>
<p>이런 경우 “Marginal Coverage는 만족하지만, Conditional Coverage는 실패했다”고 합니다. 이를 잡아내기 위해 다음 두 가지 지표를 사용합니다.</p>
<section id="feature-stratified-coverage-fsc" class="level2">
<h2 class="anchored" data-anchor-id="feature-stratified-coverage-fsc">2.1 Feature-stratified Coverage (FSC)</h2>
<p>데이터의 특성(Feature)에 따라 그룹을 나누어 커버리지를 측정하는 방법입니다. 예를 들어 인종(Race), 나이대(Age), 혹은 이미지의 밝기 등으로 데이터를 그룹화(<img src="https://latex.codecogs.com/png.latex?g=1,%20%5Cdots,%20G">)합니다.</p>
<p>각 그룹 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BI%7D_g">에 속한 데이터들의 커버리지를 계산하고, <strong>가장 성능이 안 좋은 그룹(Minimum Coverage)</strong>을 찾습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BFSC%20metric%7D%20=%20%5Cmin_%7Bg%20%5Cin%20%5C%7B1,%20%5Cdots,%20G%5C%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Cmathcal%7BI%7D_g%7C%7D%20%5Csum_%7Bi%20%5Cin%20%5Cmathcal%7BI%7D_g%7D%20%5Cmathbb%7BI%7D%5C%7B%20Y_i%20%5Cin%20%5Cmathcal%7BC%7D(X_i)%20%5C%7D%0A"></p>
<ul>
<li>만약 완벽한 Conditional Coverage라면, 이 값은 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">에 근접해야 합니다.</li>
<li>이 값이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">보다 현저히 낮다면, 특정 그룹(예: 야간 주행 이미지)에서 모델이 실패하고 있음을 의미합니다.</li>
</ul>
</section>
<section id="size-stratified-coverage-ssc" class="level2">
<h2 class="anchored" data-anchor-id="size-stratified-coverage-ssc">2.2 Size-stratified Coverage (SSC)</h2>
<p>하지만 어떤 Feature가 중요한지 모를 때는 어떻게 할까요? 이때 사용할 수 있는 아주 일반적이고 강력한 방법이 <strong>“예측 집합의 크기”</strong>로 그룹을 나누는 것입니다.</p>
<ul>
<li>그룹 1: 집합 크기가 1인 데이터들 (<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(x)%7C%20=%201">)</li>
<li>그룹 2: 집합 크기가 2인 데이터들</li>
<li>…</li>
</ul>
<p>모델이 “이건 정말 어려워서 후보를 10개나 뽑았어”라고 말한 경우(집합 크기 10), 실제로도 그 안에 정답이 90% 확률로 들어 있어야 합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BSSC%20metric%7D%20=%20%5Cmin_%7Bg%20%5Cin%20%5C%7B%20%5Ctext%7Bset%20sizes%7D%20%5C%7D%7D%20%5Cfrac%7B1%7D%7B%7C%5Cmathcal%7BI%7D_g%7C%7D%20%5Csum_%7Bi%20%5Cin%20%5Cmathcal%7BI%7D_g%7D%20%5Cmathbb%7BI%7D%5C%7B%20Y_i%20%5Cin%20%5Cmathcal%7BC%7D(X_i)%20%5C%7D%0A"></p>
<p>이 지표는 사용자가 사전에 그룹을 정의할 필요가 없으므로(Feature-agnostic), 어떤 상황에서도 바로 적용해볼 수 있는 훌륭한 진단 도구입니다.</p>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>Conformal Prediction을 평가할 때는 단순히 전체 커버리지만 보지 말고, 다음을 꼭 확인하세요:</p>
<ol type="1">
<li><strong>Histogram of Set Sizes</strong>: 쉬운 건 작게, 어려운 건 크게 잘 구분하고 있는가?</li>
<li><strong>Stratified Coverage (FSC / SSC)</strong>: 특정 그룹(인종, 나이, 혹은 모델이 불확실해하는 그룹)에서만 커버리지가 깨지고 있지는 않은가?</li>
</ol>
<p>이러한 <strong>Adaptivity</strong> 체크는 실험실 환경을 넘어 실제 서비스(Production)에 CP를 적용할 때 “Non-negotiable(타협할 수 없는)” 필수 검증 단계입니다.</p>
<hr>
<p><strong>Next Step</strong>: 다음 포스트에서는 이러한 평가 지표들을 실제로 계산할 때 필요한 데이터셋 크기(Calibration Set Size)와 통계적 검증 방법에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-01-Evaluating-Adaptivity/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 3.2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-02-The-Effect-of-the-Size-of-the-Calibration-Set/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Conformal Prediction(CP)을 실제 서비스에 배포할 때, 엔지니어가 가장 먼저 마주하는 실무적인 질문은 이것입니다.</p>
<blockquote class="blockquote">
<p><strong>“Calibration Set(<img src="https://latex.codecogs.com/png.latex?n">)의 크기는 얼마나 커야 할까요? 100개면 충분한가요, 아니면 1,000개가 필요한가요?”</strong></p>
</blockquote>
<p>이론적으로 CP의 커버리지 보장(<img src="https://latex.codecogs.com/png.latex?1-%5Calpha">)은 <img src="https://latex.codecogs.com/png.latex?n">의 크기와 상관없이 성립합니다(Finite-sample guarantee). 하지만 직관적으로 생각했을 때, 데이터가 많을수록 더 안정적일 것이라 예상할 수 있습니다.</p>
<p>이번 포스트에서는 <strong>Calibration Set의 크기 <img src="https://latex.codecogs.com/png.latex?n">이 예측 구간의 안정성(Stability)에 미치는 영향</strong>을 수학적으로 분석하고, 실무적인 가이드라인(<img src="https://latex.codecogs.com/png.latex?n%20%5Capprox%201000">)을 제시합니다.</p>
</section>
<section id="validity-vs.-stability" class="level1">
<h1>Validity vs.&nbsp;Stability</h1>
<section id="the-theoretical-guarantee-validity" class="level2">
<h2 class="anchored" data-anchor-id="the-theoretical-guarantee-validity">The Theoretical Guarantee (Validity)</h2>
<p>놀랍게도, CP의 커버리지 보장 정리(Theorem 1)는 <strong>모든 <img src="https://latex.codecogs.com/png.latex?n">에 대해 성립</strong>합니다. Calibration 데이터가 단 10개뿐이라도, 새로운 데이터에 대한 평균 커버리지는 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 만족합니다.</p>
</section>
<section id="the-catch-stability" class="level2">
<h2 class="anchored" data-anchor-id="the-catch-stability">The Catch (Stability)</h2>
<p>하지만 여기에는 중요한 디테일이 숨어 있습니다. 우리가 보장하는 것은 <strong>“Calibration Set을 무한히 새로 뽑았을 때의 평균”</strong>입니다.</p>
<p>하지만 현실에서 우리는 <strong>단 하나의 고정된 Calibration Set</strong>을 사용합니다. 만약 우리가 운이 나빠서 이상한(Bias된) 데이터가 섞인 Calibration Set을 뽑았다면 어떨까요? 이 고정된 데이터셋으로 학습된 CP 모델을 무한한 테스트 데이터에 적용했을 때의 실제 커버리지는 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">와 정확히 일치하지 않을 수 있습니다.</p>
<p>즉, <strong>Calibration Set 자체의 무작위성(Randomness)</strong> 때문에 실제 커버리지는 <strong>확률 변수(Random Quantity)</strong>가 됩니다.</p>
</section>
</section>
<section id="mathematical-derivation-beta-distribution" class="level1">
<h1>Mathematical Derivation: Beta Distribution</h1>
<p>Vladimir Vovk는 고정된 Calibration Set이 주어졌을 때, 무한한 검증 데이터에 대한 커버리지 확률 분포가 <strong>베타 분포(Beta Distribution)</strong>를 따른다는 것을 증명했습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%5Cmid%20%5C%7B(X_i,%20Y_i)%5C%7D_%7Bi=1%7D%5En)%20%5Csim%20%5Ctext%7BBeta%7D(n+1-l,%20l)%0A"></p>
<p>여기서 파라미터 <img src="https://latex.codecogs.com/png.latex?l">은 다음과 같이 정의됩니다: <img src="https://latex.codecogs.com/png.latex?%0Al%20=%20%5Clfloor%20(n+1)%5Calpha%20%5Crfloor%0A"></p>
<ul>
<li><strong>의미</strong>: 이 식은 <img src="https://latex.codecogs.com/png.latex?n">이 커질수록 커버리지 확률 분포가 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 중심으로 얼마나 뾰족하게(Sharp) 모이는지를 설명합니다.</li>
<li><strong>평균</strong>: 베타 분포의 성질에 의해 이 분포의 기댓값은 정확히 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">가 됩니다 (Validity).</li>
<li><strong>분산</strong>: <img src="https://latex.codecogs.com/png.latex?n">이 작을수록 분산이 커져서, 실제 커버리지가 목표치 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">에서 크게 벗어날 확률이 높아집니다.</li>
</ul>
</section>
<section id="visualizing-the-effect-of-n" class="level1">
<h1>Visualizing the Effect of <img src="https://latex.codecogs.com/png.latex?n"></h1>
<p>이 현상을 시각적으로 확인해보겠습니다. 아래 그래프는 Calibration Set의 크기 <img src="https://latex.codecogs.com/png.latex?n">에 따른 커버리지 확률의 밀도 함수를 보여줍니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-02-The-Effect-of-the-Size-of-the-Calibration-Set/images/figure11_coverage_distribution.png" class="img-fluid figure-img"></p>
<figcaption>Figure 11: 무한한 Validation Set에 대한 커버리지 분포. <img src="https://latex.codecogs.com/png.latex?n">이 100일 때는 분포가 넓게 퍼져 있지만, <img src="https://latex.codecogs.com/png.latex?n">이 10,000일 때는 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha(0.9)"> 근처에 매우 좁게 집중된다. 분포는 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E%7B-1/2%7D)">의 속도로 수렴한다.</figcaption>
</figure>
</div>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?n=100"> (파란색)</strong>: 그래프가 넓게 퍼져 있습니다. 운이 나쁘면 실제 커버리지가 85%나 95%가 될 수도 있습니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?n=1,000"> (주황색)</strong>: 그래프가 훨씬 좁아졌습니다. 대부분의 경우 커버리지가 <strong>88% ~ 92%</strong> 사이로 유지됩니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?n=10,000"> (초록색)</strong>: 매우 뾰족합니다. 거의 정확하게 90%를 맞춥니다.</li>
</ul>
</section>
<section id="practical-guideline-the-n1000-rule" class="level1">
<h1>Practical Guideline: The “n=1000” Rule</h1>
<p>그렇다면 실무에서는 몇 개를 써야 할까요? 논문에서는 <strong><img src="https://latex.codecogs.com/png.latex?n=1000"> 정도면 대부분의 목적에 충분하다</strong>고 제안합니다.</p>
<ol type="1">
<li><strong>안정성 확보</strong>: 위 그래프에서 보듯이, <img src="https://latex.codecogs.com/png.latex?n=1000">일 때 커버리지는 목표치(<img src="https://latex.codecogs.com/png.latex?1-%5Calpha">)에서 <img src="https://latex.codecogs.com/png.latex?%5Cpm%202%5C%25"> 내외의 오차 범위를 가집니다. 이는 대부분의 머신러닝 애플리케이션에서 허용 가능한 수준입니다.</li>
<li><strong>비용 효율성</strong>: 데이터를 10,000개까지 늘려도 얻을 수 있는 이득(오차 감소)은 크지 않습니다. (수렴 속도가 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BO%7D(n%5E%7B-1/2%7D)">로 느려지기 때문)</li>
</ol>
<section id="required-sample-size-calculation" class="level2">
<h2 class="anchored" data-anchor-id="required-sample-size-calculation">Required Sample Size Calculation</h2>
<p>만약 더 엄격한 기준(예: 99% 확률로 오차 1% 이내)이 필요하다면, 다음 표를 참고하여 필요한 <img src="https://latex.codecogs.com/png.latex?n">을 역산할 수 있습니다.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">허용 오차 (<img src="https://latex.codecogs.com/png.latex?%5Cepsilon">)</th>
<th style="text-align: left;">필요한 <img src="https://latex.codecogs.com/png.latex?n"> (신뢰도 90%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.1 (10%)</td>
<td style="text-align: left;">22</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.05 (5%)</td>
<td style="text-align: left;">102</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.01 (1%)</td>
<td style="text-align: left;">2,491</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.005 (0.5%)</td>
<td style="text-align: left;">9,812</td>
</tr>
</tbody>
</table>
<p>일반적인 기준인 90% 신뢰도(<img src="https://latex.codecogs.com/png.latex?%5Cdelta=0.1">)에서 목표 커버리지와의 오차를 1%(<img src="https://latex.codecogs.com/png.latex?%5Cepsilon=0.01">) 이내로 줄이려면 약 <strong>2,500개</strong>의 데이터가 필요합니다. 하지만 5% 오차(<img src="https://latex.codecogs.com/png.latex?%5Cepsilon=0.05">)를 허용한다면 <strong>102개</strong>로도 충분합니다.</p>
</section>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<ul>
<li>Conformal Prediction은 <img src="https://latex.codecogs.com/png.latex?n">이 작아도 평균적으로는 커버리지를 보장합니다.</li>
<li>하지만 <strong>개별 Calibration Set에 대한 신뢰도(Stability)</strong>를 높이기 위해서는 충분한 <img src="https://latex.codecogs.com/png.latex?n">이 필요합니다.</li>
<li><strong>Rule of Thumb</strong>: 약 <strong>1,000개</strong>의 Calibration 데이터를 사용하면, 실제 커버리지가 목표치에서 크게 벗어나지 않음(약 <img src="https://latex.codecogs.com/png.latex?%5Cpm%202%5C%25">)을 확신할 수 있습니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 이제 적절한 Calibration 데이터 크기도 알았으니, 실제로 우리가 구현한 CP 알고리즘이 올바른지 검증하는 구체적인 절차인 <strong>Section 3.3 Checking for Correct Coverage</strong>에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-02-The-Effect-of-the-Size-of-the-Calibration-Set/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 3.3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-03-Checking-for-Correct-Coverage/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Conformal Prediction(CP)을 구현했다면, 가장 먼저 해야 할 일은 <strong>“이게 정말 작동하는가?”</strong>를 확인하는 것입니다. 즉, 우리가 설정한 목표 커버리지(예: 90%)가 실제 테스트 데이터에서도 지켜지는지 검증해야 합니다.</p>
<p>하지만 단순히 한 번의 테스트 셋 결과만 보고 “90.1%니까 성공!”이라고 단정 짓기는 어렵습니다. 데이터의 무작위성 때문에 우연히 잘 나왔을 수도, 우연히 못 나왔을 수도 있기 때문입니다. 따라서 우리는 <strong>여러 번의 실험(Trials)</strong>을 통해 커버리지 분포를 확인해야 합니다.</p>
</section>
<section id="methodology-repeated-experiments" class="level1">
<h1>Methodology: Repeated Experiments</h1>
<p>가장 확실한 검증 방법은 <img src="https://latex.codecogs.com/png.latex?R">번의 독립적인 실험을 수행하는 것입니다. 각 실험 <img src="https://latex.codecogs.com/png.latex?j=1,%20%5Cdots,%20R">마다 새로운 Calibration Set과 Validation Set을 준비하고, 다음 과정을 반복합니다:</p>
<ol type="1">
<li>Calibration 수행 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D_j"> 계산</li>
<li>Validation Set에 대해 예측 집합 구성 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D_j"></li>
<li><strong>경험적 커버리지(Empirical Coverage)</strong> <img src="https://latex.codecogs.com/png.latex?C_j"> 계산:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0AC_j%20=%20%5Cfrac%7B1%7D%7Bn_%7Bval%7D%7D%20%5Csum_%7Bi=1%7D%5E%7Bn_%7Bval%7D%7D%20%5Cmathbb%7BI%7D%20%5C%7B%20Y_%7Bi,j%7D%5E%7B(val)%7D%20%5Cin%20%5Cmathcal%7BC%7D_j(X_%7Bi,j%7D%5E%7B(val)%7D)%20%5C%7D%0A"></p>
<p>이렇게 얻은 <img src="https://latex.codecogs.com/png.latex?R">개의 커버리지 값들 <img src="https://latex.codecogs.com/png.latex?C_1,%20%5Cdots,%20C_R">의 평균 <img src="https://latex.codecogs.com/png.latex?%5Coverline%7BC%7D">는 이론적으로 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">에 매우 근접해야 합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Coverline%7BC%7D%20=%20%5Cfrac%7B1%7D%7BR%7D%20%5Csum_%7Bj=1%7D%5E%7BR%7D%20C_j%20%5Capprox%201%20-%20%5Calpha%0A"></p>
<p>또한, <img src="https://latex.codecogs.com/png.latex?C_j">들의 히스토그램을 그렸을 때 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 중심으로 종 모양(Bell-curve) 분포를 보여야 합니다.</p>
</section>
<section id="the-practical-challenge-limited-data" class="level1">
<h1>The Practical Challenge: Limited Data</h1>
<p>현실적인 문제는 <strong>“매번 새로운 데이터를 어디서 구하는가?”</strong>입니다. 우리가 가진 데이터는 유한(총 <img src="https://latex.codecogs.com/png.latex?n_%7Btotal%7D%20=%20n%20+%20n_%7Bval%7D">)하므로, <img src="https://latex.codecogs.com/png.latex?R">번이나 새로운 데이터를 수집할 수는 없습니다.</p>
<p>따라서 우리는 <strong>Resampling (Random Split)</strong> 방식을 사용합니다. 전체 데이터를 무작위로 섞어서 Calibration/Validation 셋으로 나누는 과정을 <img src="https://latex.codecogs.com/png.latex?R">번 반복하는 것입니다.</p>
<section id="efficiency-trick-score-caching" class="level2">
<h2 class="anchored" data-anchor-id="efficiency-trick-score-caching">Efficiency Trick: Score Caching</h2>
<p>하지만 <img src="https://latex.codecogs.com/png.latex?R">번(예: 100번)이나 모델을 다시 학습시키거나 추론(Inference)을 돌리는 것은 계산 비용이 매우 큽니다. 여기서 중요한 팁은 <strong>Conformal Score를 미리 계산해두는 것(Caching)</strong>입니다.</p>
<p>CP 알고리즘은 <strong>Score값(<img src="https://latex.codecogs.com/png.latex?s_i">)들의 순위</strong>에만 의존합니다. 데이터가 어느 셋(Calibration vs Validation)에 속하느냐에 따라 역할만 달라질 뿐, 각 데이터 포인트의 Score 값 자체는 변하지 않습니다.</p>
<p>따라서 다음과 같이 효율적으로 검증할 수 있습니다:</p>
<ol type="1">
<li><strong>Pre-computation</strong>: 전체 데이터에 대해 Score를 미리 한 번만 계산합니다.</li>
<li><strong>Shuffle &amp; Split</strong>: 계산된 Score 배열만 무작위로 섞어서 나눕니다.</li>
<li><strong>Evaluate</strong>: 나누어진 Score들로 Quantile을 구하고 커버리지를 계산합니다.</li>
</ol>
<p>이 방식을 사용하면 딥러닝 모델을 매번 돌릴 필요가 없어 검증 속도가 수백 배 빨라집니다.</p>
</section>
</section>
<section id="implementation" class="level1">
<h1>Implementation</h1>
<p>Python 코드로 이를 구현하면 다음과 같습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-03-Checking-for-Correct-Coverage/images/figure12_coverage_check_code.png" class="img-fluid figure-img"></p>
<figcaption>Figure 12: Score Caching을 이용한 효율적인 커버리지 검증 코드. 전체 Score를 미리 계산해두고(<code>get_scores</code>), 반복문 안에서는 단순히 배열을 섞고(<code>shuffle</code>) 자르는 연산만 수행한다.</figcaption>
</figure>
</div>
<section id="code-explanation" class="level3">
<h3 class="anchored" data-anchor-id="code-explanation">Code Explanation</h3>
<ol type="1">
<li><strong>Load/Compute Scores</strong>: <code>get_scores(X, Y)</code>를 통해 모든 데이터의 Score를 계산하고 저장합니다.</li>
<li><strong>Loop <img src="https://latex.codecogs.com/png.latex?R"> times</strong>:
<ul>
<li><code>np.random.shuffle(scores)</code>: Score들을 섞습니다.</li>
<li><code>calib, val = scores[:n], scores[n:]</code>: <img src="https://latex.codecogs.com/png.latex?n">개는 Calibration용, 나머지는 검증용으로 나눕니다.</li>
<li><code>qhat</code>: Calibration Score들로 Quantile을 계산합니다.</li>
<li><code>mean()</code>: Validation Score들이 <code>qhat</code>보다 작은 비율(Coverage)을 계산합니다.</li>
</ul></li>
<li><strong>Check</strong>: <code>coverages.mean()</code>이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">와 비슷한지 확인하고, 히스토그램을 그립니다.</li>
</ol>
</section>
</section>
<section id="interpretation-of-results" class="level1">
<h1>Interpretation of Results</h1>
<p>검증 결과 커버리지가 정확히 90.00%가 나오지 않더라도 당황할 필요는 없습니다. <img src="https://latex.codecogs.com/png.latex?n"> (Calibration 크기), <img src="https://latex.codecogs.com/png.latex?n_%7Bval%7D"> (Validation 크기), <img src="https://latex.codecogs.com/png.latex?R"> (반복 횟수)이 모두 유한하기 때문에 <strong>약간의 변동(Benign Fluctuations)</strong>은 자연스러운 현상입니다.</p>
<ul>
<li><strong>정상</strong>: 평균이 0.89 ~ 0.91 사이이며 히스토그램이 0.9 근처에 모여 있음.</li>
<li><strong>비정상</strong>: 평균이 0.80처럼 현저히 낮거나, 히스토그램이 한쪽으로 크게 치우침 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 구현 오류(버그) 혹은 데이터 분포의 문제(i.i.d. 위반 등)를 의심해야 합니다.</li>
</ul>
<p>이 진단 과정을 통과했다면, 여러분의 Conformal Predictor는 통계적으로 신뢰할 수 있는 상태입니다.</p>
<hr>
<p><strong>Next Step</strong>: 지금까지는 표준적인 환경에서의 CP를 다루었습니다. 다음 포스트부터는 <strong>Section 4. Extensions of Conformal Prediction</strong>으로 넘어가서, 데이터 불균형, 시계열, 분포 변화 등 더 복잡하고 현실적인 문제들을 해결하는 방법을 알아보겠습니다. 첫 번째로 <strong>Group-Balanced Conformal Prediction</strong>을 다룹니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-03-Evaluating-Conformal-Prediction/Part-03-03-Checking-for-Correct-Coverage/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-01-Group-Balanced-Conformal-Prediction/</link>
  <description><![CDATA[ 





<section id="introduction-the-fairness-problem" class="level1">
<h1>Introduction: The Fairness Problem</h1>
<p>지금까지 우리는 전체 데이터셋에 대해 평균적으로 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">의 커버리지를 보장하는 방법(Marginal Coverage)을 배웠습니다. 하지만 현실 세계, 특히 의료나 금융 같은 민감한 분야에서는 “평균적인 성공”만으로는 충분하지 않습니다.</p>
<p>예를 들어, 어떤 질병 진단 AI가 백인 환자에게는 99%의 정확도를 보이지만, 유색 인종 환자에게는 80%의 정확도밖에 보이지 않는다고 가정해봅시다. 이 경우 백인 환자가 다수라면 전체 평균 정확도는 90%를 넘길 수 있겠지만, 이는 <strong>공정하지 못한(Unfair)</strong> 시스템입니다.</p>
<p><strong>Group-Balanced Conformal Prediction</strong>은 이러한 문제를 해결하기 위해, 데이터 내의 특정 그룹(인종, 성별, 연령대 등) 각각에 대해 독립적으로 커버리지를 보장하는 기법입니다.</p>
</section>
<section id="problem-formulation" class="level1">
<h1>Problem Formulation</h1>
<p>우리의 입력 데이터 <img src="https://latex.codecogs.com/png.latex?X">의 첫 번째 특성(Feature) <img src="https://latex.codecogs.com/png.latex?X_%7Bi,1%7D">이 그룹을 나타내는 범주형 변수라고 가정해봅시다. 이 그룹은 <img src="https://latex.codecogs.com/png.latex?%5C%7B1,%20%5Cdots,%20G%5C%7D"> 중 하나의 값을 가집니다.</p>
<p>기존의 CP는 다음을 보장했습니다: <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cge%201-%5Calpha%20"></p>
<p>하지만 우리가 원하는 것은 <strong>모든 그룹 <img src="https://latex.codecogs.com/png.latex?g">에 대해</strong> 다음이 성립하는 것입니다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%5Cmid%20X_%7Btest,1%7D%20=%20g)%20%5Cge%201-%5Calpha,%20%5Cquad%20%5Cforall%20g%20%5Cin%20%5C%7B1,%20%5Cdots,%20G%5C%7D%0A"></p>
<p>즉, 어떤 그룹에 속한 데이터가 들어오더라도 똑같이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 이상의 확률로 정답을 포함해야 합니다.</p>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<p>이 문제를 해결하는 방법은 직관적입니다. <strong>“그룹별로 따로따로 Conformal Prediction을 수행”</strong>하는 것입니다.</p>
<section id="step-1-stratify-calibration-data" class="level2">
<h2 class="anchored" data-anchor-id="step-1-stratify-calibration-data">Step 1: Stratify Calibration Data</h2>
<p>Calibration 데이터셋을 그룹별로 나눕니다. 그룹 <img src="https://latex.codecogs.com/png.latex?g">에 속하는 데이터들만 모아서 부분집합을 만듭니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS%5E%7B(g)%7D%20=%20%5C%7B%20(X_j,%20Y_j)%20:%20X_%7Bj,1%7D%20=%20g%20%5C%7D%0A"></p>
</section>
<section id="step-2-calibrate-per-group" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibrate-per-group">Step 2: Calibrate per Group</h2>
<p>각 그룹 <img src="https://latex.codecogs.com/png.latex?g">에 대해 독립적으로 Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(g)%7D">를 계산합니다.</p>
<ol type="1">
<li>그룹 <img src="https://latex.codecogs.com/png.latex?g"> 데이터에 대한 Conformal Score들을 계산합니다: <img src="https://latex.codecogs.com/png.latex?s%5E%7B(g)%7D_1,%20%5Cdots,%20s%5E%7B(g)%7D_%7Bn%5E%7B(g)%7D%7D"></li>
<li>해당 그룹의 데이터 개수 <img src="https://latex.codecogs.com/png.latex?n%5E%7B(g)%7D">를 기준으로 보정된 분위수를 구합니다:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%5E%7B(g)%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n%5E%7B(g)%7D+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%5E%7B(g)%7D%7D%20;%20%5C%7Bs%5E%7B(g)%7D_1,%20%5Cdots,%20s%5E%7B(g)%7D_%7Bn%5E%7B(g)%7D%7D%5C%7D%20%5Cright)%0A"></p>
<p>결과적으로 우리는 그룹의 개수만큼 서로 다른 임계값(Threshold) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(1)%7D,%20%5Cdots,%20%5Chat%7Bq%7D%5E%7B(G)%7D">를 얻게 됩니다. * 모델이 잘 맞추는 쉬운 그룹은 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">가 작을 것이고 (작은 예측 집합), * 모델이 어려워하는 그룹은 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">가 클 것입니다 (큰 예측 집합).</p>
</section>
<section id="step-3-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-3-inference">Step 3: Inference</h2>
<p>새로운 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 들어오면, 먼저 이 데이터가 어느 그룹에 속하는지(<img src="https://latex.codecogs.com/png.latex?X_%7Btest,1%7D">) 확인합니다. 그리고 해당 그룹에 맞는 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(X_%7Btest,1%7D)%7D">을 사용하여 예측 집합을 구성합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5C%7B%20y%20:%20s(x,%20y)%20%5Cle%20%5Chat%7Bq%7D%5E%7B(x_1)%7D%20%5C%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-01-Group-Balanced-Conformal-Prediction/images/group_balanced_cp_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Group-Balanced Conformal Prediction의 개념도. 전체 데이터를 파란색 그룹과 초록색 그룹으로 나누고, 각각의 분포(Histogram)에서 별도의 Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(1)%7D,%20%5Chat%7Bq%7D%5E%7B(2)%7D">를 계산하여 적용한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="theoretical-guarantee" class="level1">
<h1>Theoretical Guarantee</h1>
<p>이 방법은 Vovk에 의해 처음 제안되었으며, 다음의 명제에 의해 수학적으로 정당화됩니다.</p>
<blockquote class="blockquote">
<p><strong>Proposition 1 (Error control guarantee for group-balanced conformal prediction)</strong></p>
<p>데이터가 i.i.d. 가정하에 추출되었다면, 위 알고리즘을 통해 생성된 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">는 모든 그룹 <img src="https://latex.codecogs.com/png.latex?g">에 대해 다음을 만족한다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%5Cmid%20X_%7Btest,1%7D%20=%20g)%20%5Cge%201-%5Calpha%20"></p>
</blockquote>
<p>이로써 우리는 인종, 성별 등 민감한 속성에 관계없이 모든 사용자에게 <strong>동등한 수준의 안전장치(Equal Coverage)</strong>를 제공할 수 있게 됩니다.</p>
</section>
<section id="practical-note" class="level1">
<h1>Practical Note</h1>
<ul>
<li><strong>Explicit Groups</strong>: 인종, 성별처럼 데이터에 명시적으로 존재하는 카테고리를 사용할 수 있습니다.</li>
<li><strong>Constructed Groups (Binning)</strong>: 나이(Age)와 같은 연속형 변수라도, ‘20대’, ‘30대’ 등으로 구간화(Binning)하여 그룹을 만든 뒤 이 방법을 적용할 수 있습니다. 이를 통해 연속적인 특성에 대한 Conditional Coverage를 근사할 수 있습니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 그룹 정보가 입력(Input Feature)으로 주어지는 경우에는 위 방법을 쓰면 됩니다. 하지만, <strong>“실제 정답 클래스(Class)”별로 커버리지를 보장하고 싶다면</strong> 어떻게 해야 할까요? (예: 암 환자를 암이라고 맞출 확률과 정상인을 정상이라고 맞출 확률을 동시에 보장). 다음 포스트에서는 <strong>Section 4.2 Class-Conditional Conformal Prediction</strong>을 다루겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-01-Group-Balanced-Conformal-Prediction/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-02-Class-Conditional-Conformal-Prediction/</link>
  <description><![CDATA[ 





<section id="introduction-the-problem-with-imbalanced-classes" class="level1">
<h1>Introduction: The Problem with Imbalanced Classes</h1>
<p>머신러닝 분류 문제, 특히 의료 진단과 같은 분야에서는 <strong>클래스 불균형(Class Imbalance)</strong>이 흔하게 발생합니다. 예를 들어, 암 진단 모델을 개발한다고 가정해봅시다. * <strong>정상(Normal)</strong>: 데이터의 95% * <strong>암(Cancer)</strong>: 데이터의 5%</p>
<p>우리가 일반적인 Conformal Prediction을 사용하여 95% 커버리지를 달성했다고 칩시다. 가장 쉬운 달성 방법은 무엇일까요? <strong>“그냥 모든 환자를 ’정상’이라고 예측하고, 암 환자는 다 틀리는 것”</strong>입니다. 이렇게 해도 (정상 95% + 암 0%) / 100% <img src="https://latex.codecogs.com/png.latex?%5Capprox"> 95% 커버리지는 달성됩니다.</p>
<p>하지만 이는 재앙입니다. 우리는 암 환자에 대해서도 똑같이 95%의 정확도로 정답을 포함시키기를 원합니다. <strong>Class-Conditional Conformal Prediction</strong>은 바로 이 문제, 즉 <strong>모든 정답 클래스(Ground Truth Class)</strong>에 대해 균등한 커버리지를 보장하기 위한 방법입니다.</p>
</section>
<section id="problem-formulation" class="level1">
<h1>Problem Formulation</h1>
<p>우리의 목표는 단순한 전체 평균(<img src="https://latex.codecogs.com/png.latex?1-%5Calpha">)이 아니라, <strong>각 클래스 <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5C%7B1,%20%5Cdots,%20K%5C%7D"> 별로</strong> 조건부 커버리지를 만족하는 것입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%5Cmid%20Y_%7Btest%7D%20=%20y)%20%5Cge%201-%5Calpha,%20%5Cquad%20%5Cforall%20y%20%5Cin%20%5C%7B1,%20%5Cdots,%20K%5C%7D%0A"></p>
<p>이것이 보장된다면, “암 환자” 그룹 내에서도 정답이 예측 집합에 포함될 확률이 95% 이상이 되고, “정상인” 그룹 내에서도 마찬가지가 됩니다.</p>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<p>알고리즘은 Group-Balanced CP와 유사하게 <strong>“따로따로(Separately)”</strong> 전략을 취하지만, 추론(Inference) 단계에서 중요한 차이가 있습니다.</p>
<section id="step-1-stratify-calibration-data-by-class" class="level2">
<h2 class="anchored" data-anchor-id="step-1-stratify-calibration-data-by-class">Step 1: Stratify Calibration Data by Class</h2>
<p>Calibration 데이터셋을 실제 정답 클래스(Ground Truth Class)별로 나눕니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS%5E%7B(k)%7D%20=%20%5C%7B%20(X_j,%20Y_j)%20:%20Y_j%20=%20k%20%5C%7D%0A"></p>
</section>
<section id="step-2-calibrate-per-class" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibrate-per-class">Step 2: Calibrate per Class</h2>
<p>각 클래스 <img src="https://latex.codecogs.com/png.latex?k">에 대해 독립적으로 Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(k)%7D">를 계산합니다.</p>
<ol type="1">
<li>클래스 <img src="https://latex.codecogs.com/png.latex?k">에 속하는 데이터들의 Score <img src="https://latex.codecogs.com/png.latex?s%5E%7B(k)%7D_1,%20%5Cdots,%20s%5E%7B(k)%7D_%7Bn%5E%7B(k)%7D%7D">를 모읍니다.</li>
<li>해당 클래스의 데이터 수 <img src="https://latex.codecogs.com/png.latex?n%5E%7B(k)%7D">를 기준으로 보정된 분위수를 구합니다:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%5E%7B(k)%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n%5E%7B(k)%7D+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%5E%7B(k)%7D%7D%20;%20%5C%7Bs%5E%7B(k)%7D_1,%20%5Cdots,%20s%5E%7B(k)%7D_%7Bn%5E%7B(k)%7D%7D%5C%7D%20%5Cright)%0A"></p>
<p>결과적으로 우리는 클래스 개수만큼의 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(1)%7D,%20%5Cdots,%20%5Chat%7Bq%7D%5E%7B(K)%7D">를 얻습니다. * 샘플이 적거나 모델이 어려워하는 클래스(예: 암)는 임계값이 높게(보수적으로) 설정될 것입니다.</p>
</section>
<section id="step-3-inference-iterative-check" class="level2">
<h2 class="anchored" data-anchor-id="step-3-inference-iterative-check">Step 3: Inference (Iterative Check)</h2>
<p>이 부분이 4.1절(Group-Balanced)과 가장 다릅니다. 테스트 시점에는 입력 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">의 <strong>진짜 클래스(True Class)가 무엇인지 모릅니다.</strong> 따라서 “해당 그룹의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 가져다 쓰는” 방식은 불가능합니다.</p>
<p>대신, 우리는 <strong>“만약 정답이 클래스 <img src="https://latex.codecogs.com/png.latex?y">라면?”</strong>이라는 가정을 모든 후보 클래스에 대해 수행합니다.</p>
<p>예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(x)">는 다음 조건을 만족하는 모든 클래스 <img src="https://latex.codecogs.com/png.latex?y">를 포함합니다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5C%7B%20y%20:%20s(x,%20y)%20%5Cle%20%5Chat%7Bq%7D%5E%7B(y)%7D%20%5C%7D%0A"></p>
<ul>
<li>후보 클래스 <img src="https://latex.codecogs.com/png.latex?y">가 ‘암’이라면? <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <img src="https://latex.codecogs.com/png.latex?s(x,%20%5Ctext%7B%EC%95%94%7D)">을 계산하고, 이를 ’암’ 클래스의 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(%5Ctext%7B%EC%95%94%7D)%7D">과 비교합니다.</li>
<li>후보 클래스 <img src="https://latex.codecogs.com/png.latex?y">가 ‘정상’이라면? <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <img src="https://latex.codecogs.com/png.latex?s(x,%20%5Ctext%7B%EC%A0%95%EC%83%81%7D)">을 계산하고, 이를 ’정상’ 클래스의 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(%5Ctext%7B%EC%A0%95%EC%83%81%7D)%7D">과 비교합니다.</li>
</ul>
<p>각 클래스마다 <strong>자신만의 기준(Threshold)</strong>을 통과해야 집합에 들어갈 수 있는 것입니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-02-Class-Conditional-Conformal-Prediction/images/class_conditional_cp_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Class-Conditional Conformal Prediction의 개념도. Calibration 데이터를 색깔(클래스)별로 나누어 각각의 분포 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(1)%7D,%20%5Chat%7Bq%7D%5E%7B(2)%7D">를 구한다. 추론 시에는 각 후보 클래스 <img src="https://latex.codecogs.com/png.latex?y">가 자신의 기준 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(y)%7D">를 만족하는지 확인한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="comparison-group-balanced-vs.-class-conditional" class="level1">
<h1>Comparison: Group-Balanced vs.&nbsp;Class-Conditional</h1>
<p>이 두 가지 확장의 차이를 명확히 구분하는 것이 중요합니다.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">특징</th>
<th style="text-align: left;">Group-Balanced (4.1)</th>
<th style="text-align: left;">Class-Conditional (4.2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>기준 (Condition)</strong></td>
<td style="text-align: left;">입력 특성 (Input Feature <img src="https://latex.codecogs.com/png.latex?X_%7B:,1%7D">)</td>
<td style="text-align: left;">출력 라벨 (Output Label <img src="https://latex.codecogs.com/png.latex?Y">)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>정보 가용성</strong></td>
<td style="text-align: left;">테스트 시점에 <img src="https://latex.codecogs.com/png.latex?X">를 통해 그룹을 <strong>알 수 있음</strong></td>
<td style="text-align: left;">테스트 시점에 <img src="https://latex.codecogs.com/png.latex?Y">를 <strong>알 수 없음</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>적용 방식</strong></td>
<td style="text-align: left;">그룹을 확인하고 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 해당 그룹의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> 적용</td>
<td style="text-align: left;">모든 <img src="https://latex.codecogs.com/png.latex?y">에 대해 순회하며 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 각자의 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5E%7B(y)%7D"> 적용</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>주요 사용처</strong></td>
<td style="text-align: left;">공정성 (인종, 성별 간 평등)</td>
<td style="text-align: left;">불균형 데이터 (희귀 클래스 탐지)</td>
</tr>
</tbody>
</table>
</section>
<section id="theoretical-guarantee" class="level1">
<h1>Theoretical Guarantee</h1>
<p>Vovk의 증명에 따르면, 이 방법 또한 수학적으로 엄밀한 커버리지를 보장합니다.</p>
<blockquote class="blockquote">
<p><strong>Proposition 2 (Error control guarantee for class-balanced conformal prediction)</strong></p>
<p>데이터가 i.i.d.라면, 위 알고리즘으로 생성된 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">는 모든 클래스 <img src="https://latex.codecogs.com/png.latex?y">에 대해 다음을 만족한다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20%5Cmid%20Y_%7Btest%7D%20=%20y)%20%5Cge%201-%5Calpha%20"></p>
</blockquote>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Class-Conditional CP는 불균형한 데이터셋에서 <strong>소수 클래스(Minority Class)의 성능을 희생하지 않기 위한 필수적인 기법</strong>입니다. 비록 소수 클래스의 데이터가 적어서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">가 커지고(불확실성이 커지고), 결과적으로 예측 집합의 크기가 커질 수는 있습니다. 하지만 이는 “틀리는 것”보다는 훨씬 낫습니다. 우리는 적어도 그 안에 정답이 있다는 확신(Safety)을 가질 수 있기 때문입니다.</p>
<hr>
<p><strong>Next Step</strong>: 지금까지는 “정답을 포함할 확률(Coverage)”을 제어하는 것에 집중했습니다. 하지만 어떤 문제에서는 “포함 확률”보다 <strong>“틀렸을 때의 손실(Risk/Loss)”</strong>을 제어하는 것이 더 중요할 수 있습니다. 다음 포스트에서는 이를 일반화한 <strong>Section 4.3 Conformal Risk Control</strong>에 대해 다루겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-02-Class-Conditional-Conformal-Prediction/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.3)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-03-Conformal-Risk-Control/</link>
  <description><![CDATA[ 





<section id="introduction-beyond-coverage" class="level1">
<h1>Introduction: Beyond Coverage</h1>
<p>지금까지 우리가 다룬 Conformal Prediction의 핵심 보장은 다음과 같은 형태였습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cnotin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cle%20%5Calpha%20"></p>
<p>즉, “정답을 놓칠 확률(Miscoverage rate)”을 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하로 묶는 것이었습니다. 하지만 현실의 많은 머신러닝 문제에서는 단순히 “맞았다/틀렸다”의 이진(Binary) 에러보다 더 복잡한 <strong>손실(Loss)</strong>을 제어해야 할 때가 많습니다.</p>
<ul>
<li><strong>의료 영상 분할(Tumor Segmentation)</strong>: 암 영역을 조금이라도 놓치면(False Negative) 치명적입니다. 픽셀 단위의 재현율(Recall)을 보장해야 할 수 있습니다.</li>
<li><strong>다중 라벨 분류(Multilabel Classification)</strong>: 여러 개의 태그 중 90% 이상을 맞추기를 원할 수 있습니다 (F1-score 등).</li>
</ul>
<p><strong>Conformal Risk Control (CRC)</strong>은 이러한 요구를 반영하여, 임의의 유계 손실 함수(Bounded Loss Function)의 기댓값을 제어하는 기법입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5Bl(%5Cmathcal%7BC%7D(X_%7Btest%7D),%20Y_%7Btest%7D)%5D%20%5Cle%20%5Calpha%0A"></p>
</section>
<section id="problem-formulation" class="level1">
<h1>Problem Formulation</h1>
<p>우리의 목표는 모델의 출력 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X)">가 커질수록 <strong>손실(Loss)이 줄어드는</strong> 상황에서, 기대 손실(Expected Risk)이 사용자 지정 허용치 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하가 되도록 하는 파라미터를 찾는 것입니다.</p>
<section id="key-components" class="level2">
<h2 class="anchored" data-anchor-id="key-components">Key Components</h2>
<ol type="1">
<li><p><strong>Nested Sets (<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D_%5Clambda">)</strong>: 우리는 파라미터 <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 조절하여 예측 집합의 크기(보수적인 정도)를 조절합니다. <img src="https://latex.codecogs.com/png.latex?%5Clambda">가 커질수록 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D_%5Clambda(x)">는 더 커지고(더 많은 후보를 포함), 따라서 더 안전해집니다(Conservative).</p></li>
<li><p><strong>Monotone Loss Function (<img src="https://latex.codecogs.com/png.latex?l">)</strong>: 손실 함수 <img src="https://latex.codecogs.com/png.latex?l(%5Cmathcal%7BC%7D,%20Y)">는 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">가 커질수록 감소하거나 같아야 합니다 (Non-increasing). 또한, 손실값은 어떤 상한선 <img src="https://latex.codecogs.com/png.latex?B">를 넘지 않아야 합니다 (<img src="https://latex.codecogs.com/png.latex?l%20%5Cin%20(-%5Cinfty,%20B%5D">).</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Clambda_1%20%5Cle%20%5Clambda_2%20%5Cimplies%20l(%5Cmathcal%7BC%7D_%7B%5Clambda_1%7D(x),%20y)%20%5Cge%20l(%5Cmathcal%7BC%7D_%7B%5Clambda_2%7D(x),%20y)%20"></p></li>
</ol>
</section>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<p>CRC의 알고리즘은 기존 CP와 매우 유사하지만, Quantile 대신 <strong>경험적 리스크(Empirical Risk)</strong>를 사용한다는 점이 다릅니다.</p>
<section id="step-1-calculate-empirical-risk" class="level2">
<h2 class="anchored" data-anchor-id="step-1-calculate-empirical-risk">Step 1: Calculate Empirical Risk</h2>
<p>Calibration 데이터셋 <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cdots,%20(X_n,%20Y_n)">에 대해, 특정 파라미터 <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 썼을 때의 평균 손실(Empirical Risk)을 계산하는 함수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(%5Clambda)">를 정의합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D(%5Clambda)%20=%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20l(%5Cmathcal%7BC%7D_%7B%5Clambda%7D(X_i),%20Y_i)%0A"></p>
<p>이 함수는 <img src="https://latex.codecogs.com/png.latex?%5Clambda">가 증가함에 따라 손실이 감소하는 우하향 곡선을 그립니다.</p>
</section>
<section id="step-2-find-optimal-lambda" class="level2">
<h2 class="anchored" data-anchor-id="step-2-find-optimal-lambda">Step 2: Find Optimal <img src="https://latex.codecogs.com/png.latex?%5Clambda"></h2>
<p>우리는 기대 손실이 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하가 되기를 원합니다. 하지만 유한한 데이터(<img src="https://latex.codecogs.com/png.latex?n">)로 인한 불확실성을 고려해야 하므로, 단순히 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(%5Clambda)%20%5Cle%20%5Calpha">가 되는 지점을 찾으면 안 됩니다.</p>
<p>대신, 다음과 같이 <strong>보정된 기준(Conservative Target)</strong>을 사용합니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Clambda%7D%20=%20%5Cinf%20%5Cleft%5C%7B%20%5Clambda%20:%20%5Chat%7BR%7D(%5Clambda)%20%5Cle%20%5Calpha%20-%20%5Cfrac%7BB%20-%20%5Calpha%7D%7Bn%7D%20%5Cright%5C%7D%0A"></p>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?B"></strong>: 손실 함수의 최댓값 (Upper Bound)</li>
<li><strong>Correction Term (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BB-%5Calpha%7D%7Bn%7D">)</strong>: 데이터 개수 <img src="https://latex.codecogs.com/png.latex?n">이 적을 때 더 보수적으로 <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 선택하게 만드는 항입니다. <img src="https://latex.codecogs.com/png.latex?n">이 무한대로 가면 이 항은 0이 되어 <img src="https://latex.codecogs.com/png.latex?%5Calpha">에 수렴합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-03-Conformal-Risk-Control/images/risk_control_visualization.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Conformal Risk Control의 개념도. 파란색 실선은 <img src="https://latex.codecogs.com/png.latex?%5Clambda">에 따른 경험적 리스크 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(%5Clambda)">를 나타낸다. 목표 리스크 <img src="https://latex.codecogs.com/png.latex?%5Calpha">에서 보정항 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BB-%5Calpha%7D%7Bn%7D">만큼을 뺀 점선과 만나는 지점에서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D">를 결정한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="example-multilabel-classification" class="level1">
<h1>Example: Multilabel Classification</h1>
<p>다중 라벨 분류 문제를 예로 들어보겠습니다. 하나의 이미지가 ‘사람’, ‘차’, ‘신호등’ 등 여러 클래스(<img src="https://latex.codecogs.com/png.latex?Y_i%20%5Csubseteq%20%5C%7B1,%20%5Cdots,%20K%5C%7D">)를 가질 수 있습니다.</p>
<ol type="1">
<li><p><strong>Prediction Set</strong>: 모델이 각 클래스에 대해 예측한 점수 <img src="https://latex.codecogs.com/png.latex?f(X)_k">가 임계값 <img src="https://latex.codecogs.com/png.latex?1-%5Clambda"> 이상인 클래스들을 담습니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D_%5Clambda(x)%20=%20%5C%7B%20k%20:%20f(x)_k%20%5Cge%201-%5Clambda%20%5C%7D%20"> (<img src="https://latex.codecogs.com/png.latex?%5Clambda">가 클수록 임계값이 낮아져 더 많은 클래스가 선택됨)</p></li>
<li><p><strong>Loss Function</strong>: “전체 정답 태그 중 놓친 태그의 비율”을 손실로 정의합니다. <img src="https://latex.codecogs.com/png.latex?%20l(%5Cmathcal%7BC%7D,%20Y)%20=%201%20-%20%5Cfrac%7B%7CY%20%5Ccap%20%5Cmathcal%7BC%7D%7C%7D%7B%7CY%7C%7D%20"> 이 값은 0(모두 맞춤)과 1(하나도 못 맞춤) 사이이므로 <img src="https://latex.codecogs.com/png.latex?B=1">입니다.</p></li>
<li><p><strong>Applying CRC</strong>: 사용자가 <img src="https://latex.codecogs.com/png.latex?%5Calpha=0.1">로 설정했다면, 위 알고리즘을 통해 구한 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D">를 사용했을 때 <strong>“평균적으로 정답 태그의 90% 이상을 포함”</strong>하는 예측 집합을 얻게 됩니다.</p></li>
</ol>
</section>
<section id="theoretical-guarantee" class="level1">
<h1>Theoretical Guarantee</h1>
<p>이 알고리즘은 다음 정리에 의해 수학적으로 보장됩니다.</p>
<blockquote class="blockquote">
<p><strong>Theorem 2 (Conformal Risk Control)</strong></p>
<p>데이터가 i.i.d.이고 손실 함수가 단조 감소(Monotone)한다면, 위 알고리즘으로 선택된 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D">에 대해 다음이 성립한다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BE%7D%5Bl(%5Cmathcal%7BC%7D_%7B%5Chat%7B%5Clambda%7D%7D(X_%7Btest%7D),%20Y_%7Btest%7D)%5D%20%5Cle%20%5Calpha%20"></p>
</blockquote>
<p>이 정리는 CP가 단순히 “에러율 제어”를 넘어, FNR, False Discovery Rate 등 <strong>비즈니스에 중요한 다양한 KPI를 직접 제어</strong>할 수 있는 도구로 확장됨을 의미합니다.</p>
<hr>
<p><strong>Next Step</strong>: 지금까지 지도 학습(Supervised Learning) 환경에서의 CP를 다루었습니다. 다음 포스트에서는 정답 라벨이 없는 비지도 학습 환경, 특히 <strong>Section 4.4 Outlier Detection</strong>에 CP를 어떻게 적용하는지 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-03-Conformal-Risk-Control/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.4)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-04-Outlier-Detection/</link>
  <description><![CDATA[ 





<section id="introduction-unsupervised-setting" class="level1">
<h1>Introduction: Unsupervised Setting</h1>
<p>이전까지 우리는 입력 <img src="https://latex.codecogs.com/png.latex?X">와 정답 <img src="https://latex.codecogs.com/png.latex?Y">가 있는 지도 학습 환경을 다루었습니다. 하지만 <img src="https://latex.codecogs.com/png.latex?Y"> 라벨이 없는 경우, 특히 <strong>“정상 데이터 분포에서 벗어난 이상치(Outlier)”</strong>를 탐지해야 하는 상황은 어떻게 해야 할까요?</p>
<ul>
<li>공장 설비의 이상 진동 감지</li>
<li>네트워크 침입 탐지</li>
<li>불량품 검출</li>
</ul>
<p>[cite_start]이러한 <strong>Outlier Detection (Anomaly Detection)</strong> 문제에서도 Conformal Prediction을 활용하면, <strong>“정상 데이터를 이상치라고 오판할 확률(False Positive Rate)”</strong>을 통계적으로 제어할 수 있습니다[cite: 870, 875].</p>
</section>
<section id="problem-formulation" class="level1">
<h1>Problem Formulation</h1>
<p>[cite_start]우리는 오염되지 않은 <strong>깨끗한 데이터(Clean Dataset)</strong> <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cdots,%20X_n">을 가지고 있습니다[cite: 871]. 이들은 모두 정상(Inlier) 분포에서 나왔다고 가정합니다.</p>
<p>우리의 목표는 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 들어왔을 때, 이것이 정상 분포에서 온 것인지 아니면 이상치인지 판단하는 함수 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">를 만드는 것입니다. [cite_start]이때, <strong>실제로는 정상인데 이상치라고 잘못 판단할 확률(Type I Error)</strong>을 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하로 억제해야 합니다[cite: 876, 877].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5Ctext%7Boutlier%7D)%20%5Cle%20%5Calpha%0A"> (여기서 확률은 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 정상 데이터 분포에서 왔을 때의 확률입니다.)</p>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<p>[cite_start]알고리즘은 기존의 Conformal Prediction과 매우 유사하지만, <img src="https://latex.codecogs.com/png.latex?Y">가 없기 때문에 <strong>입력 <img src="https://latex.codecogs.com/png.latex?X">만으로 계산되는 Score</strong>를 사용합니다[cite: 879, 880].</p>
<section id="step-1-define-heuristic-score-function" class="level2">
<h2 class="anchored" data-anchor-id="step-1-define-heuristic-score-function">Step 1: Define Heuristic Score Function</h2>
<p>[cite_start]비지도 학습 모델(예: One-class SVM, Isolation Forest, Autoencoder의 Reconstruction Error 등)을 사용하여, 데이터 포인트가 이상치일수록 커지는 점수 함수 <img src="https://latex.codecogs.com/png.latex?s(x)">를 정의합니다[cite: 872, 873].</p>
<p><img src="https://latex.codecogs.com/png.latex?%20s(x):%20%5Cmathcal%7BX%7D%20%5Crightarrow%20%5Cmathbb%7BR%7D%20"> * <img src="https://latex.codecogs.com/png.latex?s(x)">가 큼 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 이상치(Outlier)일 가능성 높음 * <img src="https://latex.codecogs.com/png.latex?s(x)">가 작음 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 정상(Inlier)일 가능성 높음</p>
</section>
<section id="step-2-calibration" class="level2">
<h2 class="anchored" data-anchor-id="step-2-calibration">Step 2: Calibration</h2>
<p>[cite_start]깨끗한 데이터셋 <img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cdots,%20X_n">에 대해 점수들을 계산합니다[cite: 881]. <img src="https://latex.codecogs.com/png.latex?%20s_i%20=%20s(X_i),%20%5Cquad%20i=1,%20%5Cdots,%20n%20"></p>
<p>[cite_start]그리고 이 점수들의 분포에서 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수(Quantile)에 해당하는 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산합니다[cite: 884].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20;%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20%5Cright)%0A"></p>
</section>
<section id="step-3-detection-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-3-detection-inference">Step 3: Detection (Inference)</h2>
<p>[cite_start]새로운 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 들어오면 점수 <img src="https://latex.codecogs.com/png.latex?s(X_%7Btest%7D)">를 계산하고, 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">와 비교하여 판정합니다[cite: 884].</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(x)%20=%20%5Cbegin%7Bcases%7D%20%5Ctext%7Binlier%7D%20&amp;%20%5Ctext%7Bif%20%7D%20s(x)%20%5Cle%20%5Chat%7Bq%7D%20%5C%5C%20%5Ctext%7Boutlier%7D%20&amp;%20%5Ctext%7Bif%20%7D%20s(x)%20%3E%20%5Chat%7Bq%7D%20%5Cend%7Bcases%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-04-Outlier-Detection/images/outlier_detection_concept.png" class="img-fluid figure-img"></p>
<figcaption>Concept: Conformal Outlier Detection. 정상 데이터(Clean Data)들의 점수 분포를 통해 임계값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 설정한다. 새로운 데이터의 점수가 이 선을 넘으면 이상치로 간주한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="theoretical-guarantee-interpretation" class="level1">
<h1>Theoretical Guarantee &amp; Interpretation</h1>
<p>이 간단한 절차는 다음 명제에 의해 False Positive Rate를 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하로 보장합니다.</p>
<blockquote class="blockquote">
<p><strong>Proposition 3 (Error control guarantee for outlier detection)</strong></p>
<p>[cite_start]<img src="https://latex.codecogs.com/png.latex?X_1,%20%5Cdots,%20X_n">과 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 동일한 분포(i.i.d.)에서 추출되었다면, 위 알고리즘은 다음을 만족한다[cite: 887, 888]. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5Ctext%7Boutlier%7D)%20%5Cle%20%5Calpha%20"></p>
</blockquote>
<section id="statistical-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="statistical-interpretation">Statistical Interpretation</h3>
<p>[cite_start]이 과정은 통계적 <strong>가설 검정(Hypothesis Testing)</strong>과도 연결됩니다[cite: 892]. * <strong>귀무가설(<img src="https://latex.codecogs.com/png.latex?H_0">)</strong>: <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">는 정상 데이터 분포(Calibration Data)와 교환 가능하다(Exchangeable). * [cite_start]<strong>기각</strong>: 만약 <img src="https://latex.codecogs.com/png.latex?s(X_%7Btest%7D)">가 상위 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 범위에 들어간다면(즉, p-value &lt; <img src="https://latex.codecogs.com/png.latex?%5Calpha">), 우리는 귀무가설을 기각하고 해당 데이터를 이상치로 판단합니다[cite: 893].</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Conformal Outlier Detection은 복잡한 이상탐지 모델의 출력값을 <strong>“신뢰할 수 있는 통계적 판정”</strong>으로 변환해줍니다. 사용자는 “이 데이터는 점수가 0.8입니다”라는 모호한 말 대신, <strong>“이 데이터는 95% 신뢰수준에서 정상 범위를 벗어났습니다”</strong>라는 명확한 근거를 가지고 의사결정을 내릴 수 있습니다.</p>
<hr>
<p><strong>Next Step</strong>: 지금까지는 학습 데이터와 테스트 데이터의 분포가 같다는 가정(i.i.d.) 하에 진행했습니다. 하지만 현실에서는 시간이 지나며 분포가 변하기도 합니다. 다음 포스트에서는 이러한 <strong>Covariate Shift</strong> 환경에서 CP를 적용하는 <strong>Section 4.5 Conformal Prediction Under Covariate Shift</strong>에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-04-Outlier-Detection/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.5)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-05-Conformal-Prediction-Under-Covariate-Shift/</link>
  <description><![CDATA[ 





<section id="introduction-when-the-i.i.d.-assumption-fails" class="level1">
<h1>Introduction: When the i.i.d. Assumption Fails</h1>
<ul>
<li><p>지금까지 우리가 배운 모든 Conformal Prediction(CP) 방법론은 하나의 강력한 가정에 의존하고 있습니다.</p></li>
<li><p>바로 <strong>“테스트 데이터가 Calibration 데이터와 동일한 분포(i.i.d.)에서 왔다”</strong>는 가정입니다.</p></li>
<li><p>하지만 현실은 그렇지 않습니다. 과거의 데이터가 미래를 완벽하게 대변하지 못하는 경우가 많습니다.</p>
<ul>
<li><strong>의료 진단</strong>: 학습 데이터는 성인과 유아의 비율이 50:50이었는데, 실제 병원에는 성인이 95% 방문할 수 있습니다.</li>
<li><strong>자율 주행</strong>: 아침(밝음)에 데이터를 수집하여 학습했는데, 실제 주행은 오후(어두움)에 이루어질 수 있습니다.</li>
</ul></li>
<li><p>이러한 분포의 변화 중 <strong>Covariate Shift</strong>는 입력 변수 <img src="https://latex.codecogs.com/png.latex?X">의 분포 <img src="https://latex.codecogs.com/png.latex?P(X)">는 바뀌지만, 입력과 출력 사이의 관계 <img src="https://latex.codecogs.com/png.latex?P(Y%7CX)">는 유지되는 상황을 말합니다.</p></li>
<li><p>이번 포스트에서는 <strong>Weighted Conformal Prediction</strong>을 사용하여 이러한 변화 속에서도 커버리지를 보장하는 방법을 알아봅니다.</p></li>
</ul>
</section>
<section id="problem-formulation-covariate-shift" class="level1">
<h1>Problem Formulation: Covariate Shift</h1>
<ul>
<li>우리의 상황을 수식으로 정의해봅시다.
<ul>
<li><strong>Calibration Data</strong>: <img src="https://latex.codecogs.com/png.latex?P"> 분포에서 추출됨.</li>
<li><strong>Test Data</strong>: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D_%7Btest%7D"> 분포에서 추출됨.</li>
</ul></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%20%5Csim%20P%20%5Cquad%20%5Crightarrow%20%5Cquad%20X_%7Btest%7D%20%5Csim%20%5Cmathcal%7BP%7D_%7Btest%7D%0A"></p>
<ul>
<li>단, <img src="https://latex.codecogs.com/png.latex?Y%7CX"> (입력이 주어졌을 때 정답의 분포)는 변하지 않는다고 가정합니다.
<ul>
<li>이를 <strong>Covariate Shift</strong>라고 합니다.</li>
</ul></li>
<li>이 상황에서 기존의 일반적인 CP를 사용하면 커버리지가 깨질 수 있습니다.
<ul>
<li>예를 들어, 모델이 어려워하는 데이터(유아)가 테스트 셋에 더 많이 등장한다면, 에러율은 우리가 설정한 <img src="https://latex.codecogs.com/png.latex?%5Calpha">보다 훨씬 높아질 것입니다.</li>
</ul></li>
</ul>
</section>
<section id="the-solution-weighted-conformal-prediction" class="level1">
<h1>The Solution: Weighted Conformal Prediction</h1>
<ul>
<li>해결책의 핵심은 <strong>“테스트 분포 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BP%7D_%7Btest%7D">에서 더 자주 등장할 것 같은 데이터에 더 큰 가중치(Weight)를 주는 것”</strong>입니다.</li>
</ul>
<section id="step-1-likelihood-ratio-calculation" class="level2">
<h2 class="anchored" data-anchor-id="step-1-likelihood-ratio-calculation">Step 1: Likelihood Ratio Calculation</h2>
<ul>
<li>먼저, 두 분포 사이의 비율(Likelihood Ratio)을 계산하는 함수 <img src="https://latex.codecogs.com/png.latex?w(x)">를 정의합니다. <img src="https://latex.codecogs.com/png.latex?%0Aw(x)%20=%20%5Cfrac%7Bd%5Cmathcal%7BP%7D_%7Btest%7D(x)%7D%7BdP(x)%7D%0A">
<ul>
<li><img src="https://latex.codecogs.com/png.latex?w(x)%20%3E%201">: 해당 샘플 <img src="https://latex.codecogs.com/png.latex?x">는 학습 때보다 테스트 때 더 자주 등장합니다 (중요함).</li>
<li><img src="https://latex.codecogs.com/png.latex?w(x)%20%3C%201">: 해당 샘플 <img src="https://latex.codecogs.com/png.latex?x">는 테스트 때 덜 등장합니다 (덜 중요함).</li>
</ul></li>
</ul>
</section>
<section id="step-2-compute-normalized-weights" class="level2">
<h2 class="anchored" data-anchor-id="step-2-compute-normalized-weights">Step 2: Compute Normalized Weights</h2>
<ul>
<li>새로운 테스트 포인트 <img src="https://latex.codecogs.com/png.latex?x">가 들어왔을 때, 이 <img src="https://latex.codecogs.com/png.latex?x">와 기존 Calibration 데이터 <img src="https://latex.codecogs.com/png.latex?X_i">들에게 부여할 확률 질량(Probability Mass)을 재계산합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_i%5Ew(x)%20=%20%5Cfrac%7Bw(X_i)%7D%7B%5Csum_%7Bj=1%7D%5E%7Bn%7D%20w(X_j)%20+%20w(x)%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap_%7Btest%7D%5Ew(x)%20=%20%5Cfrac%7Bw(x)%7D%7B%5Csum_%7Bj=1%7D%5E%7Bn%7D%20w(X_j)%20+%20w(x)%7D%0A"></p>
<ul>
<li>기존 CP에서는 모든 데이터가 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn+1%7D">의 동등한 확률을 가졌습니다.</li>
<li>Weighted CP에서는 <img src="https://latex.codecogs.com/png.latex?w(%5Ccdot)">에 비례하여 확률을 다르게 배정합니다.</li>
<li>즉, 테스트 분포와 유사한 데이터일수록 <img src="https://latex.codecogs.com/png.latex?p_i">가 커집니다.</li>
</ul>
</section>
<section id="step-3-weighted-quantile-calculation" class="level2">
<h2 class="anchored" data-anchor-id="step-3-weighted-quantile-calculation">Step 3: Weighted Quantile Calculation</h2>
<ul>
<li>이제 가장 중요한 단계인 Quantile 계산입니다.</li>
<li>기존에는 단순히 점수를 정렬하고 <img src="https://latex.codecogs.com/png.latex?(1-%5Calpha)"> 지점을 찾았지만, 이제는 <strong>가중치가 반영된 누적 분포(Weighted CDF)</strong>를 사용해야 합니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D(x)%20=%20%5Cinf%20%5Cleft%5C%7B%20s_j%20:%20%5Csum_%7Bi=1%7D%5E%7Bj%7D%20p_i%5Ew(x)%20%5Cmathbb%7BI%7D%5C%7Bs_i%20%5Cle%20s_j%5C%7D%20%5Cge%201-%5Calpha%20%5Cright%5C%7D%0A">
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?s_j">는 오름차순으로 정렬된 Calibration Score라고 가정합니다.</li>
</ul></li>
<li>즉, 가중치 <img src="https://latex.codecogs.com/png.latex?p_i%5Ew(x)">들을 순서대로 더해가다가, 그 합이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 넘기는 순간의 점수를 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(x)">로 선택합니다.</li>
</ul>
</section>
</section>
<section id="intuition-how-quantile-changes" class="level1">
<h1>Intuition: How Quantile Changes</h1>
<ul>
<li>이 과정이 직관적으로 어떤 의미를 가질까요?</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-05-Conformal-Prediction-Under-Covariate-Shift/images/covariate_shift_cdf.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Standard CP vs Weighted CP의 CDF 비교. (좌측) 일반적인 CP는 모든 점수의 가중치가 같아 직선 형태의 CDF를 가진다. (우측) Covariate Shift 상황에서는 가중치에 따라 CDF가 곡선이 되며, Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">의 위치가 달라진다.</figcaption>
</figure>
</div>
<ol type="1">
<li><p><strong>Shift towards Hard Examples</strong>: 만약 테스트 분포가 모델이 어려워하는(Score가 높은) 데이터들 쪽으로 이동했다면, 높은 점수를 가진 <img src="https://latex.codecogs.com/png.latex?X_i">들의 가중치 <img src="https://latex.codecogs.com/png.latex?w(X_i)">가 커집니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> CDF 그래프에서 오른쪽 부분의 경사가 가파라집니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 지점에 도달하기 위해 더 많은 점수를 지나야 하거나, 더 높은 점수에서 도달하게 됩니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">가 증가합니다 (더 보수적인, 넓은 예측 집합 생성).</strong></p></li>
<li><p><strong>Shift towards Easy Examples</strong>: 반대로 테스트 분포가 쉬운 데이터들 위주라면, 낮은 점수들의 가중치가 커집니다. <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">가 감소합니다 (더 좁고 효율적인 예측 집합 생성).</strong></p></li>
</ol>
</section>
<section id="theorem-guarantee" class="level1">
<h1>Theorem &amp; Guarantee</h1>
<ul>
<li>Tibshirani et al.&nbsp;(2019)에 의해 제안된 이 방법은 다음 정리에 의해 커버리지를 보장합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem 3 (Conformal prediction under covariate shift)</strong></p>
<p>데이터가 위에서 정의한 Covariate Shift 가정하에 생성되었다면, Weighted Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D(X_%7Btest%7D)">를 사용한 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">는 다음을 만족한다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cge%201-%5Calpha%20"></p>
</blockquote>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<ul>
<li><p><strong>Weighted Conformal Prediction</strong>은 데이터 분포가 변하는 현실 세계의 문제에 CP를 적용하기 위한 필수적인 도구입니다.</p></li>
<li><p>단순히 <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7Bn+1%7D">이라는 고정 관념을 깨고, <strong>“테스트 시점에 더 중요한 데이터에 가중치를 준다”</strong>는 아이디어를 통해 분포 변화에 유연하게 대처할 수 있습니다.</p></li>
<li><p>이 방법은 <img src="https://latex.codecogs.com/png.latex?w(x)">를 정확히 안다면 완벽하게 작동하며, <img src="https://latex.codecogs.com/png.latex?w(x)">를 추정해야 하는 경우에도 꽤 견고한(Robust) 성능을 보여줍니다.</p></li>
</ul>
<hr>
<p><strong>Next Step</strong>: 분포가 갑작스럽게 변하는 것(Shift)도 문제지만, 시간이 지남에 따라 서서히 변하는 <strong>Drift</strong> 현상도 중요합니다. 다음 포스트에서는 시계열 데이터 등에서 발생하는 <strong>Section 4.6 Conformal Prediction Under Distribution Drift</strong>에 대해 알아보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-05-Conformal-Prediction-Under-Covariate-Shift/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 4.6)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-06-Conformal-Prediction-Under-Distribution-Shift/indx.html</link>
  <description><![CDATA[ 





<section id="introduction-when-data-changes-over-time" class="level1">
<h1>Introduction: When Data Changes Over Time</h1>
<p>이전 포스트(4.5절)에서는 입력 분포가 변하는 Covariate Shift를 다루었습니다. 하지만 그보다 더 다루기 까다로운 것은 <strong>Distribution Drift(분포 표류)</strong>입니다.</p>
<p>Distribution Drift는 데이터의 분포가 <strong>시간이 지남에 따라 서서히(Slowly varying), 혹은 알 수 없는 방식으로 변하는 현상</strong>을 말합니다. * <strong>주식 시장</strong>: 10년 전의 시장 상황과 오늘의 시장 상황은 전혀 다릅니다. * <strong>센서 데이터</strong>: 기계가 노후화되면서 센서의 측정값 분포가 서서히 달라집니다.</p>
<p>이런 시계열(Time-series) 문제에서는 “과거의 모든 데이터가 미래를 예측하는 데 동등하게 중요하다”는 i.i.d. 가정이 성립하지 않습니다. 1년 전 데이터보다 어제의 데이터가 훨씬 중요하기 때문입니다.</p>
<p>이번 포스트에서는 <strong>최신 데이터에 더 큰 가중치</strong>를 부여하는 Weighted Conformal Prediction을 통해 이 문제를 해결하는 방법을 알아봅니다.</p>
</section>
<section id="the-method-weighted-conformal-prediction-again" class="level1">
<h1>The Method: Weighted Conformal Prediction (Again)</h1>
<p>기본적인 아이디어는 4.5절의 Covariate Shift와 동일하게 <strong>Weighted Quantile</strong>을 사용하는 것입니다. 하지만 가중치 <img src="https://latex.codecogs.com/png.latex?w_i">를 결정하는 방식이 다릅니다. Likelihood Ratio를 계산하는 대신, <strong>시간적 근접성(Recency)</strong>을 기준으로 가중치를 설정합니다.</p>
<section id="step-1-define-weight-schedule" class="level2">
<h2 class="anchored" data-anchor-id="step-1-define-weight-schedule">Step 1: Define Weight Schedule</h2>
<p>사용자는 도메인 지식에 기반하여 “오래된 데이터를 얼마나 잊을 것인가”를 결정하는 가중치 스케줄을 정의합니다.</p>
<p>가장 널리 쓰이는 두 가지 방법은 다음과 같습니다:</p>
<ol type="1">
<li><p><strong>Rolling Window (Sliding Window)</strong>: 최근 <img src="https://latex.codecogs.com/png.latex?K">개의 데이터만 사용하고, 나머지는 버립니다. <img src="https://latex.codecogs.com/png.latex?%20w_i%5E%7B%5Ctext%7Bfixed%7D%7D%20=%20%5Cmathbb%7BI%7D%5C%7Bi%20%5Cge%20n%20-%20K%5C%7D%20"></p></li>
<li><p><strong>Exponential Decay (Smooth Decay)</strong>: 과거 데이터의 영향력을 지수적으로 감소시킵니다. <img src="https://latex.codecogs.com/png.latex?%20w_i%5E%7B%5Ctext%7Bdecay%7D%7D%20=%20%5Cgamma%5E%7Bn-i+1%7D%20%5Cquad%20(0%20%3C%20%5Cgamma%20%3C%201)%20"> (예: <img src="https://latex.codecogs.com/png.latex?%5Cgamma%20=%200.99">라면 바로 직전 데이터는 1, 그 전은 0.99, 그 전은 <img src="https://latex.codecogs.com/png.latex?0.98%20%5Cdots"> 가중치를 가짐)</p></li>
</ol>
</section>
<section id="step-2-normalize-weights" class="level2">
<h2 class="anchored" data-anchor-id="step-2-normalize-weights">Step 2: Normalize Weights</h2>
<p>정의된 가중치 <img src="https://latex.codecogs.com/png.latex?w_i">를 전체 합이 1이 되도록 정규화(Normalize)합니다. 이때 테스트 포인트 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">의 가중치 <img src="https://latex.codecogs.com/png.latex?w_%7Btest%7D">도 포함하여 계산합니다. (보통 <img src="https://latex.codecogs.com/png.latex?w_%7Btest%7D=1">로 둠)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctilde%7Bw%7D_i%20=%20%5Cfrac%7Bw_i%7D%7B%5Csum_%7Bj=1%7D%5E%7Bn%7D%20w_j%20+%201%7D%0A"></p>
</section>
<section id="step-3-weighted-quantile" class="level2">
<h2 class="anchored" data-anchor-id="step-3-weighted-quantile">Step 3: Weighted Quantile</h2>
<p>정규화된 가중치를 사용하여 보정된 분위수(Quantile) <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산합니다. Calibration Score <img src="https://latex.codecogs.com/png.latex?s_i">들을 오름차순 정렬했을 때, 누적 가중치 합이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">를 넘는 지점을 찾습니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Cinf%20%5Cleft%5C%7B%20q%20:%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Ctilde%7Bw%7D_i%20%5Cmathbb%7BI%7D%5C%7Bs_i%20%5Cle%20q%5C%7D%20%5Cge%201-%5Calpha%20%5Cright%5C%7D%0A"></p>
</section>
</section>
<section id="theoretical-analysis" class="level1">
<h1>Theoretical Analysis</h1>
<p>이 방식이 왜 작동하는지 수학적으로 살펴보겠습니다. Barber et al.&nbsp;(2022)의 연구에 따르면, Weighted CP는 분포 간의 거리(Total Variation Distance)에 비례하는 오차 범위 내에서 커버리지를 보장합니다.</p>
<blockquote class="blockquote">
<p><strong>Theorem 4 (Conformal prediction under distribution drift)</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?i">번째 Calibration 데이터와 테스트 데이터 사이의 TV 거리(Total Variation Distance)를 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i">라고 하자. <img src="https://latex.codecogs.com/png.latex?%20%5Cepsilon_i%20=%20d_%7BTV%7D((X_i,%20Y_i),%20(X_%7Btest%7D,%20Y_%7Btest%7D))%20"></p>
<p>이때 위에서 정의한 Weighted CP 절차는 다음을 만족한다: <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cge%201%20-%20%5Calpha%20-%202%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Ctilde%7Bw%7D_i%20%5Cepsilon_i%20"></p>
</blockquote>
<section id="interpretation" class="level3">
<h3 class="anchored" data-anchor-id="interpretation">Interpretation</h3>
<p>이 부등식의 우변에 있는 <strong>페널티 항 (<img src="https://latex.codecogs.com/png.latex?2%20%5Csum%20%5Ctilde%7Bw%7D_i%20%5Cepsilon_i">)</strong>을 줄이는 것이 핵심입니다.</p>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i"> (Drift)</strong>: 데이터 <img src="https://latex.codecogs.com/png.latex?i">가 현재 시점(<img src="https://latex.codecogs.com/png.latex?test">)과 얼마나 다른 분포를 가지는지를 나타냅니다. 오래된 데이터일수록 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i">가 클(1에 가까울) 것입니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bw%7D_i"> (Weight)</strong>: 우리가 부여한 가중치입니다.</li>
</ul>
<p>우리의 목표는 <strong><img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i">가 큰(오래되어 분포가 달라진) 데이터에 작은 가중치 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bw%7D_i">를 부여</strong>하여, 곱 <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bw%7D_i%20%5Cepsilon_i">를 0에 가깝게 만드는 것입니다. 즉, <strong>“분포가 많이 변한 데이터는 무시하겠다”</strong>는 전략을 통해 커버리지 손실을 막을 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-06-Conformal-Prediction-Under-Distribution-Shift/images/distribution_drift_weights.png" class="img-fluid figure-img"></p>
<figcaption>Figure: Distribution Drift 상황에서의 가중치 전략. (위) 데이터 분포가 시간에 따라 파란색에서 초록색으로 변함. (아래) 오래된 데이터(파란색 영역)에 낮은 가중치를 부여함으로써, 현재 시점의 분포(초록색)에 맞는 Quantile을 추정한다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="practical-consideration-effective-sample-size" class="level1">
<h1>Practical Consideration: Effective Sample Size</h1>
<p>가중치를 사용하면 분포 변화에는 대응할 수 있지만, 대가가 따릅니다. 바로 <strong>유효 샘플 수(Effective Sample Size)</strong>가 줄어든다는 점입니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0An%5E%7B%5Ctext%7Beff%7D%7D%20=%20%5Cfrac%7B(%5Csum%20w_i)%5E2%7D%7B%5Csum%20w_i%5E2%7D%0A"></p>
<ul>
<li><strong>Uniform Weights (<img src="https://latex.codecogs.com/png.latex?w_i=1">)</strong>: <img src="https://latex.codecogs.com/png.latex?n%5E%7B%5Ctext%7Beff%7D%7D%20=%20n">. 모든 데이터를 다 쓰므로 샘플 수가 많아 분산이 작습니다 (안정적).</li>
<li><strong>Concentrated Weights</strong>: 최근 데이터에만 큰 가중치를 주면 <img src="https://latex.codecogs.com/png.latex?n%5E%7B%5Ctext%7Beff%7D%7D">가 급격히 작아집니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?n%5E%7B%5Ctext%7Beff%7D%7D">가 작아지면 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 커버리지의 분산(Variance)이 커집니다. (즉, 예측 집합의 크기가 들쑥날쑥해짐)</li>
</ul></li>
</ul>
<p>따라서 <strong>Trade-off</strong>가 존재합니다: * 가중치를 너무 급격하게 줄이면(빠른 적응) <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 분포 변화에는 강하지만, 예측이 불안정해짐. * 가중치를 너무 천천히 줄이면(느린 적응) <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 예측은 안정적이지만, 이미 변해버린 과거 분포의 영향을 받음.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Conformal Prediction Under Distribution Drift는 시계열 데이터와 같이 <strong>Non-stationary</strong> 환경에서 신뢰할 수 있는 예측 구간을 만드는 방법입니다.</p>
<ul>
<li>핵심은 <strong>Weighted Quantile</strong>을 사용하는 것입니다.</li>
<li>가중치 스케줄(Sliding Window, Decay)을 통해 과거 데이터를 적절히 “망각(Forget)”해야 합니다.</li>
<li>이론적으로, 분포 변화가 큰 데이터에 가중치를 적게 줌으로써 커버리지 하락을 방어합니다.</li>
<li>하지만 너무 과도한 가중치 조절은 유효 샘플 수를 줄여 불안정성을 초래할 수 있으므로 주의가 필요합니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 이제 이론적인 확장은 모두 다루었습니다. 다음 포스트부터는 <strong>Section 5. Worked Examples</strong>로 넘어가서, 실제 데이터셋(Multilabel Classification, Tumor Segmentation 등)에 이 기법들을 어떻게 적용하는지 예제 코드를 통해 구체적으로 살펴보겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-04-Extensions-of-Conformal-Prediction/Part-04-06-Conformal-Prediction-Under-Distribution-Shift/indx.html</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 5)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>지금까지 우리는 Conformal Prediction(CP)의 다양한 이론적 확장(Extension)들을 다루었습니다. 이제 이 도구들이 실제 머신러닝 문제에서 어떻게 작동하는지 확인할 차례입니다.</p>
<p>이번 포스트에서는 논문의 <strong>Section 5</strong>에서 소개된 5가지의 구체적인 적용 사례를 살펴봅니다. 각 사례는 단순한 분류/회귀를 넘어, <strong>Risk Control</strong>, <strong>Distribution Shift</strong>, <strong>Outlier Detection</strong> 등 현실적인 난제들을 어떻게 해결하는지 보여줍니다.</p>
</section>
<section id="multilabel-classification-with-fnr-control" class="level1">
<h1>1. Multilabel Classification with FNR Control</h1>
<section id="problem-setup" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup">Problem Setup</h2>
<p>이미지 안에 있는 모든 객체를 맞추는 다중 라벨 분류(Multilabel Classification) 문제입니다. 단순히 정답을 포함하는 것을 넘어, <strong>“정답 객체 중 90% 이상을 찾아내라(Recall <img src="https://latex.codecogs.com/png.latex?%5Cge"> 90%)”</strong>와 같은 요구사항이 있을 때 사용합니다. 이를 위해 <strong>False Negative Rate (FNR)</strong>를 제어합니다.</p>
<ul>
<li><strong>Dataset</strong>: Microsoft COCO (Common Objects in Context)</li>
<li><strong>Goal</strong>: 실제 객체들(<img src="https://latex.codecogs.com/png.latex?Y">) 중 모델이 예측한 집합(<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D">)에 포함되지 않은 비율(FNR)을 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하로 유지.</li>
</ul>
</section>
<section id="methodology-conformal-risk-control" class="level2">
<h2 class="anchored" data-anchor-id="methodology-conformal-risk-control">Methodology: Conformal Risk Control</h2>
<p>여기서는 <strong>Section 4.3</strong>에서 다룬 Conformal Risk Control(CRC)을 사용합니다.</p>
<ol type="1">
<li><p><strong>Prediction Set Construction</strong>: 모델의 예측 확률 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">가 임계값 <img src="https://latex.codecogs.com/png.latex?%5Clambda"> 이상인 클래스들을 선택합니다. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D_%5Clambda(x)%20=%20%5C%7B%20k%20:%20%5Chat%7Bf%7D(x)_k%20%5Cge%20%5Clambda%20%5C%7D%20"> (<img src="https://latex.codecogs.com/png.latex?%5Clambda">가 작을수록 더 많은 클래스를 포함하므로 보수적이 됨)</p></li>
<li><p><strong>Loss Function (<img src="https://latex.codecogs.com/png.latex?l_%7BFNR%7D">)</strong>: 예측 집합이 정답을 얼마나 놓쳤는지를 정의합니다. <img src="https://latex.codecogs.com/png.latex?%20l_%7BFNR%7D(%5Cmathcal%7BC%7D_%5Clambda(x),%20y)%20=%201%20-%20%5Cfrac%7B%7C%5Cmathcal%7BC%7D_%5Clambda(x)%20%5Ccap%20y%7C%7D%7B%7Cy%7C%7D%20"></p>
<ul>
<li>만약 정답 <img src="https://latex.codecogs.com/png.latex?y=%5C%7B%5Ctext%7B%EC%82%AC%EB%9E%8C,%20%EC%B0%A8%7D%5C%7D">인데 예측 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D=%5C%7B%5Ctext%7B%EC%82%AC%EB%9E%8C%7D%5C%7D">이라면, 교집합은 1개, 정답은 2개이므로 손실은 <img src="https://latex.codecogs.com/png.latex?1%20-%201/2%20=%200.5">입니다.</li>
</ul></li>
<li><p><strong>Threshold Optimization</strong>: Calibration Set에서 경험적 리스크가 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> (보정항 포함) 이하가 되는 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Clambda%7D">를 찾습니다. <img src="https://latex.codecogs.com/png.latex?%20%5Chat%7B%5Clambda%7D%20=%20%5Cinf%20%5Cleft%5C%7B%20%5Clambda%20:%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi=1%7D%5En%20l_%7BFNR%7D(%5Cmathcal%7BC%7D_%5Clambda(X_i),%20Y_i)%20%5Cle%20%5Calpha%20-%20%5Cfrac%7B1-%5Calpha%7D%7Bn%7D%20%5Cright%5C%7D%20"></p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/images/figure13_multilabel.png" class="img-fluid figure-img"></p>
<figcaption>Figure 13: MS COCO 데이터셋에 대한 Multilabel Classification 결과 (<img src="https://latex.codecogs.com/png.latex?%5Calpha=0.1">). 빨간색 텍스트는 놓친 정답(False Negative), 파란색은 잘못 예측한 오답(False Positive), 검은색은 맞춘 정답(True Positive)을 나타낸다. 평균적으로 90% 이상의 정답 라벨을 찾아내고 있다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="tumor-segmentation" class="level1">
<h1>2. Tumor Segmentation</h1>
<section id="problem-setup-1" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-1">Problem Setup</h2>
<p>의료 영상에서 종양(Tumor) 부위를 픽셀 단위로 분할(Segmentation)하는 문제입니다. 여기서도 핵심은 <strong>“종양 픽셀을 놓치지 않는 것”</strong>입니다. 즉, 픽셀 단위의 FNR 제어가 필요합니다.</p>
<ul>
<li><strong>Dataset</strong>: Gut Polyps dataset</li>
<li><strong>Goal</strong>: 전체 종양 픽셀 중 예측 마스크가 커버하지 못한 비율을 제어.</li>
</ul>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<p>Multilabel Classification과 원리는 동일하지만, 대상이 <strong>클래스</strong>에서 <strong>픽셀</strong>로 바뀝니다.</p>
<ol type="1">
<li><strong>Output</strong>: <img src="https://latex.codecogs.com/png.latex?M%20%5Ctimes%20N"> 크기의 확률 맵 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(x)">.</li>
<li><strong>Prediction Mask</strong>: 각 픽셀 <img src="https://latex.codecogs.com/png.latex?(i,j)">에 대해 확률이 <img src="https://latex.codecogs.com/png.latex?%5Clambda"> 이상이면 종양으로 예측. <img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D_%5Clambda(x)%20=%20%5C%7B%20(i,j)%20:%20%5Chat%7Bf%7D(x)_%7B(i,j)%7D%20%5Cge%20%5Clambda%20%5C%7D%20"></li>
<li><strong>Loss Function</strong>: <img src="https://latex.codecogs.com/png.latex?%20l(%5Cmathcal%7BC%7D,%20Y)%20=%20%5Cfrac%7B%5Ctext%7B#%20of%20missed%20tumor%20pixels%7D%7D%7B%5Ctext%7Btotal%20#%20of%20tumor%20pixels%7D%7D%20"></li>
</ol>
<p>이 방법을 적용하면 의사는 “이 AI가 표시한 영역 안에 실제 종양의 90%가 포함되어 있다”는 확신을 가지고 진단에 임할 수 있습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/images/figure14_segmentation.png" class="img-fluid figure-img"></p>
<figcaption>Figure 14: 종양 분할(Tumor Segmentation) 결과 (<img src="https://latex.codecogs.com/png.latex?%5Calpha=0.1">). 붉은 영역은 모델이 놓친 종양 부위(False Negative)이다. CRC를 통해 놓치는 부위를 통계적으로 최소화할 수 있다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="weather-prediction-time-series" class="level1">
<h1>3. Weather Prediction (Time-Series)</h1>
<section id="problem-setup-2" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-2">Problem Setup</h2>
<p>시간의 흐름에 따라 기온(Temperature)을 예측하는 시계열 회귀 문제입니다. 시간이 지남에 따라 계절이 바뀌고 기후가 변하므로, 데이터는 <strong>i.i.d.가 아니며 분포가 표류(Distribution Drift)</strong>합니다.</p>
<ul>
<li><strong>Dataset</strong>: Yandex Weather Prediction (Shifts Project)</li>
<li><strong>Challenge</strong>: 과거 데이터와 현재 데이터의 상관관계가 변함.</li>
</ul>
</section>
<section id="methodology-weighted-conformal-prediction" class="level2">
<h2 class="anchored" data-anchor-id="methodology-weighted-conformal-prediction">Methodology: Weighted Conformal Prediction</h2>
<p><strong>Section 4.6</strong>의 분포 표류(Distribution Drift) 대응법을 사용합니다.</p>
<ol type="1">
<li><p><strong>Score Function</strong>: 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(X_t)">와 불확실성 추정값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bu%7D(X_t)">를 이용한 정규화된 잔차(Normalized Residual)를 사용합니다. <img src="https://latex.codecogs.com/png.latex?%20s_t%20=%20%5Cfrac%7B%7CY_t%20-%20%5Chat%7Bf%7D(X_t)%7C%7D%7B%5Chat%7Bu%7D(X_t)%7D%20"></p></li>
<li><p><strong>Weighted Quantile</strong>: 최근 <img src="https://latex.codecogs.com/png.latex?K">개의 데이터만 사용하는 <strong>Sliding Window</strong> 방식을 적용합니다. 가중치 <img src="https://latex.codecogs.com/png.latex?w_%7Bt'%7D%20=%20%5Cmathbb%7BI%7D%5C%7Bt'%20%5Cge%20t%20-%20K%5C%7D">를 사용하여, 오래된 데이터는 과감히 버리고 최근 데이터 분포에만 맞춥니다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Chat%7Bq%7D_t%20=%20%5Ctext%7BQuantile%7D_%7B%5Ctext%7Bweighted%7D%7D(s_%7Bt-K%7D,%20%5Cdots,%20s_%7Bt-1%7D)%20"></p></li>
</ol>
<p>결과적으로 급격한 기온 변화나 계절 변화가 발생했을 때, 일반 CP보다 훨씬 빠르게 적응하여 적절한 커버리지를 회복합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/images/figure15_weather.png" class="img-fluid figure-img"></p>
<figcaption>Figure 15: 시계열 기온 예측 결과. (왼쪽) 일반 CP(주황색)는 분포 변화 시 커버리지가 무너지지만, Weighted CP(파란색)는 빠르게 회복하여 목표 커버리지(0.9)를 유지한다. (오른쪽) 예측된 구간의 시각화.</figcaption>
</figure>
</div>
</section>
</section>
<section id="toxic-comment-identification-outlier-detection" class="level1">
<h1>4. Toxic Comment Identification (Outlier Detection)</h1>
<section id="problem-setup-3" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-3">Problem Setup</h2>
<p>온라인 댓글이 유해한지(Toxic) 아닌지를 판별하는 문제입니다. 정상적인 대화(Non-toxic) 데이터만 잔뜩 있는 상태에서, 새로운 댓글이 <strong>정상 범주를 벗어난(Outlier/Toxic)</strong> 것인지 탐지합니다.</p>
<ul>
<li><strong>Dataset</strong>: Jigsaw Multilingual Toxic Comment Classification</li>
<li><strong>Goal</strong>: 정상 댓글을 유해하다고 잘못 판별할 확률(False Positive Rate)을 <img src="https://latex.codecogs.com/png.latex?%5Calpha"> 이하로 제어. (Type-1 Error Control)</li>
</ul>
</section>
<section id="methodology-conformal-outlier-detection" class="level2">
<h2 class="anchored" data-anchor-id="methodology-conformal-outlier-detection">Methodology: Conformal Outlier Detection</h2>
<p><strong>Section 4.4</strong>의 방법을 적용합니다.</p>
<ol type="1">
<li><strong>Heuristic Model</strong>: BERT 기반의 유해성 점수 예측 모델 <img src="https://latex.codecogs.com/png.latex?f(x)%20%5Cin%20%5B0,%201%5D">.</li>
<li><strong>Calibration</strong>: 정상 댓글(Non-toxic) <img src="https://latex.codecogs.com/png.latex?n">개에 대해 점수 <img src="https://latex.codecogs.com/png.latex?s_i%20=%20f(X_i)">를 계산하고 Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 구합니다.</li>
<li><strong>Detection</strong>: <img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D(x)%20=%20%5Cbegin%7Bcases%7D%20%5Ctext%7BNormal%7D%20&amp;%20%5Ctext%7Bif%20%7D%20f(x)%20%5Cle%20%5Chat%7Bq%7D%20%5C%5C%20%5Ctext%7BToxic%20(Outlier)%7D%20&amp;%20%5Ctext%7Bif%20%7D%20f(x)%20%3E%20%5Chat%7Bq%7D%20%5Cend%7Bcases%7D%20"></li>
</ol>
<p>이 방식은 “이 댓글은 유해합니다”라고 경보를 울릴 때, 그 경보가 오작동일 확률을 수학적으로 보장해줍니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/images/figure16_toxic.png" class="img-fluid figure-img"></p>
<figcaption>Figure 16: 유해 댓글 탐지 예시 (<img src="https://latex.codecogs.com/png.latex?%5Calpha=0.1">). 다국어 댓글에 대해 정상 댓글을 유해하다고 잘못 분류할 확률을 10%로 제한하면서, 실제 유해 댓글의 70%를 성공적으로 잡아냈다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="selective-classification-abstention" class="level1">
<h1>5. Selective Classification (Abstention)</h1>
<section id="problem-setup-4" class="level2">
<h2 class="anchored" data-anchor-id="problem-setup-4">Problem Setup</h2>
<p>모델이 확신할 수 없을 때는 <strong>“모르겠습니다(I don’t know)”</strong>라고 대답을 거부(Abstain)하는 시스템입니다. 목표는 대답을 거부하지 않고 예측을 내놓았을 때의 정확도가 <img src="https://latex.codecogs.com/png.latex?%5Cge%201-%5Calpha">가 되도록 하는 것입니다.</p>
<ul>
<li><strong>Dataset</strong>: ImageNet</li>
<li><strong>Goal</strong>: Selective Accuracy <img src="https://latex.codecogs.com/png.latex?%5Cge%2090%5C%25">.</li>
</ul>
</section>
<section id="methodology-learn-then-test" class="level2">
<h2 class="anchored" data-anchor-id="methodology-learn-then-test">Methodology: Learn then Test</h2>
<p>이 문제는 정확도(Accuracy)가 임계값 <img src="https://latex.codecogs.com/png.latex?%5Clambda">에 따라 단조 증가(Monotone)하지 않을 수 있기 때문에, 표준 CRC 대신 <strong>Learn then Test</strong> (Appendix A 내용) 프레임워크를 사용합니다.</p>
<ol type="1">
<li><p><strong>Risk Definition</strong>: <img src="https://latex.codecogs.com/png.latex?%20R(%5Clambda)%20=%20%5Cmathbb%7BP%7D(%5Ctext%7BError%7D%20%5Cmid%20%5Ctext%7BConfidence%7D%20%5Cge%20%5Clambda)%20"> 조건부 확률이므로 직접 제어하기 까다롭습니다.</p></li>
<li><p><strong>Empirical Estimate &amp; Upper Bound</strong>: Calibration Set에서 특정 신뢰도 <img src="https://latex.codecogs.com/png.latex?%5Clambda"> 이상인 샘플들의 에러율 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D(%5Clambda)">를 계산합니다. 이 에러율은 이항 분포(Binomial Distribution)를 따르므로, 클로퍼-피어슨(Clopper-Pearson) 구간 등을 이용해 <strong>에러율의 보수적 상한선(Upper Bound) <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D%5E+(%5Clambda)"></strong>을 구합니다.</p></li>
<li><p><strong>Scan &amp; Select</strong>: <img src="https://latex.codecogs.com/png.latex?%5Chat%7BR%7D%5E+(%5Clambda)%20%5Cle%20%5Calpha">를 만족하는 가장 작은 <img src="https://latex.codecogs.com/png.latex?%5Clambda">를 선택합니다.</p></li>
</ol>
<p>결과적으로 모델은 자신이 없으면 대답을 회피하고, 대답을 했을 때는 매우 높은 정확도를 보장하게 됩니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/images/figure17_selective.png" class="img-fluid figure-img"></p>
<figcaption>Figure 17: ImageNet에 대한 Selective Classification 결과. (왼쪽) <img src="https://latex.codecogs.com/png.latex?%5Clambda">가 커질수록(가로축), 예측을 수행하는 비율(주황색)은 줄어들지만, 정확도(파란색)는 올라간다. 점선은 목표 정확도 90%를 달성하는 지점을 나타낸다. (오른쪽) 모델이 예측한 예시(가오리)와 기권을 선택한 예시(여우).</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>이상의 5가지 예제는 Conformal Prediction이 단순히 이론적인 개념에 머무르지 않고, <strong>실제 현업의 복잡한 문제들을 해결하는 강력한 도구</strong>임을 보여줍니다.</p>
<ul>
<li><strong>Segmentation/Multilabel</strong>: 단순 에러율이 아닌 FNR 등 복잡한 Metric 제어.</li>
<li><strong>Time-series</strong>: 데이터 분포 변화(Drift)에 대한 적응.</li>
<li><strong>Outlier Detection</strong>: 비지도 학습 환경에서의 통계적 보장.</li>
<li><strong>Selective Classification</strong>: 고위험 환경에서의 안전한 AI.</li>
</ul>
<p>이 모든 과정에서 가장 중요한 것은 <strong>“적절한 Score Function의 정의”</strong>와 <strong>“올바른 Calibration 기법의 선택”</strong>입니다.</p>
<hr>
<p><strong>Next Step</strong>: 이제 Conformal Prediction의 핵심 이론과 응용 사례를 모두 살펴보았습니다. 다음 포스트에서는 이 모든 내용을 아우르는 <strong>Full Conformal Prediction</strong>과 문서를 마무리하는 논의들을 정리하며 시리즈를 마치겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-05-Worked-Examples/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 6)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-06-Full-Conformal-Prediction/</link>
  <description><![CDATA[ 





<section id="introduction-the-efficiency-trade-off" class="level1">
<h1>Introduction: The Efficiency Trade-off</h1>
<ul>
<li>지금까지 우리가 다룬 방식은 <strong>Split Conformal Prediction (Inductive CP)</strong>이었습니다.</li>
<li>이 방식은 데이터를 학습용(Train)과 보정용(Calibration)으로 나누어 사용합니다.
<ul>
<li><strong>장점</strong>: 계산이 매우 빠릅니다. 모델을 딱 한 번만 학습시키면 됩니다.</li>
<li><strong>단점</strong>: 데이터를 쪼개야 하므로 <strong>통계적 효율성(Statistical Efficiency)</strong>이 떨어집니다. 보정용 데이터는 학습에 기여하지 못하고, 학습용 데이터는 보정에 기여하지 못하기 때문입니다.</li>
</ul></li>
<li>데이터가 매우 귀하거나(예: 희귀병 임상 데이터), 예측의 정확도가 비용보다 훨씬 중요한 상황이라면 어떨까요?</li>
<li>이때 우리는 계산 비용을 감수하더라도 모든 데이터를 학습과 보정에 동시에 사용하는 <strong>Full Conformal Prediction (Transductive CP)</strong>을 고려해야 합니다.</li>
</ul>
</section>
<section id="the-core-idea-what-if" class="level1">
<h1>The Core Idea: “What if?”</h1>
<ul>
<li><p>Full CP의 핵심 아이디어는 새로운 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D">에 대해 <strong>“만약 정답이 <img src="https://latex.codecogs.com/png.latex?y">라면, 이 데이터가 기존 데이터들과 잘 어울리는가(Exchangeable)?”</strong>를 테스트하는 것입니다.</p></li>
<li><p>우리는 아직 정답 <img src="https://latex.codecogs.com/png.latex?Y_%7Bn+1%7D">을 모르지만, 가능한 모든 후보 <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathcal%7BY%7D">를 하나씩 대입해볼 수는 있습니다.</p></li>
<li><p>만약 <img src="https://latex.codecogs.com/png.latex?y">가 진짜 정답이라면, <img src="https://latex.codecogs.com/png.latex?(X_%7Bn+1%7D,%20y)">를 포함한 전체 데이터셋은 통계적으로 동질적(Exchangeable)이어야 합니다.</p></li>
</ul>
</section>
<section id="the-algorithm" class="level1">
<h1>The Algorithm</h1>
<ul>
<li>알고리즘은 모든 가능한 라벨 <img src="https://latex.codecogs.com/png.latex?y">에 대해 재학습(Retraining)을 수행해야 하므로 꽤 무겁습니다.</li>
</ul>
<section id="step-1-loop-over-all-possible-labels" class="level2">
<h2 class="anchored" data-anchor-id="step-1-loop-over-all-possible-labels">Step 1: Loop over all possible labels</h2>
<ul>
<li>테스트 포인트 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D">에 대해, 가능한 모든 정답 후보 <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathcal%7BY%7D"> (예: 분류 문제의 모든 클래스)에 대해 다음 과정을 반복합니다.</li>
</ul>
</section>
<section id="step-2-augment-and-retrain" class="level2">
<h2 class="anchored" data-anchor-id="step-2-augment-and-retrain">Step 2: Augment and Retrain</h2>
<ul>
<li>기존 데이터셋에 가상의 데이터 포인트 <img src="https://latex.codecogs.com/png.latex?(X_%7Bn+1%7D,%20y)">를 추가하여 확장된 데이터셋을 만듭니다.</li>
<li>그리고 이 데이터셋으로 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D%5Ey">를 <strong>처음부터 다시 학습</strong>시킵니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctext%7BTrain%20%7D%20%5Chat%7Bf%7D%5Ey%20%5Ctext%7B%20on%20%7D%20%5C%7B(X_1,%20Y_1),%20%5Cdots,%20(X_n,%20Y_n),%20(X_%7Bn+1%7D,%20y)%5C%7D%20"></p>
<blockquote class="blockquote">
<p><strong>주의</strong>: 이때 사용하는 학습 알고리즘은 데이터의 순서에 영향을 받지 않는 <strong>대칭적(Symmetric)</strong> 알고리즘이어야 합니다.</p>
</blockquote>
</section>
<section id="step-3-compute-scores" class="level2">
<h2 class="anchored" data-anchor-id="step-3-compute-scores">Step 3: Compute Scores</h2>
<ul>
<li>학습된 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D%5Ey">를 사용하여, 확장된 데이터셋 내의 모든 포인트(<img src="https://latex.codecogs.com/png.latex?i=1%20%5Cdots%20n+1">)에 대해 Conformal Score를 계산합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20s_i%5Ey%20=%20s(X_i,%20Y_i,%20%5Chat%7Bf%7D%5Ey)%20%5Cquad%20%5Ctext%7Bfor%20%7D%20i=1,%20%5Cdots,%20n%20"> <img src="https://latex.codecogs.com/png.latex?%20s_%7Bn+1%7D%5Ey%20=%20s(X_%7Bn+1%7D,%20y,%20%5Chat%7Bf%7D%5Ey)%20"></p>
</section>
<section id="step-4-compute-quantile" class="level2">
<h2 class="anchored" data-anchor-id="step-4-compute-quantile">Step 4: Compute Quantile</h2>
<ul>
<li>이 <img src="https://latex.codecogs.com/png.latex?n+1">개의 점수들 사이에서 <img src="https://latex.codecogs.com/png.latex?s_%7Bn+1%7D%5Ey">의 위치를 확인합니다.</li>
<li>정확히는 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 분위수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D%5Ey">를 계산하여, <img src="https://latex.codecogs.com/png.latex?s_%7Bn+1%7D%5Ey">가 이 안에 들어오는지 확인합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Chat%7Bq%7D%5Ey%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5C%7Bs_1%5Ey,%20%5Cdots,%20s_%7Bn+1%7D%5Ey%5C%7D%20;%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn+1%7D%20%5Cright)%20"></p>
</section>
<section id="step-5-construct-prediction-set" class="level2">
<h2 class="anchored" data-anchor-id="step-5-construct-prediction-set">Step 5: Construct Prediction Set</h2>
<ul>
<li>위 조건을 만족하는(즉, 기존 데이터들과 잘 섞이는) 모든 <img src="https://latex.codecogs.com/png.latex?y">를 예측 집합에 포함시킵니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D(X_%7Bn+1%7D)%20=%20%5C%7B%20y%20%5Cin%20%5Cmathcal%7BY%7D%20:%20s_%7Bn+1%7D%5Ey%20%5Cle%20%5Chat%7Bq%7D%5Ey%20%5C%7D%20"></p>
</section>
</section>
<section id="statistical-interpretation-permutation-test" class="level1">
<h1>Statistical Interpretation: Permutation Test</h1>
<ul>
<li>Full Conformal Prediction이 왜 작동하는지, 그리고 왜 “데이터를 섞어서(Permutation)” 판단하는지 이해하기 위해 통계학의 <strong>순열 검정(Permutation Test)</strong> 개념을 연결해보겠습니다.</li>
</ul>
<section id="the-null-hypothesis-of-exchangeability" class="level2">
<h2 class="anchored" data-anchor-id="the-null-hypothesis-of-exchangeability">The Null Hypothesis of Exchangeability</h2>
<ul>
<li>우리가 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Bn+1%7D">에 대해 어떤 가상의 정답 <img src="https://latex.codecogs.com/png.latex?y">를 부여했을 때, 핵심 질문은 이것입니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>“이 데이터 포인트 <img src="https://latex.codecogs.com/png.latex?(X_%7Bn+1%7D,%20y)">가 기존 데이터들과 구별되지 않고 잘 섞이는가?”</strong></p>
</blockquote>
<ul>
<li><p>이를 통계적 가설 검정의 언어로 표현하면 다음과 같습니다.</p></li>
<li><p><strong>귀무가설 (<img src="https://latex.codecogs.com/png.latex?H_0">):</strong> 데이터들 <img src="https://latex.codecogs.com/png.latex?Z_1,%20%5Cdots,%20Z_%7Bn+1%7D">은 <strong>교환 가능(Exchangeable)</strong>하다.</p></li>
<li><p>즉, 데이터의 순서를 뒤섞어도 그 결합 확률 분포는 변하지 않는다.</p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?Z_i%20=%20(X_i,%20Y_i)">입니다.</li>
</ul></li>
</ul>
</section>
<section id="the-test-statistic-p-value" class="level2">
<h2 class="anchored" data-anchor-id="the-test-statistic-p-value">The Test Statistic &amp; P-value</h2>
<ul>
<li>이 귀무가설을 기각하기 위해, 우리는 데이터가 얼마나 “튀는지”를 측정하는 검정 통계량 <img src="https://latex.codecogs.com/png.latex?T">를 정의합니다.</li>
<li>Full CP에서는 <strong>Nonconformity Score (<img src="https://latex.codecogs.com/png.latex?s">)</strong>가 바로 이 역할을 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20T(Z)%20=%20s(X,%20Y)%20"></p>
<ul>
<li>점수 <img src="https://latex.codecogs.com/png.latex?s">가 클수록 해당 데이터는 다른 데이터들과 이질적이라는(Exchangeability를 위반한다는) 증거가 됩니다.</li>
<li>우리는 관측된 데이터의 점수가, 데이터를 무작위로 섞었을 때 나올 수 있는 점수들과 비교해서 얼마나 큰지 <strong>p-value</strong>를 계산합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20p%20=%20%5Cfrac%7B%5Csum_%7B%5Csigma%20%5Cin%20S_%7Bn+1%7D%7D%20%5Cmathbb%7B1%7D%5C%7B%20T(Z_%7B%5Csigma(1)%7D,%20%5Cdots,%20Z_%7B%5Csigma(n+1)%7D)%20%5Cge%20T(Z_%7Bobserved%7D)%20%5C%7D%7D%7B(n+1)!%7D%20"></p>
<ul>
<li>하지만 Full CP에서는 모든 순열을 다 계산할 필요 없이, 각 데이터 포인트의 점수 <img src="https://latex.codecogs.com/png.latex?s_i">들의 순위(Rank)만 보면 됩니다.</li>
<li>따라서 p-value는 다음과 같이 단순화됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%20p(y)%20=%20%5Cfrac%7B1%7D%7Bn+1%7D%20%5Csum_%7Bi=1%7D%5E%7Bn+1%7D%20%5Cmathbb%7B1%7D%5C%7B%20s_i%5Ey%20%5Cge%20s_%7Bn+1%7D%5Ey%20%5C%7D%20"></p>
</section>
<section id="decision-rule-validity-theorem" class="level2">
<h2 class="anchored" data-anchor-id="decision-rule-validity-theorem">Decision Rule (Validity Theorem)</h2>
<ul>
<li>순열 검정의 핵심 정리에 따르면, 귀무가설(<img src="https://latex.codecogs.com/png.latex?H_0">)이 참일 때 p-value는 균등 분포(Uniform Distribution)를 따릅니다. 따라서 다음이 성립합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem (Validity of Permutation Test)</strong></p>
<p>모든 <img src="https://latex.codecogs.com/png.latex?%5Ctau%20%5Cin%20%5B0,%201%5D">과 모든 교환 가능한 분포 <img src="https://latex.codecogs.com/png.latex?P">에 대해: <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D_P(p%20%5Cle%20%5Ctau)%20%5Cle%20%5Ctau%20"></p>
</blockquote>
<ul>
<li><p>우리는 허용 가능한 에러율 <img src="https://latex.codecogs.com/png.latex?%5Calpha">를 설정했으므로, <strong>p-value가 <img src="https://latex.codecogs.com/png.latex?%5Calpha">보다 작으면(너무 희박한 확률이면)</strong> 귀무가설을 기각합니다.</p></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?p(y)%20%5Cle%20%5Calpha"> (기각):</strong> “<img src="https://latex.codecogs.com/png.latex?y">를 정답이라고 가정했더니, 이 데이터는 상위 <img src="https://latex.codecogs.com/png.latex?%5Calpha">% 안에 들 만큼 너무 이상해. <img src="https://latex.codecogs.com/png.latex?y">는 정답이 아닌 것 같아.” <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>예측 집합에서 제외.</strong></p></li>
<li><p><strong><img src="https://latex.codecogs.com/png.latex?p(y)%20%3E%20%5Calpha"> (채택):</strong> “<img src="https://latex.codecogs.com/png.latex?y">를 정답이라고 가정해도, 데이터가 전체 무리 속에 자연스럽게 섞여 들어가네. <img src="https://latex.codecogs.com/png.latex?y">는 정답일 수도 있어.” <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>예측 집합에 포함.</strong></p></li>
<li><p>결국 Full CP의 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X_%7Bn+1%7D)">은 <strong>“순열 검정에서 기각되지 않고 살아남은 모든 <img src="https://latex.codecogs.com/png.latex?y">들의 모임”</strong>입니다.</p></li>
<li><p>이것이 바로 Full CP가 수학적으로 엄밀한 커버리지를 보장하는 이유입니다.</p></li>
</ul>
<hr>
</section>
</section>
<section id="computational-cost-vs.-statistical-efficiency" class="level1">
<h1>Computational Cost vs.&nbsp;Statistical Efficiency</h1>
<section id="the-cost" class="level2">
<h2 class="anchored" data-anchor-id="the-cost">The Cost</h2>
<ul>
<li><p>이 방법의 가장 큰 문제는 <strong>계산 비용</strong>입니다.</p></li>
<li><p>테스트 데이터가 하나 들어올 때마다, 그리고 후보 클래스 <img src="https://latex.codecogs.com/png.latex?K">개마다 매번 모델을 재학습해야 합니다.</p></li>
<li><p>총 학습 횟수: <img src="https://latex.codecogs.com/png.latex?(n+1)%20%5Ctimes%20K"></p></li>
<li><p>따라서 딥러닝과 같이 학습이 오래 걸리는 모델이나, 회귀 문제(Regression)처럼 후보 <img src="https://latex.codecogs.com/png.latex?y">가 무한히 많은 경우에는 그대로 적용하기 어렵습니다.</p></li>
</ul>
</section>
<section id="the-benefit" class="level2">
<h2 class="anchored" data-anchor-id="the-benefit">The Benefit</h2>
<ul>
<li>하지만 데이터를 나누지 않고 <img src="https://latex.codecogs.com/png.latex?n">개를 모두 사용하므로, <strong>예측 집합이 더 작고 효율적(Sharp)</strong>입니다.</li>
<li>Split CP보다 정보 손실이 적어, 데이터가 적은 상황에서는 훨씬 강력한 성능을 발휘합니다.</li>
</ul>
</section>
</section>
<section id="theoretical-guarantee" class="level1">
<h1>Theoretical Guarantee</h1>
<ul>
<li>Full CP 역시 수학적으로 엄밀한 커버리지를 보장합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem 5 (Full conformal coverage guarantee)</strong></p>
<p>데이터가 교환 가능(Exchangeable)하고 학습 알고리즘이 대칭적(Symmetric)이라면, 다음이 성립한다.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BP%7D(Y_%7Bn+1%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Bn+1%7D))%20%5Cge%201-%5Calpha%20"></p>
</blockquote>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">Split Conformal (SCP)</th>
<th style="text-align: left;">Full Conformal (FCP)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Data Usage</strong></td>
<td style="text-align: left;">Train / Calib 분할 (비효율)</td>
<td style="text-align: left;">전체 데이터 사용 (효율)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Computation</strong></td>
<td style="text-align: left;">1회 학습 (빠름)</td>
<td style="text-align: left;"><img src="https://latex.codecogs.com/png.latex?K">회 재학습 (매우 느림)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Use Case</strong></td>
<td style="text-align: left;">Big Data, Deep Learning</td>
<td style="text-align: left;">Small Data, Statistically Critical Tasks</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Interpretation</strong></td>
<td style="text-align: left;">Validation Score Quantile</td>
<td style="text-align: left;">Permutation Test</td>
</tr>
</tbody>
</table>
<ul>
<li>Full CP는 계산 비용 때문에 현대 머신러닝에서는 잘 쓰이지 않지만, <strong>Jackknife+</strong>나 <strong>CV+</strong> (Section 6.2) 같은 기법들의 이론적 토대가 되는 매우 중요한 개념입니다.</li>
</ul>
<hr>
<p><strong>Final Note</strong>: 이것으로 “A Gentle Introduction to Conformal Prediction” 논문의 핵심적인 이론과 알고리즘들을 모두 정리했습니다. 이 시리즈가 여러분의 모델에 ’신뢰’를 더하는 데 도움이 되기를 바랍니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-06-Full-Conformal-Prediction/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 7)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-07-Historical-Notes-on-Conformal-Prediction/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Conformal Prediction(CP)은 하루아침에 만들어진 기술이 아닙니다. 이 “Distribution-free”하고 “Model-agnostic”한 방법론은 20세기 초반의 확률론 기초에 대한 논쟁부터 시작하여, 소련(Soviet) 수학의 거장들과 현대 통계학자들의 연구가 합쳐져 만들어진 거대한 지적 유산입니다.</p>
<p>이번 포스트에서는 Conformal Prediction이 어떻게 탄생했고, 어떤 과정을 거쳐 현대 머신러닝의 필수적인 불확실성 도구로 자리 잡게 되었는지 그 역사를 추적해 봅니다.</p>
</section>
<section id="origins-the-soviet-roots" class="level1">
<h1>1. Origins: The Soviet Roots</h1>
<p>CP의 탄생은 <strong>Vladimir Vovk</strong>의 어린 시절과 그의 스승 <strong>Andrey Kolmogorov</strong>로부터 시작됩니다.</p>
<section id="vladimir-vovk-andrey-kolmogorov" class="level2">
<h2 class="anchored" data-anchor-id="vladimir-vovk-andrey-kolmogorov">Vladimir Vovk &amp; Andrey Kolmogorov</h2>
<p>CP의 창시자로 널리 알려진 Vladimir Vovk는 우크라이나의 작은 광산 마을에서 자랐습니다. 모스크바 국립대학교(Moscow State University) 재학 시절, 그는 확률론의 거장 안드레이 콜모고로프(Andrey Kolmogorov)의 제자가 되었습니다.</p>
<p>당시 콜모고로프는 “무한한 시퀀스뿐만 아니라 유한한 시퀀스(Finite sequences)도 중요하다”는 점을 강조했습니다. 이는 훗날 CP가 <strong>“유한한 샘플(Finite-sample)”</strong>에서도 작동하는 이론적 토대가 됩니다.</p>
</section>
<section id="randomness-deficiency-nonconformity" class="level2">
<h2 class="anchored" data-anchor-id="randomness-deficiency-nonconformity">Randomness Deficiency &amp; Nonconformity</h2>
<p>CP의 핵심 개념인 <strong>Nonconformity Score(비순응 점수)</strong>는 알고리즘 정보 이론(Algorithmic Information Theory)의 <strong>Randomness Deficiency(무작위성 결핍)</strong>라는 개념에서 유래했습니다.</p>
<ul>
<li><strong>개념</strong>: 어떤 데이터 시퀀스가 주어졌을 때, 그 안에서 특이한(Atypical) 패턴을 찾아내는 것은 쉽지만(짧은 프로그램으로 설명 가능), 평범하고 무작위적인 패턴을 특정하는 것은 어렵습니다.</li>
<li><strong>연결</strong>:
<ul>
<li><strong>Randomness Deficiency가 크다</strong> <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 패턴이 뚜렷하고 특이하다 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>High Nonconformity Score (Outlier)</strong></li>
<li><strong>Randomness Deficiency가 작다</strong> <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> 패턴이 없고 무작위적이다 <img src="https://latex.codecogs.com/png.latex?%5Crightarrow"> <strong>Low Nonconformity Score (Inlier)</strong></li>
</ul></li>
</ul>
<p>이 아이디어는 훗날 데이터가 서로 <strong>교환 가능(Exchangeable)</strong>하다는 가정하에, 새로운 데이터가 기존 데이터와 얼마나 잘 어울리는지를 측정하는 도구로 발전했습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-07-Historical-Notes-on-Conformal-Prediction/images/figure18_vovk_kvant.png" class="img-fluid figure-img"></p>
<figcaption>Figure 18: Vladimir Vovk(좌)와 그가 어린 시절 영감을 받았던 잡지 ‘Kvant’(우). 콜모고로프가 편집했던 이 잡지는 물리, 수학, 공학 지식을 다루었다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="the-birth-of-conformal-prediction-1990s---2000s" class="level1">
<h1>2. The Birth of Conformal Prediction (1990s - 2000s)</h1>
<p>1990년대 후반, Vladimir Vovk는 Alexander Gammerman, Vladimir Vapnik 등과 협력하며 현대적인 CP의 기틀을 마련했습니다.</p>
<section id="key-milestones" class="level2">
<h2 class="anchored" data-anchor-id="key-milestones">Key Milestones</h2>
<ul>
<li><strong>1996-1999</strong>: Vovk, Gammerman, Saunders 등이 초기 CP 알고리즘을 개발. [cite_start]처음에는 e-values를 사용했으나 곧 p-values 기반으로 정립됨[cite: 1464].</li>
<li><strong>2002</strong>: <strong>Split Conformal Prediction (Inductive CP)</strong> 개발. [cite_start]계산 비용을 획기적으로 줄인 이 방법은 오늘날 가장 널리 쓰이는 표준이 됨[cite: 1464].</li>
<li><strong>2003</strong>: “Conformal Predictor”라는 용어가 Glenn Shafer에 의해 처음 명명됨.</li>
<li><strong>2005</strong>: 기념비적인 저서 <em>“Algorithmic Learning in a Random World”</em> 출판.</li>
</ul>
</section>
</section>
<section id="popularization-in-the-west-2010s" class="level1">
<h1>3. Popularization in the West (2010s)</h1>
<p>소련과 유럽을 중심으로 발전하던 CP는 2010년대에 들어 미국 통계학계의 주목을 받으며 폭발적으로 성장합니다.</p>
<section id="jing-lei-larry-wasserman" class="level2">
<h2 class="anchored" data-anchor-id="jing-lei-larry-wasserman">Jing Lei &amp; Larry Wasserman</h2>
<p>카네기 멜론 대학의 Jing Lei와 Larry Wasserman은 CP를 미국 주류 통계학계에 소개하는 데 결정적인 역할을 했습니다. * 그들은 CP를 <strong>Non-parametric Regression</strong>의 관점에서 재해석했습니다. * 특히 <strong>Conformalized Quantile Regression</strong>과 같은 아이디어는 딥러닝과 결합하기 매우 용이하여, CP가 머신러닝 커뮤니티로 퍼지는 기폭제가 되었습니다.</p>
<p>이들의 연구는 Emmanuel Candès, Rina Barber, Aaditya Ramdas, Ryan Tibshirani 같은 저명한 학자들을 이 분야로 끌어들였고, <strong>Adaptive Prediction Sets</strong>, <strong>Covariate Shift</strong>, <strong>Risk Control</strong> 등 우리가 앞선 섹션들에서 배운 현대적인 기법들이 쏟아져 나오기 시작했습니다.</p>
</section>
</section>
<section id="current-trends-future" class="level1">
<h1>4. Current Trends &amp; Future</h1>
<p>오늘날 CP는 단순한 예측 구간 생성을 넘어 다양한 분야로 확장되고 있습니다.</p>
<ol type="1">
<li><strong>Practical Performance</strong>: 더 작고 효율적인 예측 집합을 만들기 위한 연구 (Balanced coverage, Class-conditional etc.).</li>
<li><strong>Statistical Extensions</strong>:
<ul>
<li><strong>Risk Control</strong>: FNR, FDR 등 복잡한 손실 함수 제어.</li>
<li><strong>Distribution Shift</strong>: 데이터 분포가 변하는 상황(Time-series, Covariate shift)에서의 견고성 확보.</li>
<li><strong>Causal Inference</strong>: 인과 추론 및 개별 처치 효과(ITE)에 대한 신뢰 구간 생성.</li>
</ul></li>
<li><strong>Applications</strong>:
<ul>
<li><strong>Outlier Detection</strong>: 비지도 학습 환경에서의 이상치 탐지.</li>
<li><strong>Safe AI</strong>: 자율 주행, 의료 진단, 로봇 제어 등 고위험 환경에서의 안전장치.</li>
</ul></li>
</ol>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Conformal Prediction의 역사는 <strong>“가정 없는(Assumption-free) 통계적 추론”</strong>을 향한 인류의 긴 여정입니다. 초기 확률론의 철학적 논쟁에서 시작하여, 정보 이론의 추상적 개념을 거쳐, 이제는 누구나 Python 코드 몇 줄로 구현할 수 있는 실용적인 도구가 되었습니다.</p>
<p>논문의 저자들은 다음과 같이 마무리합니다. &gt; “The infant field of distribution-free uncertainty quantification has ample room for significant technical contributions… there is a lot more to be done!”</p>
<p>이제 여러분도 이 역사의 일부가 되어, 더 안전하고 신뢰할 수 있는 AI를 만드는 데 기여해 보시길 바랍니다.</p>
<hr>
<p><strong>References</strong>: 이 포스트는 <em>“A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification”</em> 의 Section 7을 요약 및 재구성한 것입니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-07-Historical-Notes-on-Conformal-Prediction/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 13. IV (Part 2)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/causal-inference-13-part-02/</link>
  <description><![CDATA[ 





<section id="개요-overview" class="level1">
<h1>1. 개요 (Overview)</h1>
<ul>
<li><p>지난 포스트에서 선형 모형(Linear Model) 가정 하의 IV를 다루었다면, 이번에는 <strong>무작위 실험(Randomized Experiment)에서 배정된 처치를 따르지 않는 ’불순응(Noncompliance)’이 발생했을 때</strong>의 IV 접근법을 다룹니다.</p></li>
<li><p>경제학에서 도구변수(IV)는 주로 무작위 실험이 아닌 관찰 연구(Observational Study)에서 ’자연 실험(Natural Experiment)’의 일종으로 사용됩니다.</p></li>
<li><p>좋은 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">가 되기 위해서는 다음 두 가지 조건을 만족해야 합니다.</p>
<ol type="1">
<li><strong>Relevance (관련성):</strong> 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">는 처치 <img src="https://latex.codecogs.com/png.latex?X">에 강한 영향을 미쳐야 합니다. (<img src="https://latex.codecogs.com/png.latex?Cov(X,Z)">가 커야 함). 그렇지 않으면(Weak IV), 추정량의 분산이 매우 커집니다.</li>
<li><strong>Exclusion Restriction (배제 제약):</strong> 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">는 결과 <img src="https://latex.codecogs.com/png.latex?Y">에 직접적인 영향을 미치지 않아야 하며, 오직 <img src="https://latex.codecogs.com/png.latex?X">를 통해서만 영향을 주어야 합니다.</li>
</ol></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/causal-inference-13-part-02/images/iv_noncompliance_dag.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: IV DAG regarding Noncompliance. Z(배정)가 X(실제 처치)를 통해 Y(결과)에 영향을 미치는 구조입니다. 점선 화살표는 잠재적 교란 요인을 의미합니다.</figcaption>
</figure>
</div>
</section>
<section id="불순응-문제와-잠재적-결과-noncompliance-potential-outcomes" class="level1">
<h1>2. 불순응 문제와 잠재적 결과 (Noncompliance &amp; Potential Outcomes)</h1>
<ul>
<li>Angrist, Imbens, and Rubin (1996, JASA)은 불순응 문제를 해결하기 위해 <strong>잠재적 결과(Potential Outcomes)</strong> 프레임워크를 도입했습니다.</li>
</ul>
<section id="변수-정의" class="level2">
<h2 class="anchored" data-anchor-id="변수-정의">2.1. 변수 정의</h2>
<ul>
<li><strong><img src="https://latex.codecogs.com/png.latex?Z_i"> (Instrument/Assignment):</strong> 처치 배정 여부 (0 또는 1). 무작위 배정(Random Assignment)을 가정합니다.</li>
<li><strong><img src="https://latex.codecogs.com/png.latex?X_i"> (Treatment Received):</strong> 실제 처치를 받았는지 여부. <img src="https://latex.codecogs.com/png.latex?Z"> 이후에 결정되므로(Post-treatment), <img src="https://latex.codecogs.com/png.latex?Z">에 따른 잠재적 결과를 가집니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X_i(z)">: <img src="https://latex.codecogs.com/png.latex?Z=z">일 때 개인이 받게 될 처치 여부.</li>
</ul></li>
<li><strong><img src="https://latex.codecogs.com/png.latex?Y_i"> (Outcome):</strong> 결과 변수.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y_i(z,%20x)">: 배정이 <img src="https://latex.codecogs.com/png.latex?z">이고 실제 처치가 <img src="https://latex.codecogs.com/png.latex?x">일 때의 잠재적 결과.</li>
<li>일반적으로 Exclusion Restriction 가정 하에서 <img src="https://latex.codecogs.com/png.latex?Y_i(z=0,%20x)%20=%20Y_i(z=0,%201)">이므로 <img src="https://latex.codecogs.com/png.latex?Y_i(x)">로 표기.</li>
</ul></li>
<li>우리가 관측하는 데이터는 <img src="https://latex.codecogs.com/png.latex?Z_i,%20X_i%20=%20X_i(Z_i),%20Y_i%20=%20Y_i(Z_i)"> 입니다.</li>
</ul>
</section>
<section id="순응-유형-compliance-types" class="level2">
<h2 class="anchored" data-anchor-id="순응-유형-compliance-types">2.2. 순응 유형 (Compliance Types)</h2>
<ul>
<li>사람들이 처치 배정(<img src="https://latex.codecogs.com/png.latex?Z">)에 어떻게 반응하여 실제 처치(<img src="https://latex.codecogs.com/png.latex?X">)를 받는지에 따라 <strong>4가지 잠재적 집단(Latent Subgroups)</strong>으로 나눌 수 있습니다.</li>
<li>이를 <img src="https://latex.codecogs.com/png.latex?X_i(0)">와 <img src="https://latex.codecogs.com/png.latex?X_i(1)">의 조합으로 정의합니다.</li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 27%">
<col style="width: 27%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Type</th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?X_i(0)"></th>
<th style="text-align: center;"><img src="https://latex.codecogs.com/png.latex?X_i(1)"></th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Never-taker</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;">배정을 받든 안 받든 처치를 받지 않음</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Complier</strong></td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;">배정받으면 처치 받고, 안 받으면 안 받음 (순응자)</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Defier</strong></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: left;">배정과 반대로 행동함 (청개구리)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Always-taker</strong></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;">배정을 받든 안 받든 항상 처치를 받음</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Note:</strong> 우리는 각 개인의 진짜 Type(<img src="https://latex.codecogs.com/png.latex?S">)을 완벽하게 알 수 없습니다. 예를 들어 관측된 데이터에서 <img src="https://latex.codecogs.com/png.latex?Z=1,%20X=1">인 사람은 ’Complier’일 수도 있고 ’Always-taker’일 수도 있습니다.</p>
</blockquote>
</section>
</section>
<section id="인과-효과의-식별-identification" class="level1">
<h1>3. 인과 효과의 식별 (Identification)</h1>
<ul>
<li>우리의 목표는 인과 효과를 추정하는 것입니다.</li>
<li>이를 위해 <strong>ITT(Intent-To-Treat)</strong> 효과를 먼저 정의하고, 이를 분해하여 특정 집단의 효과를 구해봅시다.</li>
</ul>
<section id="itt-intent-to-treat-effects" class="level2">
<h2 class="anchored" data-anchor-id="itt-intent-to-treat-effects">3.1. ITT (Intent-To-Treat) Effects</h2>
<ul>
<li><p>ITT 효과는 처치 배정(<img src="https://latex.codecogs.com/png.latex?Z">)이 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 미치는 인과적 효과입니다.</p></li>
<li><p>개인의 순응 유형(<img src="https://latex.codecogs.com/png.latex?S">)은 <img src="https://latex.codecogs.com/png.latex?Z">에 따라 변하지 않는 ’Baseline Characteristic’으로 간주할 수 있습니다.</p></li>
<li><p>각 유형 <img src="https://latex.codecogs.com/png.latex?s%20%5Cin%20%5C%7Bc,%20n,%20a,%20d%5C%7D">에 대한 ITT 효과는 다음과 같습니다. <img src="https://latex.codecogs.com/png.latex?ITT_s%20=%20%5Cmathbb%7BE%7D%5BY_i(1)%20-%20Y_i(0)%20%7C%20S_i%20=%20s%5D"></p></li>
<li><p>전체 모집단에 대한 Global ITT는 각 유형별 ITT의 가중 평균으로 표현됩니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AITT%20&amp;=%20%5Cmathbb%7BE_s%7D%5B%5Cmathbb%7BE%7D%5BY_i(1)%20-%20Y_i(0)%20%7C%20S_i%20=%20s%5D%5D%20%5C%5C%0A&amp;=%20%5Cpi_c%20ITT_c%20+%20%5Cpi_n%20ITT_n%20+%20%5Cpi_a%20ITT_a%20+%20%5Cpi_d%20ITT_d%0A%5Cend%7Baligned%7D%0A"> * 여기서 <img src="https://latex.codecogs.com/png.latex?%5Cpi_s">는 각 유형의 구성 비율입니다.</p>
</section>
<section id="식별을-위한-가정-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="식별을-위한-가정-assumptions">3.2. 식별을 위한 가정 (Assumptions)</h2>
<ul>
<li><p>비모수적 식별(Nonparametric Identification)을 위해 다음 3가지 핵심 가정이 필요합니다.</p></li>
<li><p><strong>Assumption 1: Random Assignment (무작위 배정)</strong> <img src="https://latex.codecogs.com/png.latex?%5C%7BY_i(0),%20Y_i(1),%20X_i(0),%20X_i(1)%5C%7D%20%5Cperp%20Z_i"></p>
<ul>
<li>도구변수 <img src="https://latex.codecogs.com/png.latex?Z">가 잠재적 결과들과 독립이라는 가정입니다.</li>
</ul></li>
<li><p><strong>Assumption 2: Strong Monotonicity (강한 단조성)</strong></p>
<ul>
<li><ol type="1">
<li><strong>No Defiers:</strong></li>
</ol>
<ul>
<li>모든 <img src="https://latex.codecogs.com/png.latex?i">에 대해 <img src="https://latex.codecogs.com/png.latex?X_i(1)%20%5Cge%20X_i(0)"></li>
<li>우리의 예시에서는 <img src="https://latex.codecogs.com/png.latex?Z,%20X%20%5Cin%20%5C%7B0,%201%5C%7D"> 이므로, <img src="https://latex.codecogs.com/png.latex?X_i(1)%20=%201%20%5Cge%20X_i(0)%20=%200"></li>
<li>즉, 배정을 받았을 때 처치를 받을 확률이 배정을 안 받았을 때보다 작아지지는 않는다는 것입니다.</li>
<li>이 가정에 의해 <img src="https://latex.codecogs.com/png.latex?%5Cpi_d%20=%200">이 됩니다.</li>
</ul></li>
<li><ol start="2" type="1">
<li><strong>Existence of Compliers:</strong></li>
</ol>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?0%20%3C%20P(X_i=1%20%7C%20Z_i=1)%20%3C%201"></li>
<li>Complier가 존재해야 합니다.</li>
</ul></li>
</ul></li>
<li><p><strong>Assumption 3: Exclusion Restriction (ER) for Noncompliers</strong></p>
<ul>
<li>불순응자(Never-taker, Always-taker)에게는 배정(<img src="https://latex.codecogs.com/png.latex?Z">) 자체가 결과(<img src="https://latex.codecogs.com/png.latex?Y">)에 영향을 주지 않아야 합니다.</li>
<li>이들은 어차피 <img src="https://latex.codecogs.com/png.latex?Z">와 상관없이 동일한 <img src="https://latex.codecogs.com/png.latex?X">를 선택하기 때문입니다. <img src="https://latex.codecogs.com/png.latex?Y_i(1)%20=%20Y_i(0)%20%5Cquad%20%5Ctext%7Bfor%20%7D%20S_i%20%5Cin%20%5C%7Bn,%20a%5C%7D"></li>
</ul></li>
<li><p>따라서, <img src="https://latex.codecogs.com/png.latex?ITT_n%20=%200">, <img src="https://latex.codecogs.com/png.latex?ITT_a%20=%200"> 입니다.</p></li>
</ul>
</section>
</section>
<section id="cace-late-유도-derivation" class="level1">
<h1>4. CACE / LATE 유도 (Derivation)</h1>
<ul>
<li>위의 가정들을 <img src="https://latex.codecogs.com/png.latex?ITT"> 분해 식에 대입하여 <strong>Complier Average Causal Effect (CACE)</strong>를 유도해 봅시다.</li>
</ul>
<section id="유도-과정" class="level2">
<h2 class="anchored" data-anchor-id="유도-과정">4.1. 유도 과정</h2>
<ul>
<li>원래 식: <img src="https://latex.codecogs.com/png.latex?ITT%20=%20%5Cpi_c%20ITT_c%20+%20%5Cpi_n%20ITT_n%20+%20%5Cpi_a%20ITT_a%20+%20%5Cpi_d%20ITT_d"></li>
</ul>
<ol type="1">
<li><strong>Monotonicity 가정 적용:</strong> Defier가 없으므로 <img src="https://latex.codecogs.com/png.latex?%5Cpi_d%20=%200">. <img src="https://latex.codecogs.com/png.latex?%5CRightarrow%20ITT%20=%20%5Cpi_c%20ITT_c%20+%20%5Cpi_n%20ITT_n%20+%20%5Cpi_a%20ITT_a"></li>
<li><strong>Exclusion Restriction 가정 적용:</strong> Noncomplier의 ITT는 0이므로 <img src="https://latex.codecogs.com/png.latex?ITT_n%20=%200,%20ITT_a%20=%200">. <img src="https://latex.codecogs.com/png.latex?%5CRightarrow%20ITT%20=%20%5Cpi_c%20ITT_c"></li>
</ol>
<ul>
<li>결국 전체 ITT 효과는 <strong>Complier의 비율(<img src="https://latex.codecogs.com/png.latex?%5Cpi_c">) <img src="https://latex.codecogs.com/png.latex?%5Ctimes"> Complier의 ITT 효과(<img src="https://latex.codecogs.com/png.latex?ITT_c">)</strong>가 됩니다.</li>
<li>이를 <img src="https://latex.codecogs.com/png.latex?ITT_c">에 대해 정리하면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?ITT_c%20=%20%5Cfrac%7BITT%7D%7B%5Cpi_c%7D"></p>
<ul>
<li>이때, Complier에게는 <img src="https://latex.codecogs.com/png.latex?Z=X">이므로, <img src="https://latex.codecogs.com/png.latex?ITT_c">는 곧 Complier가 처치를 받았을 때의 인과 효과인 <strong>CACE</strong>가 됩니다.</li>
</ul>
</section>
<section id="complier-비율-pi_c-추정" class="level2">
<h2 class="anchored" data-anchor-id="complier-비율-pi_c-추정">4.2. Complier 비율 (<img src="https://latex.codecogs.com/png.latex?%5Cpi_c">) 추정</h2>
<ul>
<li>관측된 데이터로 <img src="https://latex.codecogs.com/png.latex?%5Cpi_c">를 어떻게 구할까요?</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D%5BX_i%20%7C%20Z_i%20=%201%5D%20&amp;=%20%5Cpi_c%20%5Ccdot%201%20+%20%5Cpi_a%20%5Ccdot%201%20+%20%5Cpi_n%20%5Ccdot%200%20%5Cquad%20(%5Ctext%7BComplier%20%5C&amp;%20Always-taker%7D)%20%5C%5C%0A%5Cmathbb%7BE%7D%5BX_i%20%7C%20Z_i%20=%200%5D%20&amp;=%20%5Cpi_c%20%5Ccdot%200%20+%20%5Cpi_a%20%5Ccdot%201%20+%20%5Cpi_n%20%5Ccdot%200%20%5Cquad%20(%5Ctext%7BAlways-taker%20only%7D)%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>두 식을 빼면:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BX_i%20%7C%20Z_i%20=%201%5D%20-%20%5Cmathbb%7BE%7D%5BX_i%20%7C%20Z_i%20=%200%5D%20=%20%5Cpi_c"></p>
<ul>
<li>이것은 <img src="https://latex.codecogs.com/png.latex?Z">가 <img src="https://latex.codecogs.com/png.latex?X">에 미치는 ITT 효과(<img src="https://latex.codecogs.com/png.latex?ITT_X">)와 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?ITT_X%20=%20%5Cmathbb%7BE%7D%5BX_i(1)%20-%20X_i(0)%5D%20=%20%5Cpi_c"></p>
</section>
<section id="cace-late-공식" class="level2">
<h2 class="anchored" data-anchor-id="cace-late-공식">4.3. CACE / LATE 공식</h2>
<ul>
<li>최종적으로 CACE는 다음과 같이 두 ITT의 비율로 표현됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?CACE%20%5Cequiv%20%5Cmathbb%7BE%7D%5BY_i(1)%20-%20Y_i(0)%20%7C%20S_i%20=%20c%5D%20=%20%5Cfrac%7BITT_Y%7D%7BITT_X%7D%20=%20%5Cfrac%7B%5Cmathbb%7BE%7D%5BY%7CZ=1%5D%20-%20%5Cmathbb%7BE%7D%5BY%7CZ=0%5D%7D%7B%5Cmathbb%7BE%7D%5BX%7CZ=1%5D%20-%20%5Cmathbb%7BE%7D%5BX%7CZ=0%5D%7D"></p>
<ul>
<li>이는 Imbens and Angrist (1994)에서 제시한 <strong>LATE (Local Average Treatment Effect)</strong>와 동일한 개념입니다. “Local”인 이유는 전체 모집단이 아닌 <strong>Complier</strong>라는 특정 하위 집단에 대한 효과이기 때문입니다.</li>
</ul>
<section id="잠재적-결과의-식별-identification-of-potential-outcomes" class="level3">
<h3 class="anchored" data-anchor-id="잠재적-결과의-식별-identification-of-potential-outcomes">4.4. 잠재적 결과의 식별 (Identification of Potential Outcomes)</h3>
<p>CACE를 계산하기 위해서는 Complier 집단의 잠재적 결과 평균(<img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_i(1)%7Cc%5D">와 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_i(0)%7Cc%5D">)을 알아야 합니다. 하지만 우리는 누가 Complier인지 개별적으로 알 수 없습니다.</p>
<p>대신, 관측 가능한 집단(<img src="https://latex.codecogs.com/png.latex?Z,%20X">)의 평균 결과가 어떤 잠재적 집단들의 혼합으로 이루어져 있는지 분해함으로써 이를 역추적할 수 있습니다.</p>
<p><strong>1. <img src="https://latex.codecogs.com/png.latex?Z=0,%20X=0"> 집단 (Compliers + Never-takers)</strong> 배정을 받지 않았고 처치도 받지 않은 그룹입니다. 이들은 <img src="https://latex.codecogs.com/png.latex?Z=0">일 때 <img src="https://latex.codecogs.com/png.latex?X=0">인 <strong>Complier</strong>와 <strong>Never-taker</strong>로 구성됩니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_i%20%7C%20X_i%20=%200,%20Z_i%20=%200%5D%20=%20%5Cfrac%7B%5Cpi_c%7D%7B%5Cpi_c%20+%20%5Cpi_n%7D%20%5Ccdot%20%5Cmathbb%7BE%7D%5BY_i(0)%20%7C%20%5Ctext%7Bcomplier%7D%5D%20+%20%5Cfrac%7B%5Cpi_n%7D%7B%5Cpi_c%20+%20%5Cpi_n%7D%20%5Ccdot%20%5Cmathbb%7BE%7D%5BY_i(0)%20%7C%20%5Ctext%7Bnever-taker%7D%5D%0A"></p>
<p><strong>2. <img src="https://latex.codecogs.com/png.latex?Z=1,%20X=0"> 집단 (Never-takers Only)</strong> 배정을 받았으나(<img src="https://latex.codecogs.com/png.latex?Z=1">) 처치를 받지 않은(<img src="https://latex.codecogs.com/png.latex?X=0">) 그룹입니다. Complier라면 처치를 받았을 것이므로, 이 그룹은 순수하게 <strong>Never-taker</strong>들입니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_i%20%7C%20X_i%20=%200,%20Z_i%20=%201%5D%20=%20%5Cmathbb%7BE%7D%5BY_i(0)%20%7C%20%5Ctext%7Bnever-taker%7D%5D%0A"></p>
<p><strong>3. <img src="https://latex.codecogs.com/png.latex?Z=0,%20X=1"> 집단 (Always-takers Only)</strong> 배정을 받지 않았는데도(<img src="https://latex.codecogs.com/png.latex?Z=0">) 처치를 받은(<img src="https://latex.codecogs.com/png.latex?X=1">) 그룹입니다. Complier라면 처치를 안 받았을 것이므로, 이 그룹은 순수하게 <strong>Always-taker</strong>들입니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_i%20%7C%20X_i%20=%201,%20Z_i%20=%200%5D%20=%20%5Cmathbb%7BE%7D%5BY_i(1)%20%7C%20%5Ctext%7Balways-taker%7D%5D%0A"></p>
<p><strong>4. <img src="https://latex.codecogs.com/png.latex?Z=1,%20X=1"> 집단 (Compliers + Always-takers)</strong> 배정을 받았고 처치도 받은 그룹입니다. 이들은 <img src="https://latex.codecogs.com/png.latex?Z=1">일 때 <img src="https://latex.codecogs.com/png.latex?X=1">인 <strong>Complier</strong>와 <strong>Always-taker</strong>로 구성됩니다. <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5BY_i%20%7C%20X_i%20=%201,%20Z_i%20=%201%5D%20=%20%5Cfrac%7B%5Cpi_c%7D%7B%5Cpi_c%20+%20%5Cpi_a%7D%20%5Ccdot%20%5Cmathbb%7BE%7D%5BY_i(1)%20%7C%20%5Ctext%7Bcomplier%7D%5D%20+%20%5Cfrac%7B%5Cpi_a%7D%7B%5Cpi_c%20+%20%5Cpi_a%7D%20%5Ccdot%20%5Cmathbb%7BE%7D%5BY_i(1)%20%7C%20%5Ctext%7Balways-taker%7D%5D%0A"></p>
<blockquote class="blockquote">
<p><strong>결론:</strong> 위 식들을 연립하면, 관측 가능한 데이터로부터 미지수인 <strong><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_i(0)%20%7C%20%5Ctext%7Bcomplier%7D%5D"></strong>와 <strong><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BY_i(1)%20%7C%20%5Ctext%7Bcomplier%7D%5D"></strong>를 비모수적으로 식별(Nonparametrically Identify)할 수 있으며, 이를 통해 최종적으로 CACE를 구할 수 있습니다. # 5. 추정량 (Estimators)</p>
</blockquote>
</section>
</section>
<section id="one-sided-noncompliance-단측-불순응" class="level2">
<h2 class="anchored" data-anchor-id="one-sided-noncompliance-단측-불순응">5.1. One-sided Noncompliance (단측 불순응)</h2>
<ul>
<li>실험 설계상 통제 집단(<img src="https://latex.codecogs.com/png.latex?Z=0">)은 절대 처치를 받을 수 없는 경우(예: 신약 임상시험)입니다.</li>
<li>즉, Always-taker가 없습니다(<img src="https://latex.codecogs.com/png.latex?X_i(0)=0">).</li>
<li>이 경우 <img src="https://latex.codecogs.com/png.latex?%5Csum%20X_i(1-Z_i)%20=%200"> 이 되며 공식은 조금 더 단순해집니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cwidehat%7BCACE%7D%20&amp;=%20%5Cfrac%7B%5Cwidehat%7BE%7D%5BY%7CZ=1%5D%20-%20%5Cwidehat%7BE%7D%5BY%7CZ=0%5D%7D%7B%5Cwidehat%7BE%7D%5BX%7CZ=1%5D%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Cfrac%7B%5Csum%20Y_i%20Z_i%7D%7B%5Csum%20Z_i%7D%20-%20%5Cfrac%7B%5Csum%20Y_i%20(1-Z_i)%7D%7B%5Csum%20(1-Z_i)%7D%7D%7B%5Cfrac%7B%5Csum%20X_i%20Z_i%7D%7B%5Csum%20Z_i%7D%7D%0A%5Cend%7Baligned%7D%0A"></p>
<pre><code>* 분모는 처치 집단 중 실제로 처치를 받은 사람의 비율</code></pre>
</section>
<section id="two-sided-noncompliance-양측-불순응" class="level2">
<h2 class="anchored" data-anchor-id="two-sided-noncompliance-양측-불순응">5.2. Two-sided Noncompliance (양측 불순응)</h2>
<ul>
<li>일반적인 경우(Wald Estimator 형태)입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BCACE%7D%20=%20%5Cfrac%7B%5Cwidehat%7BE%7D%5BY%7CZ=1%5D%20-%20%5Cwidehat%7BE%7D%5BY%7CZ=0%5D%7D%7B%5Cwidehat%7BE%7D%5BX%7CZ=1%5D%20-%20%5Cwidehat%7BE%7D%5BX%7CZ=0%5D%7D"></p>
</section>
</section>
<section id="결론" class="level1">
<h1>6. 결론</h1>
<ul>
<li>전통적인 선형 모형과 비교했을 때, Potential Outcome 프레임워크를 이용한 IV 접근법의 특징은 다음과 같습니다.</li>
</ul>
<ol type="1">
<li><strong>Constant Treatment Effect 가정 제거:</strong>
<ul>
<li>모든 사람에게 처치 효과가 동일하다는 비현실적인 가정을 하지 않습니다.</li>
</ul></li>
<li><strong>Monotonicity 가정 도입:</strong>
<ul>
<li>대신 단조성이라는 약한 가정을 도입합니다.</li>
</ul></li>
<li><strong>해석의 명확성:</strong>
<ul>
<li>우리가 구한 효과가 전체 인구에 대한 효과(ATE)가 아니라, 도구변수에 반응하여 행동을 바꾼 <strong>Complier 집단에 국한된 효과(LATE)</strong>임을 명확히 합니다.</li>
</ul></li>
</ol>
<blockquote class="blockquote">
<p><strong>Tip:</strong> 만약 Constant Treatment Effect가 성립한다면, LATE는 ATE와 같아집니다. 하지만 현실에서는 Complier에 대한 효과(<img src="https://latex.codecogs.com/png.latex?ITT_c">)가 전체 평균(<img src="https://latex.codecogs.com/png.latex?ITT">)보다 큰 경우가 많습니다 (<img src="https://latex.codecogs.com/png.latex?0%20%3C%20%5Cpi_c%20%3C%201"> 이므로 <img src="https://latex.codecogs.com/png.latex?ITT%20%3C%20ITT_c">). 즉, <img src="https://latex.codecogs.com/png.latex?ITT">는 처치 효과를 보수적으로 추정한 값으로 볼 수 있습니다.</p>
</blockquote>
<hr>
<p><strong>References:</strong> * Imbens, G. W., &amp; Angrist, J. D. (1994). Identification and Estimation of Local Average Treatment Effects. <em>Econometrica</em>. * Angrist, J. D., Imbens, G. W., &amp; Rubin, D. B. (1996). Identification of Causal Effects Using Instrumental Variables. <em>Journal of the American Statistical Association</em>.</p>



</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/causal-inference-13-part-02/</guid>
  <pubDate>Thu, 15 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-01-Conformal-Prediction/</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<ul>
<li><p>딥러닝 모델과 같은 Black-box 머신러닝 모델들은 의료 진단이나 자율 주행과 같은 High-risk 환경에서 일상적으로 사용되고 있습니다.</p></li>
<li><p>하지만 이러한 모델들은 종종 잘못된 예측을 내놓으면서도 높은 확신(Overconfidence)을 보이는 문제가 있습니다.</p></li>
<li><p><strong>Conformal Prediction(CP)</strong>는 이러한 모델의 예측에 대해 통계적으로 엄밀한 불확실성 구간(Uncertainty Sets/Intervals)을 생성하는 방법론입니다.</p></li>
<li><p>CP의 가장 큰 장점은 다음과 같습니다:</p>
<ol type="1">
<li><strong>Distribution-free</strong>: 데이터의 분포에 대한 가정(가우시안 분포 등)이 필요하지 않습니다.</li>
<li><strong>Model-agnostic</strong>: 뉴럴 네트워크를 포함한 어떤 학습된 모델(Pre-trained model)에도 적용 가능합니다.</li>
<li><strong>Finite-sample guarantee</strong>: 무한한 데이터가 아닌, 유한한 샘플 수에서도 통계적 커버리지(<img src="https://latex.codecogs.com/png.latex?1-%5Calpha">)를 보장합니다.</li>
</ol></li>
<li><p>이 글에서는 논문 <em>“A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification”</em>의 핵심 내용을 바탕으로 CP의 원리와 구현 방법을 정리합니다.</p></li>
</ul>
</section>
<section id="the-intuition-classification-example" class="level1">
<h1>The Intuition: Classification Example</h1>
<ul>
<li>CP를 이해하기 위해 가장 간단한 이미지 분류(Classification) 문제를 예로 들어보겠습니다.</li>
<li><img src="https://latex.codecogs.com/png.latex?K">개의 클래스를 분류하는 모델 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D">가 있다고 가정합시다.</li>
<li>이 모델은 입력 이미지 <img src="https://latex.codecogs.com/png.latex?x">에 대해 각 클래스에 속할 확률(Softmax score)을 출력합니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-01-Conformal-Prediction/images/figure1_prediction_sets.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: ImageNet 데이터셋에 대한 Prediction Set 예시. 다람쥐(Squirrel)와 같이 모델이 헷갈리는 이미지에 대해서는 여러 클래스를 포함하는 Set을 출력하여 정답을 포함할 확률을 보장한다.</figcaption>
</figure>
</div>
<ul>
<li><p>우리의 목표는 단순히 가장 높은 확률을 가진 하나의 라벨을 뱉는 것이 아니라, <strong>정답 라벨 <img src="https://latex.codecogs.com/png.latex?Y">를 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">(예: 90%)의 확률로 포함하는 후보 라벨들의 집합(Set) <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X)"></strong>를 만드는 것입니다. <img src="https://latex.codecogs.com/png.latex?%0A1-%5Calpha%20%5Cle%20%5Cmathbb%7BP%7D(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cle%201-%5Calpha%20+%20%5Cfrac%7B1%7D%7Bn+1%7D%0A"></p></li>
<li><p>위 식은 <strong>Marginal Coverage</strong>라고 불리며, Calibration 데이터와 테스트 데이터의 무작위성을 평균했을 때 예측 집합이 정답을 포함할 확률이 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 이상임을 의미합니다.</p></li>
</ul>
</section>
<section id="the-conformal-prediction-algorithm" class="level1">
<h1>The Conformal Prediction Algorithm</h1>
<ul>
<li>CP는 복잡한 최적화 과정 없이 <strong>Calibration Step</strong>이라고 불리는 간단한 절차를 통해 수행됩니다.</li>
<li>핵심은 모델이 학습 과정에서 보지 못한 <strong>Calibration Data</strong> (약 500개 정도의 소규모 데이터)를 사용하는 것입니다.</li>
</ul>
<section id="step-1-calibration-data-준비" class="level2">
<h2 class="anchored" data-anchor-id="step-1-calibration-data-준비">Step 1: Calibration Data 준비</h2>
<ul>
<li>학습에 사용되지 않은 <img src="https://latex.codecogs.com/png.latex?n">개의 데이터 쌍 <img src="https://latex.codecogs.com/png.latex?(X_1,%20Y_1),%20%5Cdots,%20(X_n,%20Y_n)">을 준비합니다.</li>
<li>이 데이터는 교환 가능(Exchangeable), 일반적으로는 i.i.d. 가정만 만족하면 됩니다.</li>
</ul>
</section>
<section id="step-2-conformal-score-계산" class="level2">
<h2 class="anchored" data-anchor-id="step-2-conformal-score-계산">Step 2: Conformal Score 계산</h2>
<ul>
<li>각 Calibration 데이터에 대해 모델이 얼마나 “잘못” 예측했는지를 나타내는 <strong>Conformal Score</strong> <img src="https://latex.codecogs.com/png.latex?s_i">를 계산합니다.</li>
<li>분류 문제에서 가장 일반적인 점수 함수는 다음과 같습니다:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0As_i%20=%201%20-%20%5Chat%7Bf%7D(X_i)_%7BY_i%7D%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(X_i)_%7BY_i%7D">는 정답 클래스 <img src="https://latex.codecogs.com/png.latex?Y_i">에 대한 모델의 Softmax 확률입니다.</li>
<li>모델이 정답을 확신할수록 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bf%7D(X_i)_%7BY_i%7D%20%5Capprox%201">이므로 점수 <img src="https://latex.codecogs.com/png.latex?s_i">는 0에 가까워집니다.</li>
<li>모델이 틀렸거나 불확실할수록 점수 <img src="https://latex.codecogs.com/png.latex?s_i">는 커집니다.</li>
</ul>
</section>
<section id="step-3-quantile-구하기" class="level2">
<h2 class="anchored" data-anchor-id="step-3-quantile-구하기">Step 3: Quantile 구하기</h2>
<ul>
<li><p>우리는 새로운 데이터가 들어왔을 때, 모델의 불확실성(Score)이 어느 수준 이하이어야 안심할 수 있는지를 결정해야 합니다.</p></li>
<li><p>이를 위해 계산된 점수들 <img src="https://latex.codecogs.com/png.latex?s_1,%20%5Cdots,%20s_n">의 분포에서 <strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> (Quantile)</strong> 값을 찾습니다.</p></li>
<li><p>엄밀한 커버리지를 보장하기 위해 다음과 같은 보정된 분위수(Adjusted Quantile)를 사용합니다:</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bq%7D%20=%20%5Ctext%7BQuantile%7D%5Cleft(%20%5Cfrac%7B%5Clceil%20(n+1)(1-%5Calpha)%20%5Crceil%7D%7Bn%7D%20;%20%5C%7Bs_1,%20%5Cdots,%20s_n%5C%7D%20%5Cright)%0A"></p>
<ul>
<li>이 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> 값은 “전체 데이터의 <img src="https://latex.codecogs.com/png.latex?(1-%5Calpha)"> 비율이 이 점수보다 낮다”는 경계선 역할을 합니다.</li>
</ul>
</section>
<section id="step-4-prediction-set-구성-inference" class="level2">
<h2 class="anchored" data-anchor-id="step-4-prediction-set-구성-inference">Step 4: Prediction Set 구성 (Inference)</h2>
<ul>
<li>이제 새로운 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?X_%7Btest%7D">가 들어오면, 예측 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X_%7Btest%7D)">를 다음과 같이 구성합니다:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5C%7B%20y%20:%201%20-%20%5Chat%7Bf%7D(X_%7Btest%7D)_y%20%5Cle%20%5Chat%7Bq%7D%20%5C%7D%20=%20%5C%7B%20y%20:%20%5Chat%7Bf%7D(X_%7Btest%7D)_y%20%5Cge%201%20-%20%5Chat%7Bq%7D%20%5C%7D%0A"></p>
<ul>
<li>즉, 모델의 예측 확률이 <img src="https://latex.codecogs.com/png.latex?1-%5Chat%7Bq%7D"> 이상인 모든 클래스를 후보로 포함시킵니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-01-Conformal-Prediction/images/figure2_algorithm_process.png" class="img-fluid figure-img"></p>
<figcaption>Figure 2: Conformal Prediction의 전체 프로세스 도식화 및 Python 코드 예시. (1) Score 계산, (2) Quantile 계산, (3) Prediction Set 구성의 3단계로 이루어진다.</figcaption>
</figure>
</div>
</section>
</section>
<section id="general-instructions-for-conformal-prediction" class="level1">
<h1>General Instructions for Conformal Prediction</h1>
<ul>
<li>앞서 설명한 분류 문제는 CP의 특수한 사례일 뿐입니다.</li>
<li>CP는 Regression, Segmentation 등 어떤 문제에도 적용할 수 있는 일반적인 프레임워크입니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-01-Conformal-Prediction/images/diagram_heuristic_to_rigorous.png" class="img-fluid figure-img"></p>
<figcaption>Diagram 1: Heuristic Notion of Uncertainty를 CP를 통해 Rigorous Uncertainty로 변환하는 과정</figcaption>
</figure>
</div>
<p>일반화된 CP 알고리즘은 다음과 같습니다:</p>
<ol type="1">
<li><p><strong>Heuristic Notion of Uncertainty 식별</strong>: Pre-trained 모델을 사용하여 불확실성을 나타내는 지표를 정의합니다.</p></li>
<li><p><strong>Score Function 정의</strong>: <img src="https://latex.codecogs.com/png.latex?s(x,%20y)%20%5Cin%20%5Cmathbb%7BR%7D">. 점수가 클수록 모델의 예측 <img src="https://latex.codecogs.com/png.latex?x">와 실제값 <img src="https://latex.codecogs.com/png.latex?y"> 사이의 불일치(Error)가 큼을 의미해야 합니다.</p></li>
<li><p><strong>Quantile <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D"> 계산</strong>: Calibration 데이터셋에 대해 Score를 계산하고, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Clceil(n+1)(1-%5Calpha)%5Crceil%7D%7Bn%7D"> 분위수를 구합니다.</p></li>
<li><p><strong>Prediction Set 생성</strong>: <img src="https://latex.codecogs.com/png.latex?%20%5Cmathcal%7BC%7D(X_%7Btest%7D)%20=%20%5C%7B%20y%20:%20s(X_%7Btest%7D,%20y)%20%5Cle%20%5Chat%7Bq%7D%20%5C%7D%20"> 이 집합은 Score가 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">보다 작거나 같은 모든 <img src="https://latex.codecogs.com/png.latex?y">를 포함합니다.</p></li>
</ol>
</section>
<section id="theoretical-guarantee" class="level1">
<h1>Theoretical Guarantee</h1>
<ul>
<li>Conformal Prediction이 강력한 이유는 다음의 정리에 의해 <strong>수학적으로 증명된 커버리지</strong>를 제공하기 때문입니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Theorem 1 (Conformal Coverage Guarantee)</strong></p>
<p>Calibration 데이터 <img src="https://latex.codecogs.com/png.latex?(X_i,%20Y_i)_%7Bi=1%7D%5En">와 테스트 데이터 <img src="https://latex.codecogs.com/png.latex?(X_%7Btest%7D,%20Y_%7Btest%7D)">가 i.i.d.라고 가정하자. 위에서 정의한 절차에 따라 <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bq%7D">를 계산하고 집합 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BC%7D(X_%7Btest%7D)">를 구성하면, 다음이 성립한다:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(Y_%7Btest%7D%20%5Cin%20%5Cmathcal%7BC%7D(X_%7Btest%7D))%20%5Cge%201%20-%20%5Calpha%20"></p>
</blockquote>
<ul>
<li>이 정리는 모델이 아무리 엉터리(Bad)여도 성립합니다.</li>
<li>하지만 <strong>“유용한(Useful)”</strong> 예측 집합을 얻기 위해서는 Score Function의 설계가 중요합니다.
<ul>
<li>Score Function이 불확실성을 잘 반영한다면: 쉬운 입력에는 집합 크기가 작고, 어려운 입력에는 커집니다 (Adaptive).</li>
<li>Score Function이 랜덤하다면: 집합 크기가 불필요하게 커지지만, 여전히 <img src="https://latex.codecogs.com/png.latex?1-%5Calpha"> 커버리지는 만족합니다.</li>
</ul></li>
<li>따라서 CP의 성패는 <strong>“좋은 Score Function을 어떻게 정의하느냐”</strong>에 달려 있습니다.</li>
</ul>
<hr>
<p><strong>Next Step</strong>: 다음 포스트에서는 이 일반화된 원리를 바탕으로 Classification과 Regression 문제에 적용할 수 있는 구체적인 Score Function 예시들(Adaptive Prediction Sets, Quantile Regression 등)을 다루겠습니다.</p>


</section>

 ]]></description>
  <category>Paper Review</category>
  <guid>https://shsha0110.github.io/posts/paper/A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification/Part-01-Conformal-Prediction/</guid>
  <pubDate>Wed, 14 Jan 2026 15:00:00 GMT</pubDate>
</item>
<item>
  <title>[Causal Inference] 13. IV (Part 1)</title>
  <dc:creator>유성현 </dc:creator>
  <link>https://shsha0110.github.io/posts/causal-inference-13-part-01/</link>
  <description><![CDATA[ 





<section id="개요-overview" class="level1">
<h1>1. 개요 (Overview)</h1>
<ul>
<li>인과추론에서 우리가 관심을 가지는 것은 처치(Treatment, <img src="https://latex.codecogs.com/png.latex?X">)가 결과(Outcome, <img src="https://latex.codecogs.com/png.latex?Y">)에 미치는 인과적 효과(<img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">)를 추정하는 것입니다.</li>
<li>하지만 많은 경우 관찰되지 않은 교란 요인(Unobserved Confounder)이 존재하여 단순한 회귀분석(OLS)으로는 편향된 추정치를 얻게 됩니다.</li>
<li>이때 사용할 수 있는 강력한 도구가 바로 <strong>도구변수(Instrumental Variable, IV)</strong>입니다.</li>
<li>이 글에서는 선형 가정(Linear Assumption) 하에서의 IV, 특히 <strong>2SLS (Two-Stage Least Squares)</strong> 접근법과 그 유도 과정을 다룹니다.</li>
</ul>
</section>
<section id="문제-상황-내생성-endogeneity" class="level1">
<h1>2. 문제 상황: 내생성 (Endogeneity)</h1>
<section id="인과-모형-causal-graph" class="level2">
<h2 class="anchored" data-anchor-id="인과-모형-causal-graph">2.1. 인과 모형 (Causal Graph)</h2>
<ul>
<li>우선 변수들 간의 관계를 DAG(Directed Acyclic Graph)로 표현하면 다음과 같습니다.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shsha0110.github.io/posts/causal-inference-13-part-01/images/iv_dag.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1: IV Causal Graph. Z는 도구변수, X는 처치, Y는 결과, W는 공변량, ε는 관찰되지 않은 오차항을 의미합니다.</figcaption>
</figure>
</div>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Z">: 도구변수 (Instrument)</li>
<li><img src="https://latex.codecogs.com/png.latex?X">: 처치변수 (Treatment) - <strong>Endogenous (내생적)</strong></li>
<li><img src="https://latex.codecogs.com/png.latex?Y">: 결과변수 (Outcome)</li>
<li><img src="https://latex.codecogs.com/png.latex?W">: 공변량 (Covariates) - <strong>Exogenous (외생적)</strong></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon">: 오차항 (Error term/Unobserved Confounders)</li>
</ul>
</section>
<section id="선형-모형과-ols의-한계" class="level2">
<h2 class="anchored" data-anchor-id="선형-모형과-ols의-한계">2.2. 선형 모형과 OLS의 한계</h2>
<ul>
<li>전통적인 선형 모형은 다음과 같이 설정됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20X_i%20+%20%5Cbeta_2%20W_i%20+%20%5Cepsilon_i"></p>
<ul>
<li>여기서 우리의 관심 추정량(estimand)은 <img src="https://latex.codecogs.com/png.latex?X_i">의 계수인 <strong><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"></strong>입니다.</li>
<li>하지만 <strong>Challenge</strong>가 발생합니다.</li>
<li>오차항 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i">가 처치 변수 <img src="https://latex.codecogs.com/png.latex?X_i">와 상관관계를 가질 때(Dependent/Confounded), 이를 <strong>내생성(Endogeneity)</strong>이 있다고 합니다.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Note:</strong></p>
<ul>
<li><strong>Endogenous (내생변수):</strong> <img src="https://latex.codecogs.com/png.latex?Y">와 관련된 교란요인(confounder)과 상관관계가 있는 변수 (<img src="https://latex.codecogs.com/png.latex?X">)</li>
<li><strong>Exogenous (외생변수):</strong> 교란요인과 상관관계가 없는 변수 (<img src="https://latex.codecogs.com/png.latex?W">)</li>
</ul>
</blockquote>
<ul>
<li>만약 <img src="https://latex.codecogs.com/png.latex?X">가 내생적이라면, <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">에 대한 직접적인 OLS(Ordinary Least Squares) 추정량은 <strong>편향(Biased)</strong>됩니다.</li>
<li>즉, <img src="https://latex.codecogs.com/png.latex?Cov(X,%20%5Cepsilon)%20%5Cneq%200">인 상황입니다.</li>
</ul>
</section>
</section>
<section id="도구변수-instrumental-variable" class="level1">
<h1>3. 도구변수 (Instrumental Variable)</h1>
<section id="iv의-조건" class="level2">
<h2 class="anchored" data-anchor-id="iv의-조건">3.1. IV의 조건</h2>
<ul>
<li>이 문제를 해결하기 위해 도입하는 도구변수 벡터 <img src="https://latex.codecogs.com/png.latex?Z"> (<img src="https://latex.codecogs.com/png.latex?K">차원)는 다음 두 가지 조건을 만족해야 합니다.</li>
</ul>
<ol type="1">
<li><strong>Relevance (관련성):</strong> 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">는 처치 <img src="https://latex.codecogs.com/png.latex?X">와 관련이 있어야 합니다. (그래프 상에서 <img src="https://latex.codecogs.com/png.latex?Z%20%5Crightarrow%20X"> 경로 존재)</li>
<li><strong>Exclusion Restriction (배제 제약) &amp; Independence:</strong> 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">는 오직 <img src="https://latex.codecogs.com/png.latex?X">를 통해서만 <img src="https://latex.codecogs.com/png.latex?Y">에 영향을 미쳐야 하며, 교란요인 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">과 독립이어야 합니다.
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i%20%5Cperp%20W_i"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i%20%5Cperp%20Z_i%20%7C%20W_i"></li>
<li>즉, <strong><img src="https://latex.codecogs.com/png.latex?%5Cepsilon_i%20%5Cperp%20(Z_i,%20W_i)"></strong></li>
</ul></li>
</ol>
</section>
<section id="식별-identification" class="level2">
<h2 class="anchored" data-anchor-id="식별-identification">3.2. 식별 (Identification)</h2>
<p>도구변수의 개수(<img src="https://latex.codecogs.com/png.latex?K">)와 내생변수의 개수에 따라 모델의 식별 상태가 달라집니다.</p>
<ul>
<li><strong>Over-identified (과잉 식별):</strong> <img src="https://latex.codecogs.com/png.latex?K%20%3E%201"> (도구변수가 내생변수보다 많음)</li>
<li><strong>Just-identified (적정 식별):</strong> <img src="https://latex.codecogs.com/png.latex?K%20=%201"> (도구변수와 내생변수 개수가 같음)</li>
</ul>
</section>
</section>
<section id="추정-방법-estimation-methods" class="level1">
<h1>4. 추정 방법 (Estimation Methods)</h1>
<section id="just-indentified-no-covariates" class="level2">
<h2 class="anchored" data-anchor-id="just-indentified-no-covariates">4.1. Just-indentified, No Covariates</h2>
<ul>
<li>Just-identified 케이스 (<img src="https://latex.codecogs.com/png.latex?K=1">)를 기준으로 세 가지 해석 방식을 살펴보겠습니다.</li>
<li>이때 공변량(W)는 없다고 가정합니다.</li>
<li>결론적으로 이 세 가지는 모두 동일한 <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> 값을 도출합니다.</li>
</ul>
<section id="공분산의-비율-ratio-of-covariance" class="level3">
<h3 class="anchored" data-anchor-id="공분산의-비율-ratio-of-covariance">4.1.1. 공분산의 비율 (Ratio of Covariance)</h3>
<ul>
<li>가장 직관적인 IV 추정량은 <img src="https://latex.codecogs.com/png.latex?Y">와 <img src="https://latex.codecogs.com/png.latex?Z">의 공분산을 <img src="https://latex.codecogs.com/png.latex?X">와 <img src="https://latex.codecogs.com/png.latex?Z">의 공분산으로 나눈 것입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%20iv%7D%20=%20%5Cfrac%7B%5Cwidehat%7BCov%7D(Y_i,%20Z_i)%7D%7B%5Cwidehat%7BCov%7D(X_i,%20Z_i)%7D%20=%20%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5Csum(Y_i%20-%20%5Cbar%7BY%7D)(Z_i%20-%20%5Cbar%7BZ%7D)%7D%7B%5Cfrac%7B1%7D%7BN%7D%5Csum(X_i%20-%20%5Cbar%7BX%7D)(Z_i%20-%20%5Cbar%7BZ%7D)%7D"></p>
<ul>
<li><strong>Wald Estimator (Binary IV의 경우):</strong>
<ul>
<li>만약 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">가 0과 1만 가지는 이진 변수라면, 이는 Wald Estimator가 됩니다. <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%20iv%7D%20=%20%5Cfrac%7B%5Cbar%7BY%7D_1%20-%20%5Cbar%7BY%7D_0%7D%7B%5Cbar%7BX%7D_1%20-%20%5Cbar%7BX%7D_0%7D"></li>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BY%7D_z,%20%5Cbar%7BX%7D_z">는 <img src="https://latex.codecogs.com/png.latex?Z=z">인 그룹 내에서의 평균을 의미합니다.</li>
</ul></li>
</ul>
</section>
<section id="간접-최소제곱법-indirect-least-squares-ils" class="level3">
<h3 class="anchored" data-anchor-id="간접-최소제곱법-indirect-least-squares-ils">4.1.2. 간접 최소제곱법 (Indirect Least Squares, ILS)</h3>
<ul>
<li>ILS는 두 개의 축약형(Reduced form) 회귀식을 이용하는 방식입니다.</li>
</ul>
<ol type="1">
<li><strong>Reduced Form 1 (<img src="https://latex.codecogs.com/png.latex?Y"> on <img src="https://latex.codecogs.com/png.latex?Z">):</strong> <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cpi_%7B10%7D%20+%20%5Cpi_%7B11%7D%20Z_i%20+%20%5Cepsilon_%7B1i%7D"></li>
<li><strong>Reduced Form 2 (<img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Z">):</strong> <img src="https://latex.codecogs.com/png.latex?X_i%20=%20%5Cpi_%7B20%7D%20+%20%5Cpi_%7B21%7D%20Z_i%20+%20%5Cepsilon_%7B2i%7D"></li>
</ol>
<ul>
<li>ILS 추정량은 두 회귀계수의 비율로 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%20ils%7D%20=%20%5Cfrac%7B%5Chat%7B%5Cpi%7D_%7B11%7D%7D%7B%5Chat%7B%5Cpi%7D_%7B21%7D%7D"></p>
</section>
<section id="단계-최소제곱법-two-stage-least-squares-2sls" class="level3">
<h3 class="anchored" data-anchor-id="단계-최소제곱법-two-stage-least-squares-2sls">4.1.3. 2단계 최소제곱법 (Two Stage Least Squares, 2SLS)</h3>
<ul>
<li><p>가장 널리 쓰이는 직관적인 방법으로, <img src="https://latex.codecogs.com/png.latex?X">에서 내생성을 제거한 후 회귀분석을 수행하는 방식입니다.</p></li>
<li><p><strong>Stage 1:</strong> 내생변수 <img src="https://latex.codecogs.com/png.latex?X">를 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">로 예측합니다. (OLS 수행) <img src="https://latex.codecogs.com/png.latex?X_i%20=%20%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cepsilon_i"> <img src="https://latex.codecogs.com/png.latex?%5CLongrightarrow%20%5Chat%7BX%7D_i%20=%20%5Chat%7B%5Cpi%7D_0%20+%20%5Chat%7B%5Cpi%7D_1%20Z_i"></p></li>
<li><p><strong>Stage 2:</strong> 원래의 <img src="https://latex.codecogs.com/png.latex?X"> 대신 예측된 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">를 사용하여 결과 모형을 추정합니다. <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cpi_2%20+%20%5Cpi_3%20%5Chat%7BX%7D_i%20+%20%5Cepsilon_i"> 여기서 구해진 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cpi%7D_3">가 바로 2SLS 추정량 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%202sls%7D">입니다.</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>Intuition:</strong></p>
<p>1단계를 통해 <img src="https://latex.codecogs.com/png.latex?X">의 변동 중 <img src="https://latex.codecogs.com/png.latex?Z">에 의해 설명되는 부분(즉, 오차항 <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">과 상관없는 “unconfounded portion”)만을 추출하여 2단계에서 인과 효과를 추정하는 것입니다.</p>
</blockquote>
</section>
<section id="결론" class="level3">
<h3 class="anchored" data-anchor-id="결론">4.1.4. 결론</h3>
<ul>
<li>위의 세 방식에 따른 추정치는 모두 동일합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%20iv%7D%20=%20%5Chat%7B%5Cbeta%7D_%7B1,%20ils%7D%20=%20%5Chat%7B%5Cbeta%7D_%7B1,%202sls%7D"></p>
<hr>
</section>
</section>
<section id="just-indentified-with-covariates" class="level2">
<h2 class="anchored" data-anchor-id="just-indentified-with-covariates">4.2. Just-indentified, With Covariates</h2>
<ul>
<li>Just-identified 케이스 (<img src="https://latex.codecogs.com/png.latex?K=1">)를 기준으로 두 가지 해석 방식을 살펴보겠습니다.</li>
<li>이때 공변량(W)을 함께 고려합니다.</li>
<li>결론적으로 이 두 가지는 모두 동일한 <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> 값을 도출합니다.</li>
</ul>
<section id="간접-최소제곱법-indirect-least-squares-ils-1" class="level3">
<h3 class="anchored" data-anchor-id="간접-최소제곱법-indirect-least-squares-ils-1">4.2.2. 간접 최소제곱법 (Indirect Least Squares, ILS)</h3>
<ul>
<li>ILS는 두 개의 축약형(Reduced form) 회귀식을 이용하는 방식입니다.</li>
</ul>
<ol type="1">
<li><strong>Reduced Form 1 (<img src="https://latex.codecogs.com/png.latex?Y"> on <img src="https://latex.codecogs.com/png.latex?Z">):</strong> <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cpi_%7B10%7D%20+%20%5Cpi_%7B11%7D%20Z_i%20+%20%5Cpi_%7B12%7D%20W_i%20+%20%5Cepsilon_%7B1i%7D"></li>
<li><strong>Reduced Form 2 (<img src="https://latex.codecogs.com/png.latex?X"> on <img src="https://latex.codecogs.com/png.latex?Z">):</strong> <img src="https://latex.codecogs.com/png.latex?X_i%20=%20%5Cpi_%7B20%7D%20+%20%5Cpi_%7B21%7D%20Z_i%20+%20%5Cpi_%7B22%7D%20Z_i%20+%20%5Cepsilon_%7B2i%7D"></li>
</ol>
<ul>
<li>ILS 추정량은 두 회귀계수의 비율로 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%20ils%7D%20=%20%5Cfrac%7B%5Chat%7B%5Cpi%7D_%7B11%7D%7D%7B%5Chat%7B%5Cpi%7D_%7B21%7D%7D"></p>
</section>
<section id="단계-최소제곱법-two-stage-least-squares-2sls-1" class="level3">
<h3 class="anchored" data-anchor-id="단계-최소제곱법-two-stage-least-squares-2sls-1">4.2.2. 2단계 최소제곱법 (Two Stage Least Squares, 2SLS)</h3>
<ul>
<li><p>가장 널리 쓰이는 직관적인 방법으로, <img src="https://latex.codecogs.com/png.latex?X">에서 내생성을 제거한 후 회귀분석을 수행하는 방식입니다.</p></li>
<li><p><strong>Stage 1:</strong> 내생변수 <img src="https://latex.codecogs.com/png.latex?X">를 도구변수 <img src="https://latex.codecogs.com/png.latex?Z">로 예측합니다. (OLS 수행) <img src="https://latex.codecogs.com/png.latex?X_i%20=%20%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20W_i%20+%20%5Cepsilon_%7B1i%7D"> <img src="https://latex.codecogs.com/png.latex?%5CLongrightarrow%20%5Chat%7BX%7D_i%20=%20%5Chat%7B%5Cpi%7D_0%20+%20%5Chat%7B%5Cpi%7D_1%20Z_i%20+%20%5Chat%7B%5Cpi%7D_2%20W_i%20"></p></li>
<li><p><strong>Stage 2:</strong> 원래의 <img src="https://latex.codecogs.com/png.latex?X"> 대신 예측된 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">를 사용하여 결과 모형을 추정합니다. <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20%5Chat%7BX%7D_i%20+%20%5Cbeta_2%20%5Chat%7BW%7D_i%20+%20%5Cepsilon_%7B2i%7D"> 여기서 구해진 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_1">이 바로 2SLS 추정량 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D_%7B1,%202sls%7D">입니다.</p></li>
</ul>
<hr>
</section>
</section>
</section>
<section id="수학적-유도-derivation" class="level1">
<h1>5. 수학적 유도 (Derivation)</h1>
<ul>
<li>수학적 증명을 단계별로 상세히 살펴보겠습니다.</li>
<li>공변량 <img src="https://latex.codecogs.com/png.latex?W">가 포함된 일반적인 경우를 가정합니다.</li>
</ul>
<section id="설정-setup" class="level2">
<h2 class="anchored" data-anchor-id="설정-setup">5.1. 설정 (Setup)</h2>
<ul>
<li>우리는 다음 두 식을 관찰할 수 있습니다.</li>
</ul>
<ol type="1">
<li><strong>First Stage (X에 대한 식):</strong> <img src="https://latex.codecogs.com/png.latex?X_i%20=%20%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20W_i%20+%20%5Cepsilon_%7B1i%7D"></li>
<li><strong>Structural Model (Y에 대한 식):</strong> <img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20%7BX%7D_i%20+%20%5Cunderbrace%7B%5Cbeta_2%20%7BW%7D_i%20+%20%5Cepsilon_%7B2i%7D%7D_%7B%5Ceta_%7Bi%7D%7D"></li>
</ol>
<ul>
<li>가정: <img src="https://latex.codecogs.com/png.latex?Cov%5BX_i,%20%5Ceta_i%5D%20%5Cneq%200"> (내생성 존재), 하지만 <img src="https://latex.codecogs.com/png.latex?Cov%5B%5Ceta_i,%20Z_i%5D%20=%200"> (도구변수의 외생성).</li>
</ul>
</section>
<section id="iv-estimator-유도-ratio-방식" class="level2">
<h2 class="anchored" data-anchor-id="iv-estimator-유도-ratio-방식">5.2. IV Estimator 유도 (Ratio 방식)</h2>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?Y_i"> 식의 양변과 <img src="https://latex.codecogs.com/png.latex?Z_i">의 공분산을 취해봅시다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0ACov%5BY_i,%20Z_i%5D%20&amp;=%20Cov%5B%5Cbeta_0%20+%20%5Cbeta_1%20X_i%20+%20%5Ceta_i,%20%5C;%20Z_i%5D%20%5C%5C%0A&amp;=%20%5Cbeta_1%20Cov%5BX_i,%20Z_i%5D%20+%20%5Cunderbrace%7BCov%5B%5Ceta_i,%20Z_i%5D%7D_%7B0%20%5Ctext%7B%20(by%20assumption)%7D%7D%20%5C%5C%0A&amp;=%20%5Cbeta_1%20Cov%5BX_i,%20Z_i%5D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>따라서, <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">에 대해 정리하면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1%20=%20%5Cfrac%7BCov%5BY_i,%20Z_i%5D%7D%7BCov%5BX_i,%20Z_i%5D%7D%20=%20%5Cfrac%7BCov%5BY_i,%20Z_i%5D%20/%20Var%5BZ_i%5D%7D%7BCov%5BX_i,%20Z_i%5D%20/%20Var%5BZ_i%5D%7D%20=%20%5Cfrac%7B%5Ctext%7BReduced%20form%20slope%7D%7D%7B%5Ctext%7BFirst%20stage%20slope%7D%7D"></p>
<ul>
<li>이 유도 과정은 <strong>ILS 방식(<img src="https://latex.codecogs.com/png.latex?%5Cpi_%7B11%7D/%5Cpi_%7B21%7D">)</strong>과 정확히 일치함을 보여줍니다.</li>
</ul>
</section>
<section id="sls-estimator-유도-substitution-방식" class="level2">
<h2 class="anchored" data-anchor-id="sls-estimator-유도-substitution-방식">5.3. 2SLS Estimator 유도 (Substitution 방식)</h2>
<ul>
<li><p>2SLS가 어떻게 작동하는지 수식 대입을 통해 확인해 봅시다.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?Y"> 식의 <img src="https://latex.codecogs.com/png.latex?X_i"> 자리에 First Stage 식(<img src="https://latex.codecogs.com/png.latex?%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20W_i%20+%20%5Cepsilon_%7B1i%7D">)을 대입합니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AY_i%20&amp;=%20%5Cbeta_0%20+%20%5Cbeta_1%20%7BX%7D_i%20+%20%5Cbeta_2%20%7BW%7D_i%20+%20%5Cepsilon_%7B2i%7D%20%5C%5C%0A&amp;=%20%5Cbeta_0%20+%20%5Cbeta_1(%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20W_i%20+%20%5Cepsilon_%7B1i%7D)%20+%20%5Cbeta_2%20%7BW%7D_i%20+%20%5Cepsilon_%7B2i%7D%20%5C%5C%0A&amp;=%20%5Cbeta_0%20+%20%5Cbeta_1%20(%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20%7BW%7D_i)%20+%20%5Cbeta_2%20%7BW%7D_i%20+%20%5Cunderbrace%7B%5Cbeta_1%20%5Cepsilon_%7B1i%7D%20+%20%5Cepsilon_%7B2i%7D%7D_%7B%5Cxi_i%7D%20%5C%5C%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>이를 다시 정리하면, <img src="https://latex.codecogs.com/png.latex?%5Cpi_0%20+%20%5Cpi_1%20Z_i%20+%20%5Cpi_2%20%7BW%7D_i"> 부분은 <img src="https://latex.codecogs.com/png.latex?E%5BX_i%20%7C%20W_i,%20Z_i%5D"> 즉, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D_i">의 핵심 부분임을 알 수 있습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?Y_i%20=%20%5Cbeta_0%20+%20%5Cunderbrace%7B%5Cbeta_1%20E%5BX_i%20%7C%20W_i,%20Z_i%5D%7D_%7B%5Chat%7BX_i%7D%7D%20+%20%5Cbeta_2%20W_i%20+%20%5Cxi_i"></p>
<ul>
<li>결국 <img src="https://latex.codecogs.com/png.latex?Y">를 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">와 <img src="https://latex.codecogs.com/png.latex?W">에 대해 회귀분석하는 형태가 되며, 이때 <img src="https://latex.codecogs.com/png.latex?%5Chat%7BX%7D">의 계수는 원래 식의 <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1">이 됩니다.</li>
</ul>
<hr>
</section>
</section>
<section id="행렬-연산-matrix-notation-for-2sls" class="level1">
<h1>6. 행렬 연산 (Matrix Notation for 2SLS)</h1>
<section id="recap-matrix-notation-for-ols-regression" class="level2">
<h2 class="anchored" data-anchor-id="recap-matrix-notation-for-ols-regression">6.1. Recap (Matrix Notation for OLS Regression)</h2>
<section id="선형-회귀-모형의-행렬-표현" class="level3">
<h3 class="anchored" data-anchor-id="선형-회귀-모형의-행렬-표현">6.1.1. 선형 회귀 모형의 행렬 표현</h3>
<ul>
<li>먼저, 선형 회귀 모형을 행렬 형태로 정의해 봅시다. 기본 식은 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20X%5Cbeta%20+%20u%0A"></p>
<ul>
<li>여기서 각 변수가 의미하는 바를 구체적인 행렬로 풀어서 쓰면 다음과 같습니다. <img src="https://latex.codecogs.com/png.latex?X">는 <img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k"> 행렬(matrix)입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Cbegin%7Bpmatrix%7D%20y_1%20%5C%5C%20%5Cvdots%20%5C%5C%20y_n%20%5Cend%7Bpmatrix%7D,%20%5Cquad%0AX%20=%20%5Cbegin%7Bpmatrix%7D%20x'_1%20%5C%5C%20%5Cvdots%20%5C%5C%20x'_n%20%5Cend%7Bpmatrix%7D%0A=%20%5Cbegin%7Bpmatrix%7D%0A1%20&amp;%20x_%7B12%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7B1k%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20%5C%5C%0A1%20&amp;%20x_%7Bn2%7D%20&amp;%20%5Ccdots%20&amp;%20x_%7Bnk%7D%0A%5Cend%7Bpmatrix%7D,%20%5Cquad%0Au%20=%20%5Cbegin%7Bpmatrix%7D%20u_1%20%5C%5C%20%5Cvdots%20%5C%5C%20u_n%20%5Cend%7Bpmatrix%7D%0A"></p>
<hr>
</section>
<section id="최소자승법ols-추정량" class="level3">
<h3 class="anchored" data-anchor-id="최소자승법ols-추정량">6.1.2. 최소자승법(OLS) 추정량</h3>
<ul>
<li>우리의 목표는 오차의 제곱합을 최소화하는 <img src="https://latex.codecogs.com/png.latex?%5Cbeta">의 추정량, 즉 <img src="https://latex.codecogs.com/png.latex?b">를 찾는 것입니다.</li>
<li>이를 위해 목적 함수 <img src="https://latex.codecogs.com/png.latex?S(b)">를 다음과 같이 정의합니다.</li>
<li>이는 관측값(<img src="https://latex.codecogs.com/png.latex?y">)과 예측값(<img src="https://latex.codecogs.com/png.latex?Xb">) 차이의 제곱합입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AS(b)%20=%20(y%20-%20Xb)'(y%20-%20Xb)%0A"></p>
<ul>
<li>OLS 추정량 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">는 이 <img src="https://latex.codecogs.com/png.latex?S(b)">를 최소화하는 값입니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5Cbeta%7D%20=%20%5Cunderset%7Bb%20%5Cin%20%5Cmathbb%7BR%7D%5Ek%7D%7B%5Carg%20%5Cmin%7D%20%5C,%20S(b)%20=%20(X'X)%5E%7B-1%7DX'y%0A"></p>
</section>
<section id="유도-과정-derivation" class="level3">
<h3 class="anchored" data-anchor-id="유도-과정-derivation">6.1.3. 유도 과정 (Derivation)</h3>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?S(b)">를 최소화하기 위해 <img src="https://latex.codecogs.com/png.latex?b">에 대해 미분을 수행합니다.</li>
</ul>
<section id="단계-1계-조건-first-order-condition-foc" class="level4">
<h4 class="anchored" data-anchor-id="단계-1계-조건-first-order-condition-foc">1단계: 1계 조건 (First Order Condition, FOC)</h4>
<ul>
<li>목적 함수를 <img src="https://latex.codecogs.com/png.latex?b">로 미분했을 때 0이 되어야 합니다.</li>
<li>행렬 미분 공식을 적용하면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Ctext%7BFOC%20:%20%7D%20%5Cquad%20%5Cfrac%7B%5Cpartial%20S(b)%7D%7B%5Cpartial%20b%7D%20&amp;=%20-2X'y%20+%202X'Xb%20%5C%5C%0A&amp;=%20-2X'(y%20-%20Xb)%20=%200%0A%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="단계-정규-방정식-normal-equation과-해-구하기" class="level4">
<h4 class="anchored" data-anchor-id="단계-정규-방정식-normal-equation과-해-구하기">2단계: 정규 방정식 (Normal Equation)과 해 구하기</h4>
<ul>
<li>1계 조건(FOC)에 따라, 최적의 추정량 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">는 다음의 <strong>정규 방정식(Normal Equation)</strong>을 만족해야 합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AX'(y%20-%20X%5Chat%7B%5Cbeta%7D)%20=%200%0A"></p>
<ul>
<li>이 식을 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbeta%7D">에 대해 정리해 봅시다.</li>
<li><img src="https://latex.codecogs.com/png.latex?X">가 <strong>Full Rank</strong>를 가진다고 가정하면, 역행렬이 존재하므로 유일한 해(unique solution)를 구할 수 있습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AX'y%20-%20X'X%5Chat%7B%5Cbeta%7D%20&amp;=%200%20%5C%5C%0AX'X%5Chat%7B%5Cbeta%7D%20&amp;=%20X'y%20%5C%5C%0A%5Ctherefore%20%5Cquad%20%5Chat%7B%5Cbeta%7D%20&amp;=%20(X'X)%5E%7B-1%7DX'y%0A%5Cend%7Baligned%7D%0A"></p>
</section>
</section>
<section id="기하학적-해석-geometric-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="기하학적-해석-geometric-interpretation">6.1.4 기하학적 해석 (Geometric Interpretation)</h3>
<ul>
<li>행렬 <img src="https://latex.codecogs.com/png.latex?X">를 열 벡터(column vector)들의 집합으로 생각해보겠습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AX%20=%20(x_1,%20...,%20x_k)%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?x_i">는 <img src="https://latex.codecogs.com/png.latex?X">의 <img src="https://latex.codecogs.com/png.latex?i">번째 열 벡터를 의미합니다.</li>
<li>이때 종속변수 벡터 <img src="https://latex.codecogs.com/png.latex?y">와 설명변수 벡터들 <img src="https://latex.codecogs.com/png.latex?x_1,%20...,%20x_k">는 모두 <img src="https://latex.codecogs.com/png.latex?n">차원 유클리드 공간 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En"> 상의 점(혹은 벡터)들입니다.</li>
</ul>
<section id="range-space-열공간의-정의" class="level4">
<h4 class="anchored" data-anchor-id="range-space-열공간의-정의">1. Range Space (열공간)의 정의</h4>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?X">의 <strong>Range Space</strong> (또는 Column Space, 열공간)인 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D(X)">는 다음과 같이 정의됩니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathcal%7BR%7D(X)%20&amp;=%20%5C%7B%20c%20%5Cin%20%5Cmathbb%7BR%7D%5En%20:%20c%20=%20Xb%20%5Ctext%7B%20for%20%7D%20b%20%5Cin%20%5Cmathbb%7BR%7D%5Ek%20%5C%7D%20%5C%5C%0A&amp;=%20%5C%7B%20c%20%5Cin%20%5Cmathbb%7BR%7D%5En%20:%20c%20=%20x_1b_1%20+%20%5Ccdots%20+%20x_kb_k,%20%5Cquad%20b%20=%20(b_1,%20...,%20b_k)'%20%5Cin%20%5Cmathbb%7BR%7D%5Ek%20%5C%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>즉, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D(X)">는 <strong><img src="https://latex.codecogs.com/png.latex?X">의 열 벡터(<img src="https://latex.codecogs.com/png.latex?x_1,%20...,%20x_k">)들의 선형 결합(linear combination)으로 만들 수 있는 모든 가능한 벡터들의 집합</strong>을 의미합니다.</li>
<li>수학적으로는 <img src="https://latex.codecogs.com/png.latex?X">의 열들에 의해 생성(span)된 <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D%5En">의 <strong>선형 부분공간(linear subspace)</strong>입니다.</li>
</ul>
</section>
<section id="회귀분석regression에서의-의미" class="level4">
<h4 class="anchored" data-anchor-id="회귀분석regression에서의-의미">2. 회귀분석(Regression)에서의 의미</h4>
<ul>
<li><p>이 기하학적 정의는 회귀분석의 목표를 명확하게 보여줍니다.</p></li>
<li><p>우리가 구하고자 하는 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7By%7D%20=%20Xb">는 정의상 반드시 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D(X)"> 안에 존재해야 합니다.</p></li>
<li><p>하지만 실제 관측값 <img src="https://latex.codecogs.com/png.latex?y">는 오차(error) 때문에 일반적으로 <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D(X)"> 바깥에 위치합니다.</p></li>
<li><p>따라서 회귀분석(Regression Problem)은 기하학적으로 다음과 같이 해석됩니다:</p></li>
</ul>
<blockquote class="blockquote">
<p><strong>“<img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BR%7D(X)"> 공간 안에 있는 수많은 점들 중에서, 실제 데이터 <img src="https://latex.codecogs.com/png.latex?y">와 가장 가까운 점(closest point)을 찾는 과정”</strong></p>
</blockquote>
<ul>
<li>이 ’가장 가까운 점’을 찾기 위해 우리는 수직 투영(Orthogonal Projection)을 사용하게 되며, 이것이 바로 최소자승법(OLS)의 원리입니다.</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="estimating-2sls-models" class="level2">
<h2 class="anchored" data-anchor-id="estimating-2sls-models">6.2. Estimating 2SLS Models</h2>
<section id="모형-설정-및-변수-정의" class="level3">
<h3 class="anchored" data-anchor-id="모형-설정-및-변수-정의">1. 모형 설정 및 변수 정의</h3>
<ul>
<li><p>2SLS(Two-Stage Least Squares) 모형을 추정하기 위해 변수들을 다음과 같이 정의합니다.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7By%7D">: 종속 변수 벡터 (<img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%201">)</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">: 내생 변수(Endogenous variables) 행렬 (<img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k_1">)</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">: 외생 변수(Exogenous regressors) 행렬 (<img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20k_2">)</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D">: 도구 변수(Instruments) 행렬. 여기에는 외생 변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">가 포함됩니다. (<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D%20=%20%5B%5Cmathbf%7BZ%7D_1%20%5C;%20%5Cmathbf%7BW%7D%5D">)</p></li>
<li><p>우리의 목표는 아래의 2SLS 추정량을 유도하는 것입니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7B%5CGamma%7D_%7B2SLS%7D%20=%20%5Cleft(%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D'%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D%20%5Cright)%5E%7B-1%7D%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D'%20%5Cmathbf%7By%7D%0A"></p>
<ul>
<li>여기서 <img src="https://latex.codecogs.com/png.latex?%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D">는 1단계 회귀분석을 통해 예측된 내생변수 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7BX%7D%7D">와 기존의 외생변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">를 합친 <strong>새로운 설명변수 행렬</strong> (<img src="https://latex.codecogs.com/png.latex?n%20%5Ctimes%20(k_1%20+%20k_2)">)입니다.</li>
<li>이를 원소별로 풀어서 쓰면 다음과 같습니다. <img src="https://latex.codecogs.com/png.latex?%0A%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D%20=%0A%5Cbegin%7Bpmatrix%7D%0A%5Chat%7Bx%7D_%7B1,1%7D%20&amp;%20%5Ccdots%20&amp;%20%5Chat%7Bx%7D_%7B1,k_1%7D%20&amp;%20w_%7B1,1%7D%20&amp;%20%5Ccdots%20&amp;%20w_%7B1,k_2%7D%20%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20&amp;%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%0A%5Chat%7Bx%7D_%7Bn,1%7D%20&amp;%20%5Ccdots%20&amp;%20%5Chat%7Bx%7D_%7Bn,k_1%7D%20&amp;%20w_%7Bn,1%7D%20&amp;%20%5Ccdots%20&amp;%20w_%7Bn,k_2%7D%0A%5Cend%7Bpmatrix%7D%0A">
<ul>
<li>좌측 블록(<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7BX%7D%7D">)은 도구변수(<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D">)로 예측된 값들로 구성되어 있고, 우측 블록(<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">)은 원래의 외생변수 값들이 그대로 들어갑니다.</li>
</ul></li>
</ul>
<hr>
</section>
<section id="사영-행렬projection-matrix-p_z-의-도입" class="level3">
<h3 class="anchored" data-anchor-id="사영-행렬projection-matrix-p_z-의-도입">2. 사영 행렬(Projection Matrix) <img src="https://latex.codecogs.com/png.latex?P_Z"> 의 도입</h3>
<ul>
<li>도구변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D">에 의한 사영 행렬(Projection Matrix)을 <img src="https://latex.codecogs.com/png.latex?P_Z">라고 표기하겠습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0AP_Z%20=%20%5Cmathbf%7BZ%7D(%5Cmathbf%7BZ%7D'%5Cmathbf%7BZ%7D)%5E%7B-1%7D%5Cmathbf%7BZ%7D'%0A"></p>
<ul>
<li><p>이 행렬 <img src="https://latex.codecogs.com/png.latex?P_Z">은 다음과 같은 매우 중요한 두 가지 성질을 가집니다.</p>
<ul>
<li><ol type="1">
<li><strong>대칭성 (Symmetric):</strong> <img src="https://latex.codecogs.com/png.latex?P_Z'%20=%20P_Z"></li>
</ol></li>
<li><ol start="2" type="1">
<li><strong>멱등성 (Idempotent):</strong> <img src="https://latex.codecogs.com/png.latex?P_ZP_Z%20=%20P_Z"></li>
</ol></li>
</ul></li>
</ul>
<hr>
</section>
</section>
<section id="a-부분의-유도-행렬의-곱" class="level2">
<h2 class="anchored" data-anchor-id="a-부분의-유도-행렬의-곱">3. A 부분의 유도 (행렬의 곱)</h2>
<ul>
<li><p>추정량의 앞부분인 역행렬 내부, 즉 <img src="https://latex.codecogs.com/png.latex?A%20=%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D'%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D">를 정리해 봅시다.</p></li>
<li><p>도구변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D">에 의한 사영 행렬(Projection Matrix)을 <strong><img src="https://latex.codecogs.com/png.latex?P_Z"></strong>라고 합시다. (<img src="https://latex.codecogs.com/png.latex?P_Z%20=%20%5Cmathbf%7BZ%7D(%5Cmathbf%7BZ%7D'%5Cmathbf%7BZ%7D)%5E%7B-1%7D%5Cmathbf%7BZ%7D'">)</p></li>
<li><p>1단계 예측값 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7BX%7D%7D">는 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BX%7D">를 투영한 것이므로 <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cmathbf%7BX%7D%7D%20=%20P_Z%5Cmathbf%7BX%7D">입니다.</p></li>
<li><p>외생변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D">는 도구변수 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D">에 포함되므로 투영해도 자신이 됩니다. 즉, <img src="https://latex.codecogs.com/png.latex?P_Z%5Cmathbf%7BW%7D%20=%20%5Cmathbf%7BW%7D">입니다.</p></li>
<li><p>따라서 전체 설명변수 행렬은 <img src="https://latex.codecogs.com/png.latex?P_Z">를 묶어 다음과 같이 쓸 수 있습니다.</p></li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%20%5C%5C%0A%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%20%5Cquad%20%5Cmathbf%7BW%7D%5D%20&amp;=%20%5BP_Z%5Cmathbf%7BX%7D%20%5Cquad%20P_Z%5Cmathbf%7BW%7D%5D%20%5C%5C%0A&amp;=%20P_Z%20%5B%5Cmathbf%7BX%7D%20%5Cquad%20%5Cmathbf%7BW%7D%5D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>이제 편의상 전체 설명변수 행렬을 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D%20=%20%5B%5Cmathbf%7BX%7D%20%5Cquad%20%5Cmathbf%7BW%7D%5D">라 정의하고 <img src="https://latex.codecogs.com/png.latex?A">를 전개합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AA%20&amp;=%20(P_Z%5Cmathbf%7BR%7D)'%20(P_Z%5Cmathbf%7BR%7D)%20%5C%5C%0A&amp;=%20%5Cmathbf%7BR%7D'%20P_Z'%20P_Z%20%5Cmathbf%7BR%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7BR%7D'%20P_Z%20P_Z%20%5Cmathbf%7BR%7D%20%5Cquad%20(%5Cbecause%20P_Z'%20=%20P_Z)%20%5C%5C%0A&amp;=%20%5Cmathbf%7BR%7D'%20P_Z%20%5Cmathbf%7BR%7D%20%5Cquad%20(%5Cbecause%20P_Z%20P_Z%20=%20P_Z)%0A%5Cend%7Baligned%7D%0A"></p>
<hr>
</section>
<section id="b-부분의-유도-y와의-곱" class="level2">
<h2 class="anchored" data-anchor-id="b-부분의-유도-y와의-곱">4. B 부분의 유도 (<img src="https://latex.codecogs.com/png.latex?y">와의 곱)</h2>
<ul>
<li>추정량의 뒷부분인 <img src="https://latex.codecogs.com/png.latex?B%20=%20%5B%5Chat%7B%5Cmathbf%7BX%7D%7D%5C;%20%5Cmathbf%7BW%7D%5D'%20%5Cmathbf%7By%7D"> 도 동일한 방식으로 전개합니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AB%20&amp;=%20(P_Z%5Cmathbf%7BR%7D)'%20%5Cmathbf%7By%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7BR%7D'%20P_Z'%20%5Cmathbf%7By%7D%20%5C%5C%0A&amp;=%20%5Cmathbf%7BR%7D'%20P_Z%20%5Cmathbf%7By%7D%20%5Cquad%20(%5Cbecause%20P_Z'%20=%20P_Z)%0A%5Cend%7Baligned%7D%0A"></p>
<hr>
</section>
<section id="최종-결과-2sls-추정량" class="level2">
<h2 class="anchored" data-anchor-id="최종-결과-2sls-추정량">5. 최종 결과: 2SLS 추정량</h2>
<ul>
<li>위의 <img src="https://latex.codecogs.com/png.latex?A">와 <img src="https://latex.codecogs.com/png.latex?B"> 결과를 합치면 다음과 같습니다.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Chat%7B%5CGamma%7D_%7B2SLS%7D%20&amp;=%20A%5E%7B-1%7D%20B%20%5C%5C%0A&amp;=%20(%5Cmathbf%7BR%7D'%20P_Z%20%5Cmathbf%7BR%7D)%5E%7B-1%7D%20%5Cmathbf%7BR%7D'%20P_Z%20%5Cmathbf%7By%7D%0A%5Cend%7Baligned%7D%0A"></p>
<ul>
<li>이 식은 <strong>“원래의 변수 행렬(<img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> 혹은 <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BW%7D_%7Ball%7D">)을 도구변수 공간에 투영(<img src="https://latex.codecogs.com/png.latex?P_Z">)시킨 뒤, 그 투영된 변수들을 사용하여 OLS를 수행하는 것”</strong>과 수학적으로 동일함을 보여줍니다.</li>
</ul>



</section>
</section>

 ]]></description>
  <category>Causal Inference</category>
  <guid>https://shsha0110.github.io/posts/causal-inference-13-part-01/</guid>
  <pubDate>Wed, 14 Jan 2026 15:00:00 GMT</pubDate>
</item>
</channel>
</rss>
