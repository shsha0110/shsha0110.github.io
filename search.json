[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html",
    "href": "posts/causal-inference-08-part-02/index.html",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "",
    "text": "지난 포스트에서는 가장 일반적인 형태의 비관측 교란 상황에서 Natural Bounds를 구하는 법을 배웠습니다.\n이번에는 인과추론에서 매우 중요한 환경인 도구 변수(Instrumental Variable, IV)가 주어졌을 때, 인과 효과의 범위를 어떻게 더 좁힐 수 있는지 알아봅니다.\n특히, 이 문제를 해결하기 위해 Canonical Type Models (CTM)을 도입하고, 이를 선형 계획법(Linear Programming) 문제로 변환하여 최적의 범위(Tight Bounds)를 찾아내는 과정을 다룹니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#natural-bounds-iv-모델",
    "href": "posts/causal-inference-08-part-02/index.html#natural-bounds-iv-모델",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "2.1 Natural Bounds (IV 모델)",
    "text": "2.1 Natural Bounds (IV 모델)\n\n관측 분포 \\(P(X, Y|Z)\\)가 주어졌을 때, 인과 효과 \\(P(y|do(x))\\)는 구간 \\([a(y, x), b(y, x)]\\) 안에 존재합니다. 여기서 각 경계값은 다음과 같습니다.\n\n\\[\na(y, x) = \\max_z P(y, x|z)\n\\] \\[\nb(y, x) = \\min_z [P(y, x|z) + 1 - P(x|z)]\n\\]\n\n모델 \\(\\mathcal{M}_z\\)에 Natural Bounds를 적용하면 아래와 같습니다.\n\n우선 Natural Bounds 식에서 시작합니다. \\[\nP(y|do(x)) \\le P(y, x) + 1 - P(x)\n\\]\n\\(do(z)\\)를 한 그래프에서도 Natural Bounds는 성립하므로, \\[\nP(y|do(x,z)) \\le P(y, x|do(z)) + 1 - P(x|do(z))\n\\]\n\n하지만 Natural Bounds는 이론적 한계까지 좁혀지지 않은 상태(Not Tight)입니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#모델-집합-정의-model-definition",
    "href": "posts/causal-inference-08-part-02/index.html#모델-집합-정의-model-definition",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "3.1 모델 집합 정의 (Model Definition)",
    "text": "3.1 모델 집합 정의 (Model Definition)\n\n주어진 그래프 구조와 호환되는 모든 구조적 인과 모델(SCM)의 집합을 \\(\\mathbf{M}\\)이라고 합시다.\n모델 \\(\\mathscr{M}\\)은 다음과 같이 정의됩니다.\n\n\\[\n\\mathscr{M} = \\begin{cases}\nU \\sim P(U) \\\\\nZ \\leftarrow f_Z(U_Z) \\\\\nX \\leftarrow f_X(Z, U) \\\\\nY \\leftarrow f_Y(X, U)\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#최적화-문제-optimization-problem",
    "href": "posts/causal-inference-08-part-02/index.html#최적화-문제-optimization-problem",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "3.2 최적화 문제 (Optimization Problem)",
    "text": "3.2 최적화 문제 (Optimization Problem)\n\n관측 데이터 \\(P(x, y|z)\\)가 주어졌을 때, 참(True) 인과 효과 \\(P(y|do(x))\\)는 다음 최적화 문제로 정의된 구간 \\([a(y; x), b(y; x)]\\) 내에 존재해야 합니다.\n\n\\[\na(y; x) = \\min_{\\mathscr{M} \\in \\mathbf{M} \\mid P_{\\mathscr{M}}(y,x|z) = P(x,y|z)} P_{\\mathscr{M}}(y|do(x))\n\\]\n\\[\nb(y; x) = \\max_{\\mathscr{M} \\in \\mathbf{M} \\mid P_{\\mathscr{M}}(y,x|z) = P(x,y|z)} P_{\\mathscr{M}}(y|do(x))\n\\]\n\n\\(\\mathscr{M} \\in \\mathbf{M} \\mid P_{\\mathscr{M}}(y,x|z) = P(x,y|z)\\)는 가상 모델(\\(\\mathscr{M}\\)) 중, 그 모델이 만들어내는 통계(\\(P_{\\mathscr{M}}\\))가 실제 관측된 데이터(\\(P\\))와 정확히 일치하는 모델만을 고려함을 의미합니다.\nDifficulty: 이 최적화 문제를 해결하는 것은 다음과 같은 이유로 어렵습니다.\n\n\n함수 집합 \\(\\mathbf{F}\\) (\\(f_X, f_Y\\) 등)의 모수적 형태(parametric form)가 주어지지 않았습니다.\n\n\n잠재 변수의 분포 \\(P(U)\\)를 알 수 없습니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#반응-변수-response-variables",
    "href": "posts/causal-inference-08-part-02/index.html#반응-변수-response-variables",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "4.1 반응 변수 (Response Variables)",
    "text": "4.1 반응 변수 (Response Variables)\n\n변수 \\(X\\)와 \\(Y\\)가 부모 변수의 값에 따라 어떻게 반응하는지를 결정하는 잠재적 특성을 \\(R_X, R_Y\\)라고 정의합시다.\n\n\n4.1.1 \\(R_X\\): 배정(\\(Z\\))에 대한 순응 유형\n\n\\(Z\\)가 \\(0\\) 또는 \\(1\\)일 때, \\(X\\)가 어떻게 반응하는지에 따라 총 4가지 유형(\\(2^2\\))이 존재합니다.\n\nNever-taker (\\(R_X=0\\)): 배정과 상관없이 처치를 받지 않음 (\\(Z=0 \\to X=0, Z=1 \\to X=0\\))\nComplier (\\(R_X=1\\)): 배정된 대로 따름 (\\(Z=0 \\to X=0, Z=1 \\to X=1\\))\nDefier (\\(R_X=2\\)): 배정과 반대로 행동함 (\\(Z=0 \\to X=1, Z=1 \\to X=0\\))\nAlways-taker (\\(R_X=3\\)): 배정과 상관없이 무조건 처치를 받음 (\\(Z=0 \\to X=1, Z=1 \\to X=1\\))\n\n\n\n\n4.1.2 \\(R_Y\\): 처치(\\(X\\))에 대한 반응 유형\n\n마찬가지로 \\(X\\)가 \\(0\\) 또는 \\(1\\)일 때, 결과 \\(Y\\)가 어떻게 나오는지에 따라 4가지 유형이 있습니다.\n\nNever-recover (\\(R_Y=0\\)): 약을 먹든 안 먹든 낫지 않음.\nHelped (\\(R_Y=1\\)): 약을 먹으면 낫고, 안 먹으면 안 나음 (인과 효과 있음).\nHurt (\\(R_Y=2\\)): 약을 먹으면 오히려 아프고, 안 먹어야 나음.\nAlways-recover (\\(R_Y=3\\)): 약을 먹든 안 먹든 항상 나음.\n\n\n\n\n\nFigure 1: Partition of U. U의 공간이 Rx와 Ry의 조합(총 16가지)에 의해 분할되는 모습을 나타낸 그림.\n\n\n\nNote: \\(U\\)는 \\(R_X\\)(4가지)와 \\(R_Y\\)(4가지)의 조합인 총 16개의 영역으로 나뉩니다.*\n\n\n\n4.1.3 동치 정리 (Equivalence Theorem)\n\n\n\nFigure 2: IV Model and Canonical Model. IV 모델에 대응되는 Canonical 모델의 모습.\n\n\n\nTheorem: 모든 IV 모델은 그에 상응하는 Canonical Type Model로 변환될 수 있으며, 동일한 관측 분포와 인과 효과를 가집니다.\n\n\nFor any IV model \\(\\mathscr{M}_{1}\\), there exists a canonical type model \\(\\mathscr{M}_{2}\\) such that \\(P_{\\mathscr{M}_{1}}(x,y|z) = P_{\\mathscr{M}_{2}}(x,y|z)\\), \\(P_{\\mathscr{M}_{1}}(y|do(x)) = P_{\\mathscr{M}_{2}}(y|do(x))\\)\n\n\n이 정리는 우리가 복잡하고 무한한 형태의 \\(U\\)를 고려할 필요 없이, 유한한 16개의 파라미터(\\(R_X, R_Y\\)의 결합 분포)만 최적화하면 된다는 것을 보장합니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#파라미터-정의-r_x-r_y-rightarrow-q_j-k",
    "href": "posts/causal-inference-08-part-02/index.html#파라미터-정의-r_x-r_y-rightarrow-q_j-k",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "5.1 파라미터 정의 (\\((R_{x}, R_{y}) \\rightarrow q_{j, k}\\))",
    "text": "5.1 파라미터 정의 (\\((R_{x}, R_{y}) \\rightarrow q_{j, k}\\))\n\n우리가 찾아야 할 미지수는 \\(R_X\\)와 \\(R_Y\\)의 결합 확률분포입니다.\n\n\\[\nq_{j,k} = P(R_X=j, R_Y=k), \\quad j,k \\in \\{0,1,2,3\\}\n\\]\n\n총 16개의 \\(q_{j,k}\\)가 있으며, 이들의 합은 1이어야 합니다.\n\n\\[\n\\sum_{j} \\sum_{k} q_{j,k} = 1\n\\]"
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#관측-데이터와의-연결-q_jk-rightarrow-px-x-y-y-z-z",
    "href": "posts/causal-inference-08-part-02/index.html#관측-데이터와의-연결-q_jk-rightarrow-px-x-y-y-z-z",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "5.2 관측 데이터와의 연결 (\\(q_{jk} \\rightarrow P(X = x, Y = y | Z = z)\\))",
    "text": "5.2 관측 데이터와의 연결 (\\(q_{jk} \\rightarrow P(X = x, Y = y | Z = z)\\))\n\n관측된 데이터 \\(P_{yx.z} = P(X=x, Y=y | Z=z)\\)는 \\(q_{j,k}\\)들의 선형 결합으로 표현됩니다.\n예를 들어, \\(P(Y=0, X=0 | Z=0)\\)인 경우를 봅시다.\n\n\\(Z=0\\)일 때 \\(X=0\\)이 되려면: Never-taker(\\(R_{x}=0\\)) 또는 Complier(\\(R_{x}=1\\))여야 합니다.\n\\(X=0\\)일 때 \\(Y=0\\)이 되려면: 처치를 안 받았을 때 안 나아야 하므로 Never-recover(\\(R_{y}=0\\)) 또는 Helped(\\(R_{y}=1\\))여야 합니다.\n\n따라서 해당 \\(q_{j,k}\\)들을 모두 더하면: \\[\nP_{00.0} = q_{00} + q_{01} + q_{10} + q_{11}\n\\]\n이와 같은 방식으로 8개의 관측 확률(\\(P_{yx.z}\\))을 모두 \\(q\\)에 대한 선형 방정식으로 만들 수 있습니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#인과-효과-q_j-k-rightarrow-py-dox",
    "href": "posts/causal-inference-08-part-02/index.html#인과-효과-q_j-k-rightarrow-py-dox",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "5.3 인과 효과 (\\(q_{j, k} \\rightarrow P(Y | do(x))\\))",
    "text": "5.3 인과 효과 (\\(q_{j, k} \\rightarrow P(Y | do(x))\\))\n\n우리가 알고 싶은 인과 효과 \\(P(Y=1 | do(X=1))\\) 또한 \\(q_{j,k}\\)의 합으로 표현됩니다.\n\n\\[\n\\begin{aligned}\nP(Y=1 | do(X=1)) &= P(R_Y \\in \\{\\text{Helped, Always-recover}\\}) \\\\\n&= P(R_Y=1) + P(R_Y=3) \\\\\n&= \\sum_{j=0}^3 (q_{j1} + q_{j3})\n\\end{aligned}\n\\]\n\n\\(do(x)\\)의 의미\n\n\\(do(X=x)\\)는 \\(R_X\\)의 성향을 무시하고(무력화하고) 강제로 처치를 가하는 것입니다.\n따라서 인과 효과를 계산할 때는 \\(R_X\\)에 대한 조건 없이, 오직 \\(Y\\)가 처치 \\(x\\)에 어떻게 반응하는지(\\(R_Y\\))만 고려하면 됩니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-02/index.html#해결-및-결과",
    "href": "posts/causal-inference-08-part-02/index.html#해결-및-결과",
    "title": "[Causal Inference] 8. Partial Identification (Part 2)",
    "section": "5.4 해결 및 결과",
    "text": "5.4 해결 및 결과\n\nCanonical Type Model을 도입함으로써, 우리는 무한한 차원의 문제를 16개의 변수(\\(q_{j,k}\\))를 가진 선형 계획법(Linear Programming) 문제로 변환했습니다.\n이를 구체적인 수식으로 살펴보겠습니다.\n\n\n5.4.1 최적화 문제 정의 (Optimization Setup)\n\n목표: 관측 데이터(\\(P\\))와 확률의 공리(\\(\\sum q = 1, q \\ge 0\\))를 만족하면서, 인과 효과를 최소화/최대화하는 \\(q\\)의 조합을 찾는 것입니다.\n\n\n5.4.1.1 목적 함수 (Objective Function):\n\n예를 들어, \\(P(Y=1 | do(X=0))\\)의 범위를 구한다고 가정해 봅시다.\n\\(X=0\\)으로 처치를 고정했을 때 \\(Y=1\\)이 나오는 반응 유형은 Hurt(\\(R_{y}=2\\))와 Always-recover(\\(R_{y}=3\\))입니다.\n따라서 목적 함수는 다음과 같습니다.\n\n\\[\n\\min_\\mathbf{q} \\sum_{j=0}^3 (q_{j2} + q_{j3}) \\quad \\text{또는} \\quad \\max_\\mathbf{q} \\sum_{j=0}^3 (q_{j2} + q_{j3})\n\\]\n\n\n5.4.1.2 제약 조건 (Constraints):\n\n관측확률에 대한 선형 등식: \\[\n\\begin{aligned}\np_{00.0} &= q_{00} + q_{01} + q_{10} + q_{11} \\\\\np_{00.1} &= q_{00} + q_{01} + q_{20} + q_{21} \\\\\n&\\vdots \\\\\np_{11.1} &= q_{12} + q_{13} + q_{32} + q_{33}\n\\end{aligned}\n\\]\n확률의 공리: \\(\\sum_{j,k} q_{j,k} = 1\\), \\(q_{j,k} \\ge 0\\)\n\n\n\n5.4.1.3 Closed-form Solution\n\n이 선형 계획법 문제를 심플렉스 알고리즘 등으로 풀면, Closed-form Solution를 얻을 수 있습니다.\n인과 효과 \\(P(Y=1 | do(X=0))\\)의 하한(Lower Bound)은 다음 값들 중 최댓값(\\(\\max\\))으로 결정됩니다.\n\n\\[\nP(Y=1 | do(X=0)) \\ge \\max \\begin{cases}\np_{10.0} \\\\\np_{10.1} \\\\\np_{10.0} + p_{11.0} - p_{00.1} - p_{11.1} \\\\\np_{01.0} + p_{10.0} - p_{00.1} - p_{01.1} \\\\\n\\end{cases}\n\\]\n\\[\nP(Y=1 | do(X=0)) \\le \\min \\begin{cases}\n1 - p_{00.0} \\\\\n1 - p_{00.1} \\\\\np_{10.0} + p_{11.0} + p_{01.1} + p_{10.1} \\\\\np_{01.0} + p_{10.0} + p_{10.1} + p_{11.1} \\\\\n\\end{cases}\n\\]\n\n\n\n5.4.2 결과의 의미\n\n위 수식은 복잡해 보이지만 중요한 함의를 갖습니다.\n\n\n데이터만으로 계산 가능: 우변의 모든 항(\\(p_{yx.z}\\))은 관측 데이터로부터 바로 구할 수 있는 값들입니다.\n\n\nTightness: 이 범위는 IV 모델의 가정(\\(Z \\perp U\\) 등)을 수학적으로 완벽하게 반영한 결과입니다. 따라서 추가적인 가정이 없다면 이보다 더 좁은 범위를 찾는 것은 불가능합니다.\n\n\nNote: Natural Bounds는 단순히 관측된 확률의 교집합만 고려하지만, Balke-Pearl Bounds(LP)는 도구 변수의 구조적 정보를 최적화 과정에 반영하여 훨씬 더 정보가 풍부한(Informative) 결과를 제공합니다.*"
  },
  {
    "objectID": "posts/causal-inference-08-part-04/index.html",
    "href": "posts/causal-inference-08-part-04/index.html",
    "title": "[Causal Inference] 8. Partial Identification (Part 4)",
    "section": "",
    "text": "이번 포스트에서는 실험 설계에서 흔히 발생하는 비순응(Non-compliance) 문제를 다룹니다.\n할당된 처치(\\(Z\\))와 실제 받은 처치(\\(X\\))가 일치하지 않는 상황에서, 인과 효과(ATE)를 식별하고 추정하는 다양한 방법들을 코드로 구현하고 비교해 봅니다.\n주요 내용은 다음과 같습니다.\n\n\n데이터 생성 (Data Generation): Compliance Type과 Response Type을 기반으로 가상 데이터 생성\n점 추정 (Point Estimate): 도구변수(IV)를 활용한 LATE 추정 (DoWhy 라이브러리 활용)\n구간 추정 (Interval Estimate):\n\nNatural Bounds (Manski): 가정 없이 관측 데이터만으로 도출한 구간\nIV Natural Bounds: 도구변수 가정을 추가했을 때의 구간\nBalke-Pearl Bounds (LP): 선형 계획법(Linear Programming)을 이용한 최적 구간\n\n결과 비교 및 시각화"
  },
  {
    "objectID": "posts/causal-inference-08-part-04/index.html#natural-bounds",
    "href": "posts/causal-inference-08-part-04/index.html#natural-bounds",
    "title": "[Causal Inference] 8. Partial Identification (Part 4)",
    "section": "5. Natural Bounds",
    "text": "5. Natural Bounds\n비교를 위해 더 넓은 구간을 가지는 Natural Bounds를 계산합니다.\n\nIV Natural Bounds: 도구변수가 있을 때의 최악의 경우(Worst-case) 구간\nPure Manski Bounds: 도구변수 없이 관측 데이터 \\(P(Y, X)\\)만으로 계산한 구간 (가장 넓음)\n\n\n\nCode\n# ==============================================================================\n# 4.1 Natural Bounds\n# ==============================================================================\ndef calculate_natural_bounds_iv(df):\n    p_xy_z = df.groupby(['Y', 'X', 'Z']).size() / df.groupby('Z').size()\n    \n    def get_p_xy_z(y, x, z):\n        return p_xy_z.get((y, x, z), 0)\n    \n    def get_p_x_z(x, z):\n        return get_p_xy_z(0, x, z) + get_p_xy_z(1, x, z)\n\n    # 1. P(Y=1 | do(X=1)) Bounds \n    # a = max_z P(y, x | z)\n    # b = min_z P(y, x | z) + 1 - P(x | z)\n    lower_do1 = max(get_p_xy_z(1, 1, 0), get_p_xy_z(1, 1, 1))\n    upper_do1 = min(get_p_xy_z(1, 1, 0) + 1 - get_p_x_z(1, 0), \n                    get_p_xy_z(1, 1, 1) + 1 - get_p_x_z(1, 1))\n    \n    # 2. P(Y=1 | do(X=0)) Bounds\n    lower_do0 = max(get_p_xy_z(1, 0, 0), get_p_xy_z(1, 0, 1))\n    upper_do0 = min(get_p_xy_z(1, 0, 0) + 1 - get_p_x_z(0, 0), \n                    get_p_xy_z(1, 0, 1) + 1 - get_p_x_z(0, 1))\n    \n    # 3. ATE Bounds (Worst Case)\n    # ATE = do(1) - do(0)\n    # Min ATE = Min(do1) - Max(do0)\n    # Max ATE = Max(do1) - Min(do0)\n    nat_min = lower_do1 - upper_do0\n    nat_max = upper_do1 - lower_do0\n    \n    return nat_min, nat_max\n\n# ==============================================================================\n# 4.2 Natural Bounds\n# ==============================================================================\ndef calculate_natural_bounds(df):\n    p_xy = df.groupby(['Y', 'X']).size() / len(df)\n    \n    def get_p_xy(y, x):\n        return p_xy.get((y, x), 0)\n\n    def get_p_x(x):\n        return get_p_xy(0, x) + get_p_xy(1, x)\n        \n    # 1. P(Y=1 | do(X=1)) Bounds \n    # a = P(y, x)\n    # b = P(y, x) + 1 - P(x)\n    lower_do1 = get_p_xy(1, 1)\n    upper_do1 = get_p_xy(1, 1) + 1 - get_p_x(1)\n    # 2. P(Y=1 | do(X=0)) Bounds \n    # a = P(y, x)\n    # b = P(y, x) + 1 - P(x)\n    lower_do0 = get_p_xy(1, 0)\n    upper_do0 = get_p_xy(1, 0) + 1 - get_p_x(0)\n    \n    # 3. ATE Bounds (Worst Case)\n    # ATE = do(1) - do(0)\n    # Min ATE = Min(do1) - Max(do0)\n    # Max ATE = Max(do1) - Min(do0)\n    nat_min = lower_do1 - upper_do0\n    nat_max = upper_do1 - lower_do0\n    \n    return nat_min, nat_max"
  },
  {
    "objectID": "posts/causal-inference-08-part-04/index.html#strong-iv-scenario",
    "href": "posts/causal-inference-08-part-04/index.html#strong-iv-scenario",
    "title": "[Causal Inference] 8. Partial Identification (Part 4)",
    "section": "7.1 Strong IV Scenario",
    "text": "7.1 Strong IV Scenario\n\n첫 번째는 Strong IV 상황입니다.\nComplier의 비율이 약 80%로 매우 높고, Never-taker나 Defier의 비율이 낮습니다.\n즉, 의사의 처방(Z)이 실제 복용 여부(X)를 매우 강력하게 결정합니다.\n\n\n\nCode\n# 1. Data Generation\nprobs = np.array([\n    # Ry=0(Never) Ry=1(Helped) Ry=2(Hurt) Ry=3(Always)\n    # -------------------------------------------------------\n    [0.05,        0.05,        0.00,      0.00],   # Never-taker (Rx=0)\n    [0.10,        0.60,        0.05,      0.05],   # Complier (Rx=1)\n    [0.00,        0.00,        0.00,      0.00],   # Defier (Rx=2)\n    [0.05,        0.05,        0.00,      0.00]    # Always-taker (Rx=3)\n])\n\ndf, true_ate, probs = generate_class_scenario_data(probs) \n\n# 2. Point Estimate\npoint_est = point_estimate(df)\n\n# 3. Interval Estimate (LP)\nlp_min, lp_max = lp_bounds(df)\n\n# 4. Natural Bounds\nnat_min, nat_max = calculate_natural_bounds_iv(df)\nmanski_min, manski_max = calculate_natural_bounds(df)\n\n# 5. Visualization\ncompare_and_visualize(\n    df=df, \n    true_ate=true_ate, \n    point_est=point_est, \n    lp_bounds=(lp_min, lp_max), \n    nat_bounds=(nat_min, nat_max), \n    manski_bounds=(manski_min, manski_max),\n    probs_matrix=probs\n)\n\n\n\n\n\n\n\n\n\nTrue ATE      : 0.6500\nDoWhy Estimate: 0.6853\n------------------------------\n1. Pure Manski (No Z) : [-0.1769, 0.8231] (Width: 1.0000)\n2. IV Natural (With Z) : [0.4991, 0.6972] (Width: 0.1981)\n3. Balke-Pearl (LP)   : [0.4991, 0.6972] (Width: 0.1981)\n\n\n\n해석\n\nComplier 비중: 약 80%로, 도구변수 \\(Z\\)가 \\(X\\)를 강력하게 설명합니다.\nLP Bounds: 너비가 약 0.20으로 매우 좁게 형성되었습니다. 이는 관측 데이터만으로도 True ATE의 범위를 상당히 좁힐 수 있음을 의미합니다.\nEstimates: Point Estimate (LATE)가 True ATE에 매우 근접해 있습니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-04/index.html#weak-iv-scenario",
    "href": "posts/causal-inference-08-part-04/index.html#weak-iv-scenario",
    "title": "[Causal Inference] 8. Partial Identification (Part 4)",
    "section": "7.2 Weak IV Scenario",
    "text": "7.2 Weak IV Scenario\n\n두 번째는 Weak IV 상황입니다.\nComplier의 비율이 5%로 극히 낮고, Never-taker(약 35%)와 Always-taker(약 40%)의 비율이 매우 높습니다.\n즉, 의사가 처방을 하든 말든(Z), 사람들은 대부분 자기 맘대로 행동하므로 도구변수가 무력합니다.\n\n\n\nCode\n# 1. Data Generation\nprobs = np.array([\n    # Ry=0(Never) Ry=1(Helped) Ry=2(Hurt) Ry=3(Always)\n    # -------------------------------------------------------\n    [0.10,        0.10,        0.05,      0.10],   # Never-taker (Rx=0)\n    [0.01,        0.02,        0.01,      0.01],   # Complier (Rx=1)\n    [0.05,        0.05,        0.05,      0.05],   # Defier (Rx=2)\n    [0.10,        0.10,        0.10,      0.10]    # Always-taker (Rx=3)\n])\n\ndf, true_ate, probs = generate_class_scenario_data(probs) \n\n# 2. Point Estimate\npoint_est = point_estimate(df)\n\n# 3. Interval Estimate (LP)\nlp_min, lp_max = lp_bounds(df)\n\n# 4. Natural Bounds\nnat_min, nat_max = calculate_natural_bounds_iv(df)\nmanski_min, manski_max = calculate_natural_bounds(df)\n\n# 5. Visualization\ncompare_and_visualize(\n    df=df, \n    true_ate=true_ate, \n    point_est=point_est, \n    lp_bounds=(lp_min, lp_max), \n    nat_bounds=(nat_min, nat_max), \n    manski_bounds=(manski_min, manski_max),\n    probs_matrix=probs\n)\n\n\n\n\n\n\n\n\n\nTrue ATE      : 0.0600\nDoWhy Estimate: -0.1393\n------------------------------\n1. Pure Manski (No Z) : [-0.4769, 0.5231] (Width: 1.0000)\n2. IV Natural (With Z) : [-0.4082, 0.4322] (Width: 0.8404)\n3. Balke-Pearl (LP)   : [-0.4082, 0.4322] (Width: 0.8404)\n\n\n\n해석\n\nComplier 비중: 5%에 불과하여 \\(Z\\)와 \\(X\\)의 상관관계가 매우 약합니다.\nLP Bounds: 너비가 0.84로 매우 넓습니다. 이는 1.0(전체 가능한 범위)에 가까운 수치로, 도구변수 가정을 도입했음에도 불구하고 ATE에 대해 “거의 아무것도 모르는 상태”와 다를 바 없습니다.\nEstimates: Point Estimate (LATE)가 True ATE와 큰 차이를 보이고, 부호가 다르다. 이는 분모(\\(E[X∣Z=1]−E[X∣Z=0]\\))가 0에 가까워지면서 추정량이 불안정해지기 때문입니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html",
    "href": "posts/causal-inference-04-part-01/index.html",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "",
    "text": "이번 포스트에서는 인과추론에서 가장 핵심적인 개념 중 하나인 교란(Confounding)과 이를 해결하기 위한 백도어 기준(Back-door Criterion)으로 나아가기 전, 식별 가능성(Identification Problem)에 대해 다시 한번 짚어보고자 합니다.\n강의 자료는 서울대학교 데이터사이언스대학원 이상학 교수님의 “Confounding and Backdoor” 수업 자료를 바탕으로 합니다\n\n\n\nRecap(Identification Problem) and Example of Identifiable and Non-identifiable Effects\nConfounding Bias\nBack-door Criterion\nEvaluation"
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html#목차",
    "href": "posts/causal-inference-04-part-01/index.html#목차",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "",
    "text": "Recap(Identification Problem) and Example of Identifiable and Non-identifiable Effects\nConfounding Bias\nBack-door Criterion\nEvaluation"
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html#설정-setup",
    "href": "posts/causal-inference-04-part-01/index.html#설정-setup",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "4.1 설정 (Setup)",
    "text": "4.1 설정 (Setup)\n아래와 같은 “Bow-tie” 형태의 그래프를 고려합니다. 여기서 \\(U_{XY}\\)는 \\(X\\)와 \\(Y\\)에 동시에 영향을 주는 관측되지 않은 교란 변수(Unobserved Confounder)입니다.\n\n\n\nFigure 3. Causal Graph with Unobserved Confounder\n\n\n두 모델 \\(\\mathcal{H}^{(1)}\\)과 \\(\\mathcal{H}^{(2)}\\)는 다음과 같은 구조적 방정식(Structural Equations)을 갖습니다. 여기서 \\(U_Y, U_{XY}\\)는 모두 베르누이 분포(동전 던지기, \\(p=0.5\\))를 따릅니다.\n\n4.1.1 Model 1 (\\(\\mathcal{H}^{(1)}\\))\n\\[\n\\begin{cases}\nX \\leftarrow U_{XY} \\\\\nY \\leftarrow (X \\oplus U_{XY}) \\lor U_Y\n\\end{cases}\n\\] * 여기서 \\(\\oplus\\)는 XOR 연산, \\(\\lor\\)는 OR 연산을 의미합니다.\n\n\n4.1.2 Model 2 (\\(\\mathcal{H}^{(2)}\\))\n\\[\n\\begin{cases}\nX \\leftarrow U_{XY} \\\\\nY \\leftarrow U_Y\n\\end{cases}\n\\] * 모델 2에서는 \\(X\\)가 \\(Y\\)에 아무런 영향을 주지 않습니다 (끊어진 인과 관계)."
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html#관측-분포의-일치-observational-equivalence",
    "href": "posts/causal-inference-04-part-01/index.html#관측-분포의-일치-observational-equivalence",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "4.2 관측 분포의 일치 (Observational Equivalence)",
    "text": "4.2 관측 분포의 일치 (Observational Equivalence)\n놀랍게도 두 모델은 관측 데이터 상으로는 완벽하게 동일합니다.\n\nModel 1의 경우: 구조 방정식을 보면 \\(X\\)는 \\(U_{XY}\\)와 같습니다. 따라서 \\(X \\oplus U_{XY}\\)는 항상 \\(0\\)이 됩니다 (\\(X\\)와 \\(U_{XY}\\)가 같으므로). \\[Y = 0 \\lor U_Y = U_Y\\] 결국 관측 환경에서는 \\(Y\\)가 오직 \\(U_Y\\)에 의해서만 결정됩니다\nModel 2의 경우: 정의상 \\(Y \\leftarrow U_Y\\)이므로, 역시 \\(Y\\)는 \\(U_Y\\)에 의해 결정됩니다.\n\n결과적으로 두 모델 모두 \\(P(Y=1) = P(U_Y=1) = 0.5\\)이며, \\(X\\)와 \\(Y\\)의 결합 분포 표(Truth Table)도 완전히 일치합니다\n\n\n\n\n\n\n\n\n\n\n\\(U_Y\\)\n\\(U_{XY}\\) (\\(=X\\))\n\\(Y^{(1)}\\) (Model 1)\n\\(Y^{(2)}\\) (Model 2)\n\\(P(u)\\)\n\n\n\n\n0\n0\n0\n0\n1/4\n\n\n0\n1\n0\n0\n1/4\n\n\n1\n0\n1\n1\n1/4\n\n\n1\n1\n1\n1\n1/4"
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html#인과-효과의-불일치-interventional-differences",
    "href": "posts/causal-inference-04-part-01/index.html#인과-효과의-불일치-interventional-differences",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "4.3 인과 효과의 불일치 (Interventional Differences)",
    "text": "4.3 인과 효과의 불일치 (Interventional Differences)\n이제 \\(do(x)\\) 연산을 통해 실제로 인과 효과를 계산해 보면 두 모델의 차이가 드러납니다. \\(do(x)\\)는 \\(X\\)를 강제로 특정 값으로 고정하는 것이므로, 더 이상 \\(X\\)는 \\(U_{XY}\\)의 영향을 받지 않습니다.\n\n\n\nFigure 4. Causal Graph with Unobserved Confounder\n\n\n우리는 \\(P(y=1 | do(x=0))\\)을 계산해 보겠습니다\n\n4.3.1 Case 1: Model \\(\\mathcal{H}^{(1)}\\)\n구조 방정식: \\(Y \\leftarrow (x \\oplus U_{XY}) \\lor U_Y\\). 여기서 \\(x=0\\)으로 고정했습니다.\n\n\\(U_{XY}=0, U_Y=0 \\Rightarrow Y = (0 \\oplus 0) \\lor 0 = 0\\)\n\\(U_{XY}=0, U_Y=1 \\Rightarrow Y = (0 \\oplus 0) \\lor 1 = 1\\) (\\(U_Y=1\\) 이면 어차피 \\(Y=1\\))\n\\(U_{XY}=1, U_Y=0 \\Rightarrow Y = (0 \\oplus 1) \\lor 0 = 1\\) (\\(U_{XY}=1 \\nRightarrow X=1\\) 이므로, 여기서 차이 발생!)\n\\(U_{XY}=1, U_Y=1 \\Rightarrow Y = (0 \\oplus 1) \\lor 1 = 1\\) (\\(U_Y=1\\) 이면 어차피 \\(Y=1\\))\n\n4가지 경우 중 3가지 경우에 \\(Y=1\\)이 됩니다. \\[\\therefore P^{(1)}(y=1|do(x=0)) = \\frac{3}{4}\\]\n\n\n4.3.2 Case 2: Model \\(\\mathcal{H}^{(2)}\\)\n구조 방정식: \\(Y \\leftarrow U_Y\\). \\(X\\)의 값과 상관없이 \\(Y\\)는 \\(U_Y\\)를 따릅니다.\n\n\\(U_Y=0 \\Rightarrow Y=0\\)\n\\(U_Y=1 \\Rightarrow Y=1\\)\n\n\\[\\therefore P^{(2)}(y=1|do(x=0)) = \\frac{1}{2}\\]"
  },
  {
    "objectID": "posts/causal-inference-04-part-01/index.html#결론",
    "href": "posts/causal-inference-04-part-01/index.html#결론",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 1)",
    "section": "4.4 결론",
    "text": "4.4 결론\n\n\n\n\\(Y\\)\n\\(P^{(1)}(Y|do(x))\\)\n\\(P^{(2)}(Y|do(x))\\)\n\n\n\n\n0\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{2}\\)\n\n\n1\n\\(\\frac{3}{4}\\)\n\\(\\frac{1}{2}\\)\n\n\n\n두 모델은 동일한 인과 그래프와 동일한 관측 분포 \\(P(v)\\)를 가지지만, 계산된 인과 효과 \\(P(y|do(x))\\)는 \\(\\frac{3}{4}\\)와 \\(\\frac{1}{2}\\)로 서로 다릅니다\n\nEven though both models induce \\(I\\) (Graph) and have the same \\(P(v)\\), the effect \\(P^{(1)}(y|do(x)) \\neq P^{(2)}(y|do(x))\\).\n\n이는 주어진 그래프 구조(Unobserved Confounder가 존재하는 경우)만으로는 데이터에서 인과 효과를 유일하게 식별해낼 수 없음을(Non-identifiable) 의미합니다. 이러한 경우, 추가적인 가정이나 데이터를 통한 식별 전략이 필요합니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-05/index.html",
    "href": "posts/causal-inference-04-part-05/index.html",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 5)",
    "section": "",
    "text": "이전 포스트들에서 배운 교란(Confounding)과 Back-door Criterion을 실제 코드로 구현해 보는 시간입니다.\n특히 심슨의 역설(Simpson’s Paradox) 상황을 시뮬레이션하고, DoWhy 라이브러리의 성향 점수 층화(Propensity Score Stratification) 방법을 통해 올바른 인과 효과를 추정해 봅니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-05/index.html#모델-정의-및-식별-modeling-identification",
    "href": "posts/causal-inference-04-part-05/index.html#모델-정의-및-식별-modeling-identification",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 5)",
    "section": "5.1 모델 정의 및 식별 (Modeling & Identification)",
    "text": "5.1 모델 정의 및 식별 (Modeling & Identification)\n\n인과 그래프(Graph)를 정의하고 Back-door 기준을 통해 식별 가능성을 확인합니다.\n\n\n\nCode\n# 단계 1: 모델 정의 (Causal Graph 생성)\nmodel = CausalModel(\n    data=df,\n    treatment='Exercise',\n    outcome='Cholesterol',\n    common_causes=['Age'] # 교란 변수 지정\n)\n\n# 단계 2: 식별 (Identification)\nidentified_estimand = model.identify_effect()\nprint(identified_estimand)\n\n\nEstimand type: EstimandType.NONPARAMETRIC_ATE\n\n### Estimand : 1\nEstimand name: backdoor\nEstimand expression:\n     d                         \n───────────(E[Cholesterol|Age])\nd[Exercise]                    \nEstimand assumption 1, Unconfoundedness: If U→{Exercise} and U→Cholesterol then P(Cholesterol|Exercise,Age,U) = P(Cholesterol|Exercise,Age)\n\n### Estimand : 2\nEstimand name: iv\nNo such variable(s) found!\n\n### Estimand : 3\nEstimand name: frontdoor\nNo such variable(s) found!"
  },
  {
    "objectID": "posts/causal-inference-04-part-05/index.html#인과-효과-추정-estimation-stratification",
    "href": "posts/causal-inference-04-part-05/index.html#인과-효과-추정-estimation-stratification",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 5)",
    "section": "5.2 인과 효과 추정 (Estimation: Stratification)",
    "text": "5.2 인과 효과 추정 (Estimation: Stratification)\n\n여기서는 성향 점수 층화(Propensity Score Stratification) 방법을 사용합니다. 나이가 비슷하여 운동할 확률(성향 점수)이 유사한 사람들끼리 묶어서 비교하는 방식입니다.\n\n\n\nCode\n# 단계 3: 추정 (Estimation)\n# num_strata=5: 데이터를 성향 점수에 따라 5개 구간으로 나눔\nestimate = model.estimate_effect(\n    identified_estimand,\n    method_name=\"backdoor.propensity_score_stratification\",\n    method_params={'num_strata': 5, 'clipping_threshold': 5}\n)\n\nprint(\"=\"*50)\nprint(f\"2. 인과 효과 추정 (Causal Estimate): {estimate.value:.4f}\")\nprint(f\"   -&gt; 실제 효과 (-10.0)에 매우 근접함\")\nprint(\"=\"*50)\n\n\n==================================================\n2. 인과 효과 추정 (Causal Estimate): -9.1264\n   -&gt; 실제 효과 (-10.0)에 매우 근접함\n==================================================\n\n\n/opt/anaconda3/envs/causal-inference-study/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: divide by zero encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n/opt/anaconda3/envs/causal-inference-study/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: overflow encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n/opt/anaconda3/envs/causal-inference-study/lib/python3.9/site-packages/sklearn/linear_model/_linear_loss.py:330: RuntimeWarning: invalid value encountered in matmul\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights"
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html",
    "href": "posts/causal-inference-04-part-03/index.html",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "",
    "text": "지난 포스트에서는 교란 편향(Confounding Bias)이 인과 효과 추정을 방해하는 주된 요인임을 살펴보았습니다.\n그렇다면 우리는 복잡한 인과 그래프(DAG)에서 어떤 변수들의 집합(\\(Z\\))을 조절(Control/Adjustment)해야 이 편향을 제거할 수 있을까요?\n이번 포스트에서는 그 해답이 되는 Back-door Criterion(백도어 기준)의 정의와 이를 이용한 보정 공식을 다룹니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#definition",
    "href": "posts/causal-inference-04-part-03/index.html#definition",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "2.1 Definition",
    "text": "2.1 Definition\n\n주어진 인과 그래프 \\(G\\)에서 변수 \\(X\\)와 \\(Y\\)에 대해, 변수들의 집합 \\(Z\\)가 다음 두 가지 조건을 만족할 때, \\(Z\\)는 Back-door Criterion을 만족한다고 정의합니다.\n\n\n(i) No node in \\(Z\\) is a descendant of \\(X\\). (\\(Z\\)에 속한 어떤 변수도 \\(X\\)의 자손이 아니어야 합니다.)\n(ii) \\(Z\\) blocks every path between \\(X\\) and \\(Y\\) that contains an arrow into \\(X\\). (\\(Z\\)는 \\(X\\)로 들어오는 화살표를 포함하는 \\(X\\)와 \\(Y\\) 사이의 모든 경로를 차단해야 합니다.)"
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#의미-해석",
    "href": "posts/causal-inference-04-part-03/index.html#의미-해석",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "2.2 의미 해석",
    "text": "2.2 의미 해석\n\n조건 (i)은 \\(X\\)의 결과(Effect)로 나타나는 변수를 통제하지 말라는 뜻입니다. \\(X\\)의 자손을 통제하면 \\(X\\)가 \\(Y\\)에 미치는 실제 인과 경로를 막아버리거나 새로운 편향을 만들 수 있기 때문입니다.\n조건 (ii)의 “arrow into \\(X\\)”는 소위 백도어 경로(Back-door Path)를 의미합니다. 이는 \\(X\\)의 원인이 되는 변수들에 의해 생성되는 비인과적 상관관계(Spurious Correlation)의 경로입니다. 이 경로를 차단함으로써 우리는 \\(X\\)에서 \\(Y\\)로 나가는 순수한 인과적 경로만 남길 수 있습니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#분석-포인트",
    "href": "posts/causal-inference-04-part-03/index.html#분석-포인트",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "5.1 분석 포인트",
    "text": "5.1 분석 포인트\n\n이 그래프에서 \\(X\\)와 \\(Y\\) 사이의 백도어 경로를 차단해야 합니다. \\(X\\)에서 나가는 화살표를 지운 그래프(\\(G_{\\underline{X}}\\))를 상상했을 때 \\(X\\)와 \\(Y\\)가 d-separation 되는지 확인하는 것과 같습니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#가능한-z-집합의-예시",
    "href": "posts/causal-inference-04-part-03/index.html#가능한-z-집합의-예시",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "5.2 가능한 \\(Z\\) 집합의 예시",
    "text": "5.2 가능한 \\(Z\\) 집합의 예시\n\n\\(Z = \\{Z_4, Z_2\\}\\)\n\\(Z = \\{Z_4, Z_5\\}\\)\n\\(Z = \\{Z_4, Z_2, Z_5\\}\\)"
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#주의할-점-collider-z_4",
    "href": "posts/causal-inference-04-part-03/index.html#주의할-점-collider-z_4",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "5.3 주의할 점: Collider (\\(Z_4\\))",
    "text": "5.3 주의할 점: Collider (\\(Z_4\\))\n\n위 그래프에서 \\(Z_4\\)는 \\(Z_1 \\to Z_4 \\leftarrow Z_2\\) 구조를 갖는 Collider입니다.\n만약 \\(Z_4\\)를 집합에 포함하지 않으면, \\(Z_1 - Z_4 - Z_2\\) 경로는 자연스럽게 막혀(Blocked) 있습니다.\n하지만 \\(Z_4\\)를 집합에 포함시켜 조건부로 잡으면(Conditioning), 오히려 \\(Z_1\\)과 \\(Z_2\\) 사이의 경로가 열리게 됩니다\n따라서 \\(Z_4\\)를 조정 변수로 사용할 때는, 이로 인해 열리는 다른 경로(\\(Z_1 \\dots Z_2\\))를 막아줄 추가적인 변수(\\(Z_2\\) 등)를 함께 포함해야 합니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#case-z-emptyset-no-confounding",
    "href": "posts/causal-inference-04-part-03/index.html#case-z-emptyset-no-confounding",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "5.4 Case: \\(Z = \\emptyset\\) (No Confounding)",
    "text": "5.4 Case: \\(Z = \\emptyset\\) (No Confounding)\n\n만약 \\(X\\)로 들어오는 백도어 경로가 아예 없는 그래프라면 어떨까요?\n\n\n\n\nFigure 3. Case where no adjustment is needed\n\n\n\n이 경우 공집합 \\(Z = \\emptyset\\)도 Back-door Criterion을 만족합니다.\n즉, 별도의 조정 없이 관측된 조건부 확률이 곧 인과 효과가 됩니다.\n\n\\[P(y|x) = P(y|do(x))\\]\n\n이것이 바로 “교란이 없으면 상관관계는 인과관계이다(Correlation is Causation)”가 성립하는 유일한 순간입니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#recap-adjustment-by-direct-parents",
    "href": "posts/causal-inference-04-part-03/index.html#recap-adjustment-by-direct-parents",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "6.1 Recap: Adjustment by Direct Parents",
    "text": "6.1 Recap: Adjustment by Direct Parents\n\n일반적으로 \\(X\\)의 모든 직접적인 부모(Direct Parents, \\(Pa_X\\))를 통제하면 Back-door Criterion을 항상 만족합니다.\n\n\\[P(y|do(x)) = \\sum_{pa_X} P(y | x, pa_X) P(pa_X)\\]\n\n하지만 모든 부모 변수를 관측하는 것은 현실적으로 어려울 수 있습니다.\n그렇다면, 부모가 아닌 Back-door Criterion을 만족하는 임의의 집합 \\(Z\\)를 통제해도 왜 동일한 결과가 나올까요?\n아래 증명은 Direct Parents Adjustment 공식에서 시작하여 Back-door Adjustment 공식으로 유도되는 과정을 보여줍니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#가정-assumptions",
    "href": "posts/causal-inference-04-part-03/index.html#가정-assumptions",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "6.2 가정 (Assumptions)",
    "text": "6.2 가정 (Assumptions)\n\n집합 \\(Z\\)가 Back-door criterion을 만족한다고 가정합시다.\nLet \\(Z^- = Z \\setminus Pa_X\\) (부모 변수를 제외한 나머지 \\(Z\\)의 요소들).\n\n\nCondition (i): \\(Z\\)의 어떤 노드도 \\(X\\)의 자손이 아님. \\[\\Rightarrow X \\perp (Z \\setminus Pa_X) \\mid Pa_X\\] \\[\\Rightarrow P(z^{-} | x, pa_{X}) = P(z^{-} | pa_{X})\\]\n\n“미래는 과거를 바꿀 수 없다”는 원리입니다.\n인과 그래프에서 부모(\\(Pa_{X}\\))와 \\(Z\\)는 \\(X\\)보다 먼저 일어난 일(또는 원인)이고, \\(X\\)는 그 결과입니다.\n이미 부모(\\(Pa_{X}\\))의 상태를 다 알고 있다면, 결과인 \\(X\\)가 무엇이든 간에 그 원인이나 별개의 사건인 \\(Z\\)의 확률은 변하지 않습니다.\n\nCondition (ii): \\(Z\\)는 \\(X\\)로 들어오는 모든 뒷문 경로를 차단함. \\[\\Rightarrow Y \\perp (Pa_X \\setminus Z) \\mid Z, X\\] \\[\\Rightarrow P(y | x, pa_{X}, z^{-}) = P(y | x, z)\\] (\\(Z\\)와 \\(X\\)가 주어졌을 때, \\(Y\\)는 \\(Z\\)에 포함되지 않은 나머지 부모들과 독립이다)\n\n“Z가 이미 충분한 정보를 담고 있다”는 원리입니다.\n원래 \\(Y\\)를 예측하려면 교란 요인인 모든 부모들(\\(Pa_{X}\\))을 다 봐야 합니다. 하지만 \\(Z\\)가 ’백도어 기준’을 만족한다는 것은, \\(Z\\)가 부모들이 \\(Y\\)에 미치는 교란을 대신해서 다 막아주고 있다는 뜻입니다.\n따라서 일단 \\(Z\\)를 알고 나면, 굳이 \\(Z\\)에 포함되지 않은 나머지 부모들(\\(Pa_{X}\\))을 추가로 더 안다고 해서 \\(Y\\)에 대한 예측이 달라지지 않습니다. \\(Z\\)가 그 역할을 완벽히 대체했기 때문입니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-03/index.html#증명-derivation",
    "href": "posts/causal-inference-04-part-03/index.html#증명-derivation",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 3)",
    "section": "6.3 증명 (Derivation)",
    "text": "6.3 증명 (Derivation)\n\n부모 변수를 통한 조정(Adjustment by Direct Parents) 공식에서 시작합니다.\n\n\\[\n\\begin{aligned}\nP(y|do(x)) &= \\sum_{pa_X} P(y|x, pa_X)P(pa_X) \\\\\n&= \\sum_{z^-, pa_X} P(y|x, pa_X, z^-)P(z^-|x, pa_X)P(pa_X) && \\because \\text{Expand by } Z^- \\text{ (Total Probability)} \\\\\n&= \\sum_{z^-, pa_X} P(y|x, z)P(z^-|pa_X)P(pa_X) && \\because \\text{Apply Assumptions Condition (i) \\& (ii)} \\\\\n&= \\sum_{z^-, pa_X} P(y|x, z)P(z^-, pa_X) && \\because \\text{Chain Rule } P(A|B)P(B) = P(A,B) \\\\\n&= \\sum_{z} P(y|x, z) \\sum_{pa_X \\setminus z} P(z^-, pa_X) && \\because \\text{Rearrange Summation } (Z \\cup (Pa_X \\setminus Z) = \\{Z, Pa_X\\}) \\\\\n&= \\sum_{z} P(y|x, z)P(z) && \\because \\text{Marginalize out } Pa_X \\setminus Z\n\\end{aligned}\n\\]\n\n결론: \n\n\nAdjustment by \\(Z\\) is equivalent to adjustment by direct parents whenever \\(Z\\) is back-door admissible.\n\n\n즉, \\(Z\\)가 Back-door criterion만 만족한다면, 직접적인 부모를 모두 측정하지 못하더라도 \\(Z\\)를 통해 인과 효과를 정확히 계산할 수 있음이 수학적으로 증명됩니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-04/index.html",
    "href": "posts/causal-inference-04-part-04/index.html",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 4)",
    "section": "",
    "text": "지난 포스트에서는 Back-door Criterion을 통해 어떤 공변량 집합 \\(Z\\)를 보정(Adjustment)해야 인과 효과 \\(P(y|do(x))\\)를 식별할 수 있는지 알아보았습니다.\n\n\\[P(y|do(x)) = \\sum_{z} P(y|x,z)P(z)\\]\n\n이번 포스트에서는 이 식을 실제 데이터(Practice)에서 어떻게 평가하고 계산하는지, 특히 계산 복잡도 문제를 해결하기 위한 Inverse Probability Weighting (IPW) 기법에 대해 다룹니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-04/index.html#derivation",
    "href": "posts/causal-inference-04-part-04/index.html#derivation",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 4)",
    "section": "3.1 Derivation",
    "text": "3.1 Derivation\n\n기존의 Back-door Adjustment 식을 베이즈 정리와 결합 확률 법칙을 이용해 재구성해 봅시다.\n\n\\[\n\\begin{align}\nP(y|do(x)) &= \\sum_{z} P(y|x,z)P(z) \\\\\n&= \\sum_{z} \\frac{P(y,x,z)}{P(x,z)} P(z) \\\\\n&= \\sum_{z} \\frac{P(y,x,z)}{P(x|z)P(z)} P(z) \\\\\n&= \\sum_{z} \\frac{P(y,x,z)}{P(x|z)} \\quad\n\\end{align}\n\\]\n\n이제 우리는 \\(P(z)\\)를 따로 추정하거나 모든 \\(z\\)에 대해 합을 구할 필요 없이, 결합 확률 \\(P(y,x,z)\\)를 \\(P(x|z)\\)로 나눈 형태로 식을 단순화했습니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-04/index.html#propensity-score-gz",
    "href": "posts/causal-inference-04-part-04/index.html#propensity-score-gz",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 4)",
    "section": "3.2 Propensity Score \\(g(z)\\)",
    "text": "3.2 Propensity Score \\(g(z)\\)\n\n위 식의 분모에 있는 \\(P(x|z)\\)는 공변량 \\(Z\\)가 주어졌을 때 원인 변수 \\(X\\)가 할당될 확률을 의미하며, 이를 Propensity Score라고 부릅니다.\n보통 \\(g(z)\\)로 표기하며 로지스틱 회귀(Logistic Regression) 등의 모델을 사용하여 추정합니다.\n\n\\[g(z) = P(X=x|Z=z)\\]"
  },
  {
    "objectID": "posts/causal-inference-04-part-04/index.html#핵심-포인트",
    "href": "posts/causal-inference-04-part-04/index.html#핵심-포인트",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 4)",
    "section": "4.1 핵심 포인트",
    "text": "4.1 핵심 포인트\n\n\\(\\mathbb{1}(\\cdot)\\) (Indicator Function): 조건이 참이면 1, 거짓이면 0을 반환하는 함수입니다.\n시간 복잡도 \\(O(N)\\): 가장 마지막 식을 보면, 더 이상 \\(Z\\)의 모든 조합에 대해 합을 구할 필요가 없습니다. 단순히 관측된 \\(N\\)개의 데이터 샘플을 한 번씩만 순회하며 가중치(\\(1/g(z_i)\\))를 더하면 계산이 끝납니다. 이것이 IPW가 강력한 이유입니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-04/index.html#시간-복잡도의-개선",
    "href": "posts/causal-inference-04-part-04/index.html#시간-복잡도의-개선",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 4)",
    "section": "4.2 시간 복잡도의 개선",
    "text": "4.2 시간 복잡도의 개선\n\n이 방식의 가장 큰 장점은 계산 효율성입니다.\n고차원 \\(Z\\)에 대해 적분하거나 합을 구하는 대신, 샘플 수 \\(N\\)에 비례하는 선형 시간(\\(O(N)\\)) 만에 계산이 가능합니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html",
    "href": "posts/causal-inference-04-part-02/index.html",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "",
    "text": "지난 포스트에서는 인과 효과의 식별 가능성(Identifiability)에 대해 다루었습니다. 이번에는 현실 데이터 분석에서 가장 빈번하게 마주치는 문제이자, 인과추론이 필요한 가장 큰 이유 중 하나인 교란 편향(Confounding Bias)에 대해 구체적인 예시를 통해 알아보겠습니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html#단순-관측-직관과-반대되는-결과",
    "href": "posts/causal-inference-04-part-02/index.html#단순-관측-직관과-반대되는-결과",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "2.1 1. 단순 관측: 직관과 반대되는 결과",
    "text": "2.1 1. 단순 관측: 직관과 반대되는 결과\n\n수집한 데이터를 단순히 산점도(Scatter Plot)로 그려보았더니 놀라운 결과가 나타났습니다.\n\n\n\n\nFigure 1. Observed Correlation between Exercise and Cholesterol\n\n\n\n관측 결과: 운동을 많이 하는 사람일수록 콜레스테롤 수치가 더 높게 나타납니다.\n의문: \\(P(\\text{cholesterol} | \\text{exercise})\\)를 보면, 운동이 콜레스테롤을 높이는 것처럼 보입니다. 과연 “더 많은 운동 \\(\\Rightarrow\\) 콜레스테롤 증가”라고 결론 내릴 수 있을까요?"
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html#교란-변수의-발견-나이age",
    "href": "posts/causal-inference-04-part-02/index.html#교란-변수의-발견-나이age",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "2.2 2. 교란 변수의 발견: 나이(Age)",
    "text": "2.2 2. 교란 변수의 발견: 나이(Age)\n\n이 이상한 현상을 이해하기 위해, 우리는 데이터에 숨겨진 제3의 변수인 ‘나이(Age)’를 색상으로 구분하여 다시 시각화해보았습니다.\n\n\n\n\nFigure 2. Scatter Plot Stratified by Age\n\n\n\n그래프를 자세히 보면 다음과 같은 패턴이 드러납니다:\n\n나이와 운동: 나이가 많은 사람(노란색 계열)들이 건강 관리를 위해 운동을 더 많이 하는 경향이 있습니다.\n나이와 콜레스테롤: 동시에, 나이가 많을수록 자연적으로 콜레스테롤 수치가 높습니다.\n\n즉, ‘나이’가 운동량(\\(X\\))과 콜레스테롤(\\(Y\\)) 모두에 영향을 미치는 교란 변수(Confounder)로 작용하고 있었던 것입니다."
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html#계층별-분석-인과-효과의-확인",
    "href": "posts/causal-inference-04-part-02/index.html#계층별-분석-인과-효과의-확인",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "2.3 3. 계층별 분석: 인과 효과의 확인",
    "text": "2.3 3. 계층별 분석: 인과 효과의 확인\n\n이제 나이(Age)를 고정한 상태에서 운동과 콜레스테롤의 관계를 다시 살펴봅시다.\n\n\n\n\nFigure 3. True Causal Effect within Age Groups\n\n\n\n각 연령대 그룹(같은 색깔) 내부를 들여다보면, 운동을 많이 할수록 콜레스테롤 수치가 낮아지는 것을 명확히 볼 수 있습니다.\n결론: 운동의 진짜 인과 효과는 콜레스테롤을 낮추는 것입니다. (\\(More\\ exercise \\Rightarrow Lower\\ Cholesterol\\))"
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html#복잡한-인과-그래프에서의-문제",
    "href": "posts/causal-inference-04-part-02/index.html#복잡한-인과-그래프에서의-문제",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "4.1 복잡한 인과 그래프에서의 문제",
    "text": "4.1 복잡한 인과 그래프에서의 문제\n\n다음과 같이 복잡한 인과 그래프를 고려해 봅시다.\n\n\n\n\nFigure 4. A Complex Causal Graph\n\n\n\nGoal: 변수 \\(Z_1, ..., Z_k\\)가 측정되었을 때, \\(X\\)가 \\(Y\\)에 미치는 인과 효과 \\(Q = P(y|do(x))\\)를 찾아내는 것.\n여기서 중요한 질문이 생깁니다.\n\n“부모 변수(Parents) 중 일부만 관측되었을 때, 타겟 인과 효과 \\(Q\\)를 식별할 수 있는가?”"
  },
  {
    "objectID": "posts/causal-inference-04-part-02/index.html#주의해야-할-구조-collider",
    "href": "posts/causal-inference-04-part-02/index.html#주의해야-할-구조-collider",
    "title": "[Causal Inference] 4. Confounding and Backdoor (Part 2)",
    "section": "4.2 주의해야 할 구조: Collider",
    "text": "4.2 주의해야 할 구조: Collider\n\n그래프 분석 시 주의해야 할 점은 단순히 모든 변수를 통제(Conditioning)한다고 좋은 것이 아니라는 점입니다.\n예를 들어 위 그래프에서 \\(Z_4\\)와 같은 변수를 살펴봅시다.\n\n\nNote: \\(Z_4\\)는 \\(Z_1\\)과 \\(Z_2\\)의 화살표를 동시에 받는 Collider (\\(Z_1 \\rightarrow Z_4 \\leftarrow Z_2\\))입니다. \\(Z_4\\)를 조건부(given)로 잡을 경우, 오히려 \\(Z_1\\)과 \\(Z_2\\) 사이에 길이 뚫리는 현상이 발생하여 새로운 편향을 만들 수 있습니다.\n\n\n따라서 우리는 어떤 변수를 조정 집합(Adjustment Set)에 넣어야 편향을 제거하고 올바른 인과 효과를 구할 수 있는지 판단하는 체계적인 기준이 필요합니다. 이것이 바로 다음 포스트에서 다룰 Back-door Criterion입니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-01/index.html",
    "href": "posts/causal-inference-08-part-01/index.html",
    "title": "[Causal Inference] 8. Partial Identification (Part 1)",
    "section": "",
    "text": "이전까지 우리는 인과 효과 \\(P(y|do(x))\\)를 관측 데이터 \\(P(v)\\)로부터 정확한 하나의 값(Point estimate)으로 계산할 수 있는지, 즉 식별 가능성(Identifiability)을 따졌습니다.\n하지만 현실의 많은 문제에서는 그래프 구조상 식별이 불가능한 경우(Non-identifiable)가 많습니다.\n이때 우리는 포기하는 대신 “그렇다면 인과 효과가 존재할 수 있는 범위(Bound)라도 알 수 없을까?”라는 질문을 던지게 됩니다.\n이것이 바로 부분 식별(Partial Identification) 문제입니다.\n\n\n\n\nFigure 1: Identification Process Flowchart. 식별 엔진이 ’No’를 반환했을 때, 구간 [a, b]를 찾는 과정으로 넘어가는 흐름도.\n\n\n\nNote: 식별 불가능(No) 판정이 났을 때, 우리는 0과 1 사이의 어딘가에 존재하는 구간 \\([a(y;x), b(y;x)]\\)를 찾게 됩니다. *"
  },
  {
    "objectID": "posts/causal-inference-08-part-01/index.html#문제-설정",
    "href": "posts/causal-inference-08-part-01/index.html#문제-설정",
    "title": "[Causal Inference] 8. Partial Identification (Part 1)",
    "section": "3.1 문제 설정",
    "text": "3.1 문제 설정\n다음과 같이 \\(X\\)와 \\(Y\\) 사이에 비관측 교란 변수 \\(U\\)가 존재하는 가장 일반적인 그래프를 가정해 봅시다.\n\n\n\nFigure 2: Basic Confounded Graph. X와 Y가 U에 의해 교란된 구조.\n\n\n\n우리의 목표는 \\(P(x, y)\\)가 주어졌을 때, \\(P(y|do(x))\\)의 범위를 구하는 것입니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-01/index.html#theorem",
    "href": "posts/causal-inference-08-part-01/index.html#theorem",
    "title": "[Causal Inference] 8. Partial Identification (Part 1)",
    "section": "3.2 Theorem",
    "text": "3.2 Theorem\n\n관측 분포 \\(P(x, y)\\)가 주어졌을 때, 인과 효과 \\(P(y|do(x))\\)는 다음 구간 안에 존재합니다.\n\n\\[\nP(y, x) \\le P(y|do(x)) \\le P(y, x) + 1 - P(x)\n\\]\n\n하한 (Lower Bound): \\(a(y;x) = P(y, x)\\)\n상한 (Upper Bound): \\(b(y;x) = P(y, x) + 1 - P(x)\\)\n\n직관적 해석\n\n이 수식은 데이터를 통해 “확실히 아는 것”과 “모르는 것”을 구분하는 것으로 이해할 수 있습니다.\n우리가 \\(do(x)\\)를 했을 때, 원래 자연스럽게 \\(X=x\\)를 선택했던 사람들은 관측 데이터 \\(P(y, x)\\)와 똑같이 행동할 것입니다. 이는 최소한 보장되는 비율입니다 (하한).\n반면, 원래 \\(X \\ne x\\)였던 사람들(처치를 받지 않은 비율 \\(P(x')\\))이 강제로 처치를 받았을 때 어떻게 행동할지는 데이터로 알 수 없습니다.\n상한의 논리: “처치 받지 않은 비율(\\(P(x')\\))에 속하는 사람들이 강제로 처치를 받았을 때, 모두가 \\(Y=y\\)로 변화한다(성공한다)”고 가장 긍정적으로 가정하면 최댓값이 됩니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-01/index.html#유도-과정-derivation",
    "href": "posts/causal-inference-08-part-01/index.html#유도-과정-derivation",
    "title": "[Causal Inference] 8. Partial Identification (Part 1)",
    "section": "3.3 유도 과정 (Derivation)",
    "text": "3.3 유도 과정 (Derivation)\n\n왜 저런 범위가 나오는지 수식으로 살펴봅시다. 구조적 인과 모델(SCM)에서 \\(P(y|do(x))\\)는 다음과 같이 정의됩니다.\n\n\\[\nP(y|do(x)) = \\sum_u P(y|x, u)P(u)\n\\]\n\n확률의 성질을 이용하여 \\(P(u)\\)를 두 부분으로 나눌 수 있습니다: \\(P(u) = P(u, x) + \\{P(u) - P(u, x)\\}\\). 이를 식에 대입하면:\n\n\\[\n\\begin{aligned}\nP(y|do(x)) &= \\sum_u P(y|x, u) [P(u, x) + \\{P(u) - P(u, x)\\}] \\\\\n&= \\underbrace{\\sum_u P(y|x, u)P(u, x)}_{\\text{Part A}} + \\underbrace{\\sum_u P(y|x, u)\\{P(u) - P(u, x)\\}}_{\\text{Part B}}\n\\end{aligned}\n\\]\n\n여기서 Part A를 정리하면:\n\n\\[\n\\text{Part A} = \\sum_u P(y|x, u)P(x|u)P(u) = \\sum_u P(y, x, u) = P(y, x)\n\\]\n\n즉, Part A는 우리가 관측 가능한 데이터 \\(P(y, x)\\)와 같습니다. 이제 Part B 때문에 범위가 생깁니다.\n\n\n3.3.1 하한 (Lower Bound) 유도\n\n\\(P(y|x, u)\\)는 확률이므로 \\(0 \\le P(y|x, u) \\le 1\\).\n\\(P(u) = \\sum_x P(u, x)\\)이므로 \\(P(u) \\ge P(u, x)\\).\n\n\\[\n\\begin{aligned}\nP(y|do(x)) &= \\sum_u P(y|x, u)P(u, x) + \\sum_u P(y|x, u)\\{P(u) - P(u, x)\\} \\\\\n&= P(y,x) + \\sum_u P(y|x, u)\\{P(u) - P(u, x)\\} \\\\\n&\\ge P(y, x) + 0 = P(y, x) \\\\\n\\end{aligned}\n\\]\n\n\n3.3.2 상한 (Upper Bound) 유도\n\n\\(P(y|x, u)\\)는 확률이므로 \\(0 \\le P(y|x, u) \\le 1\\).\n\nPart B의 조건부 확률이 모두 1일 때 전체 값은 최대가 됩니다. \\[\n\\begin{aligned}\nP(y|do(x)) &= \\sum_u P(y|x, u)P(u, x) + \\sum_u P(y|x, u)\\{P(u) - P(u, x)\\} \\\\\n&= P(y,x) + \\sum_u P(y|x, u)\\{P(u) - P(u, x)\\} \\\\\n&\\le P(y, x) + \\sum_u \\{P(u) - P(u, x)\\} \\\\\n&= P(y, x) + 1 - P(x)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/causal-inference-08-part-01/index.html#범위의-타당성-증명-tightness",
    "href": "posts/causal-inference-08-part-01/index.html#범위의-타당성-증명-tightness",
    "title": "[Causal Inference] 8. Partial Identification (Part 1)",
    "section": "3.4 범위의 타당성 증명 (Tightness)",
    "text": "3.4 범위의 타당성 증명 (Tightness)\n\n구한 범위 \\([P(y, x), P(y, x) + 1 - P(x)]\\)가 Tight하다는 것은, 추가적인 가정 없이는 이 범위를 더 좁힐 수 없음을 의미합니다.\n이를 증명하기 위해, 관측 데이터 \\(P(x, y)\\)와 완벽히 일치하면서(Compatible), 인과 효과가 각각 하한값과 상한값을 갖는 두 개의 가상 모델(\\(M^{(1)}, M^{(2)}\\))을 만들어낼 수 있음을 보이면 됩니다.\n\n\n3.4.1 모델 설계 전략\n\n두 모델 모두 \\(U \\sim P(u)\\)와 \\(X \\leftarrow f_X(u)\\)는 동일하게 두고, \\(Y\\)를 결정하는 함수 \\(f_Y(x, u)\\)만 다르게 설정합니다.\n\n\\[\nf_Y(x, u) =\n\\begin{cases}\nf_Y^{\\text{observation}}(x, u) & \\text{if } x = f_X(u) \\quad \\text{(관측된 경우)} \\\\\nf_Y^{\\text{counterfactual}}(x, u) & \\text{if } x \\ne f_X(u) \\quad \\text{(반사실적 경우)}\n\\end{cases}\n\\]\n\n\n3.4.2 Case 분류\n\n우리는 상황을 두 가지 케이스로 나눌 수 있습니다.\n\nCase 1 (Observation): \\(x = f_X(u)\\).\n\n즉, 개인이 원래 \\(x\\)를 선택하려던 경우입니다.\n이 경우 모델은 관측 데이터와 일치해야 하므로 선택의 여지가 없습니다.\n\nCase 2 (Counterfactual): \\(x \\ne f_X(u)\\).\n\n즉, 원래 다른 것을 하려던 사람에게 억지로 \\(x\\)를 시킬 때입니다.\n이 부분은 데이터로 관측되지 않으므로, 우리가 임의로 0(실패) 또는 1(성공)을 부여하여 범위를 만듭니다.\n\n\n\n\n\n3.4.3 두 모델의 구성\n\n모델 \\(\\mathcal{M}_x^{(1)}\\) (하한 모델):\n\nCase 2일 때 무조건 \\(Y=0\\)을 출력하게 설정합니다.\n인과 효과: \\(P^{(1)}(y=1|do(x)) = P(y=1, x) + 0 = \\text{Lower Bound}\\)\n\n모델 \\(\\mathcal{M}_x^{(2)}\\) (상한 모델):\n\nCase 2일 때 무조건 \\(Y=1\\)을 출력하게 설정합니다.\n인과 효과: \\(P^{(2)}(y=1|do(x)) = P(y=1, x) + 1- P(x) = \\text{Upper Bound}\\)\n\n\n\n두 모델 모두 관측 상황(Case 1)에서는 동일하게 작동하므로 \\(P(x, y)\\) 분포는 같습니다.\n하지만 \\(do(x)\\) 개입 시(Case 2), \\(U\\)와 \\(X\\)가 일치할 필요가 없으므로 \\(\\mathcal{M}_x^{(1)}\\)와 \\(\\mathcal{M}_x^{(2)}\\)가 \\(Y = 1\\) 일 때, 다른 확률을 가질 수 있습니다.\n\n\\[\n\\begin{aligned}\nP^{(i)}(Y = 1|do(x)) &= \\sum_{u} P^{(i)}(Y = 1|do(x), u)P(u|do(x)) \\\\\n&= \\sum_{u} P^{(i)}(Y = 1|x, u)P(u) && \\because \\text{Rule 2, Rule 3}\\\\\n&= P^{(i)}(Y = 1|x, f_X(U) = x)P(f_X(U) = x) \\\\\n&\\quad + P^{(i)}(Y = 1|x, f_X(U) \\neq x)P(f_X(U) \\neq x) \\\\\n&= P^{(i)}(Y = 1|x)P(x) + \\mathbf{1}[i = 2]\\{1 - P(x)\\} \\\\\n&= P^{(i)}(x, Y = 1) + \\mathbf{1}[i = 2](1 - P(x))\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nP^{(1)}(Y = 1|do(x)) &= a(y;x) = P(y = 1, x) \\quad \\text{while} \\\\\nP^{(2)}(Y = 1|do(x)) &= b(y;x) = P(y = 1, x) + 1 - P(x)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/causal-inference-01/index.html",
    "href": "posts/causal-inference-01/index.html",
    "title": "[Causal Inference] 1. Introduction",
    "section": "",
    "text": "데이터사이언스대학원(GSDS) 이상학 교수님의 “데이터사이언스를 위한 인과추론” 강의 1강을 정리한 포스트입니다. 왜 우리가 머신러닝을 넘어 인과추론을 배워야 하는지, 그리고 그 핵심 프레임워크인 Judea Pearl의 인과 계층에 대해 다룹니다."
  },
  {
    "objectID": "posts/causal-inference-01/index.html#상관관계correlation는-인과관계causation가-아니다",
    "href": "posts/causal-inference-01/index.html#상관관계correlation는-인과관계causation가-아니다",
    "title": "[Causal Inference] 1. Introduction",
    "section": "1. 상관관계(Correlation)는 인과관계(Causation)가 아니다",
    "text": "1. 상관관계(Correlation)는 인과관계(Causation)가 아니다\n통계학 수업을 들었다면 누구나 한 번쯤 들어봤을 문장입니다. 하지만 실제 데이터 분석에서 우리는 이 함정에 너무나 쉽게 빠집니다. 강의 자료에 나온 재미있는 예시들을 살펴봅시다.\n\n(1) 자폐증과 유기농 식품 판매량\n미국 내 자폐증(Autism) 진단 수와 유기농 식품 판매량의 추이를 그린 그래프입니다.\n\n\n\nAutism vs Organic Food Sales\n\n\n두 변수의 상관계수(\\(r\\))는 무려 0.9971입니다. 그렇다면 유기농 식품이 자폐증의 원인일까요? 당연히 아닙니다. 단순히 시간이 흐르며 두 지표가 같이 상승했을 뿐입니다. 이를 허위 상관(Spurious Correlation)이라고 합니다.\n\n\n(2) 소방관이 많으면 불이 커진다?\n데이터를 보면 투입된 소방관의 수(\\(X\\))와 화재의 크기(\\(Y\\))는 아주 강한 양의 상관관계를 가집니다.\n\\[Y = \\beta X + \\beta_0\\]\n단순 회귀분석을 돌리면 \\(\\beta\\)(기울기)는 양수가 나옵니다. 그렇다면 화재 피해를 줄이기 위해 소방관을 줄여야 할까요? 상식적으로 말이 안 됩니다. 사실은 화재가 크기 때문에(Cause) 소방관이 많이 출동한 것(Effect)이거나, 건물의 크기 같은 제3의 요인이 작용했을 것입니다. 데이터(\\(X, Y\\))만 봐서는 이 인과의 방향을 알 수 없습니다."
  },
  {
    "objectID": "posts/causal-inference-01/index.html#심슨의-역설-simpsons-paradox-데이터는-거짓말을-한다",
    "href": "posts/causal-inference-01/index.html#심슨의-역설-simpsons-paradox-데이터는-거짓말을-한다",
    "title": "[Causal Inference] 1. Introduction",
    "section": "2. 심슨의 역설 (Simpson’s Paradox): 데이터는 거짓말을 한다",
    "text": "2. 심슨의 역설 (Simpson’s Paradox): 데이터는 거짓말을 한다\n가장 충격적이었던 예시는 심슨의 역설입니다. 어떤 신약(Drug)의 회복률(Recovery Rate)을 분석한 데이터입니다.\n\n상황 1: 전체 데이터 (Total)\n\n\n\n\n회복(Y)\n미회복(\\(\\neg Y\\))\n회복률(Rate)\n\n\n\n\n약 복용 (Drug)\n20\n20\n50%\n\n\n미복용 (No Drug)\n16\n24\n40%\n\n\n\n전체로 보면 약을 먹었을 때 회복률이 더 높습니다(50% &gt; 40%). 의사는 약을 처방해야 할 것 같습니다.\n\n\n상황 2: 성별로 나누어 본 데이터 (Stratified by Sex)\n하지만 남성과 여성으로 데이터를 쪼개서 보면 이야기가 달라집니다.\n\n남성 (Male): 약 복용(60%) &lt; 미복용(70%)\n여성 (Female): 약 복용(20%) &lt; 미복용(30%)\n\n남성 그룹에서도, 여성 그룹에서도 약을 안 먹는 게 회복률이 더 높습니다. 전체 데이터에서는 약이 좋다고 하는데, 쪼개보니 약이 나쁘다고 합니다. 도대체 의사는 어떤 테이블을 믿어야 할까요?\n\n\n결론: 인과 그래프(Causal Graph)가 필요하다\n정답은 “데이터만으로는 알 수 없다”입니다. 성별(\\(F\\)), 약 복용(\\(X\\)), 회복(\\(Y\\)) 사이의 인과 구조가 어떻게 그려지느냐에 따라 우리가 봐야 할 테이블이 달라집니다. 이것이 바로 우리가 단순히 데이터(\\(P(Y|X)\\))를 보는 것을 넘어, 인과 구조(Structure)를 고민해야 하는 이유입니다."
  },
  {
    "objectID": "posts/causal-inference-01/index.html#pearl의-인과-계층-pearls-causal-hierarchy",
    "href": "posts/causal-inference-01/index.html#pearl의-인과-계층-pearls-causal-hierarchy",
    "title": "[Causal Inference] 1. Introduction",
    "section": "3. Pearl의 인과 계층 (Pearl’s Causal Hierarchy)",
    "text": "3. Pearl의 인과 계층 (Pearl’s Causal Hierarchy)\nJudea Pearl은 인과추론의 수준을 세 단계의 사다리(Ladder)로 정의했습니다.\n\nLevel 1: 관찰 (Association / Seeing)\n\n질문: “What is?” (만약 \\(X\\)를 본다면, \\(Y\\)는 어떨까?)\n수식: \\(P(y|x)\\)\n특징: 기존의 머신러닝(Deep Learning, Decision Tree 등)이 가장 잘하는 영역입니다. 데이터의 상관성을 파악합니다.\n\n\n\nLevel 2: 개입 (Intervention / Doing)\n\n질문: “What if I do?” (만약 내가 \\(X\\)를 강제로 시킨다면, \\(Y\\)는 어떻게 될까?)\n수식: \\(P(y | do(x))\\)\n특징: 여기서부터 진짜 인과추론의 영역입니다. 단순히 \\(X\\)를 관찰하는 것(\\(x\\))과, 실험자가 개입하여 값을 바꾸는 것(\\(do(x)\\))은 다릅니다. 강화학습(RL)이나 A/B 테스트가 여기에 속합니다.\n\n\n\nLevel 3: 반사실 (Counterfactuals / Imagining)\n\n질문: “What if I had acted differently?” (그때 내가 다른 선택을 했더라면, 결과는 달라졌을까?)\n수식: \\(P(y_x | x', y')\\)\n특징: 이미 일어난 일(\\(x', y'\\))을 바탕으로, 일어나지 않은 가상의 세계(\\(y_x\\))를 추론합니다. 인간만이 가진 고도의 추론 능력인 회고(Retrospection)와 상상(Imagining)의 영역입니다."
  },
  {
    "objectID": "posts/causal-inference-01/index.html#마치며",
    "href": "posts/causal-inference-01/index.html#마치며",
    "title": "[Causal Inference] 1. Introduction",
    "section": "4. 마치며",
    "text": "4. 마치며\n기존의 머신러닝은 Level 1 (Association)에 머물러 있었습니다. 하지만 진정한 지능(Intelligent Agent)이나 정책 결정(Policy Making)을 위해서는 Level 2, 3의 추론이 필수적입니다. 이번 강의를 통해 단순히 데이터를 예측(Prediction)하는 것을 넘어, 세상의 작동 원리(Mechanism)를 이해하는 인과추론의 세계로 나아가고자 합니다.\nReference: * Prof. Sanghack Lee, Causal Inference for Data Science, GSDS SNU. * Judea Pearl et al., Causal Inference in Statistics: A Primer."
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html",
    "href": "posts/causal-inference-08-part-03/index.html",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "",
    "text": "지난 포스트들에서 우리는 Natural Bounds와 도구 변수(IV)를 활용한 식별 범위 추정에 대해 알아보았습니다.\n이번 포스트에서는 Partial Identification 시리즈의 마지막으로, Backdoor 기준을 만족하는 변수들이 부분적으로만 관측될 때(Partially-observed) 어떻게 인과 효과의 범위를 구할 수 있는지 살펴봅니다.\n이는 데이터가 서로 다른 소스에서 수집되어 결합 분포(Joint Distribution)를 완전히 알 수 없는 현실적인 상황(예: \\(X, Y\\) 데이터와 \\(U\\) 데이터가 따로 있을 때)에서 매우 유용한 접근법입니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html#목표-식-재구성",
    "href": "posts/causal-inference-08-part-03/index.html#목표-식-재구성",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "4.1 목표 식 재구성",
    "text": "4.1 목표 식 재구성\n\nBackdoor 공식을 조건부 확률 정의에 따라 풀어쓰면 다음과 같습니다.\n\n\\[\n\\begin{aligned}\nP(y|do(x)) &= \\sum_{w, u} P(y|x, w, u)P(w, u) \\\\\n&= \\sum_{w, u} \\frac{P(x, y, w, u)P(w, u)}{P(x, w, u)}\n\\end{aligned}\n\\]\n\n이 식의 구성 요소들을 각각 미지수로 정의하여 최적화 문제를 설계합니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html#파라미터-정의",
    "href": "posts/causal-inference-08-part-03/index.html#파라미터-정의",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "4.2 파라미터 정의",
    "text": "4.2 파라미터 정의\n\n각 \\((w, u)\\) 조합에 대해 다음 세 가지 변수를 정의합니다.\n\n\n\\(a_{w,u} = P(x, y, w, u)\\): 전체 결합 확률의 일부\n\n\n\\(b_{w,u} = P(w, u)\\): 교란 변수들의 결합 확률\n\n\n\\(c_{w,u} = P(x, w, u)\\): 처치와 교란 변수들의 결합 확률\n\n\n이제 우리의 목표 함수(Objective Function)는 다음과 같습니다.\n\n\\[\n\\text{min / max } \\sum_{w, u} \\frac{a_{w,u} \\cdot b_{w,u}}{c_{w,u}}\n\\]"
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html#제약-조건-constraints",
    "href": "posts/causal-inference-08-part-03/index.html#제약-조건-constraints",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "4.3 제약 조건 (Constraints)",
    "text": "4.3 제약 조건 (Constraints)\n\n우리가 관측한 데이터(\\(P(X,Y,W)\\)와 \\(P(U)\\))는 이 미지수들의 합으로 표현되어야 합니다. 이것이 최적화 문제의 제약 조건이 됩니다.\n\n관측 데이터와의 일치:\n\n\n\\(\\sum_u a_{w,u} = P(x, y, w)\\)\n\\(\\sum_u b_{w,u} = P(w)\\)\n\\(\\sum_u c_{w,u} = P(x, w)\\)\n\n\n확률의 기본 성질 (Fréchet Inequalities):\n\n\n\\(P(w, u) = \\sum_x P(x, w, u) = \\sum_x \\sum_y P(x, y, w, u)\\)이므로 \\[b_{w, u} \\ge c_{w,u} \\ge a_{w, u}\\]\n결합 확률은 개별 확률보다 클 수 없고, 합집합 확률보다 작을 수 없다는 성질을 이용해 각 파라미터의 범위를 제한합니다.\n\\(a_{w,u} = P(x, y, w, u)\\)의 경우, \\[\\max\\{0, P(x, y, w) + P(u) - 1\\} \\le a_{w,u} \\le \\min\\{P(x, y, w), P(u)\\}\\]\n\\(b_{w,u} = P(w, u)\\)의 경우, \\[\\max\\{0, P(w) + P(u) - 1\\} \\le b_{w,u} \\le \\min\\{P(w), P(u)\\}\\]\n\\(c_{w, u} = P(x, w, u)\\)의 경우, \\[\\max\\{0, P(x, w) + P(u) - 1\\} \\le c_{w,u} \\le \\min\\{P(x, w), P(u)\\}\\]\n\n이러한 제약 조건 하에서 목표 함수를 최적화하면, Natural Bounds보다 좁은 범위(Valid Bounds)를 얻을 수 있습니다."
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html#요약",
    "href": "posts/causal-inference-08-part-03/index.html#요약",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "5.1 요약",
    "text": "5.1 요약\n\n\nNatural Bounds: 그래프 구조에 대한 최소한의 가정만으로 구하는 가장 넓은 범위.\n\n\nCanonical Type Model (IV): 도구 변수가 있을 때, 비관측 변수 \\(U\\)를 유한한 반응 유형(Response Types)으로 나누어 선형 계획법(LP)으로 해결.\n\n\nPartially-observed Covariates: 데이터가 파편화되어 있을 때, 확률 분포의 제약 조건을 이용한 최적화 문제로 해결."
  },
  {
    "objectID": "posts/causal-inference-08-part-03/index.html#추가적인-연구-주제들",
    "href": "posts/causal-inference-08-part-03/index.html#추가적인-연구-주제들",
    "title": "[Causal Inference] 8. Partial Identification (Part 3)",
    "section": "5.2 추가적인 연구 주제들",
    "text": "5.2 추가적인 연구 주제들\n\nPartial Identification은 여전히 활발히 연구되고 있는 분야입니다. 더 깊이 있는 학습을 위해 다음 논문들을 참고할 수 있습니다.\n\n\n연속 변수(Continuous variables)에서의 Bound: Zhang and Bareinboim (2022)\n\n\nHigh Dimensional Data: Li and Pearl (2022)\n\n\n비순응(Non-Compliance) 분석: Balke and Pearl (1997), Chickering and Pearl (1996)\n\n\nNote: 더 복잡한 구조(예: 또 다른 Canonical Type Model)에서도 유사한 방식의 접근이 가능합니다.\n\n\n식별 불가능하다고 해서 분석을 멈추는 것이 아니라, “데이터가 허용하는 한계 내에서 최선의 정보”를 뽑아내는 것이 바로 Partial Identification의 핵심입니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "shsha0110.github.io",
    "section": "",
    "text": "[Causal Inference] 8. Partial Identification (Part 4)\n\n\n\nCausal Inference\n\n\n\nNon-Compliance Simulation\n\n\n\n\n\nJan 13, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 8. Partial Identification (Part 3)\n\n\n\nCausal Inference\n\n\n\nBounds on Partially-observed Covariates\n\n\n\n\n\nJan 13, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 8. Partial Identification (Part 2)\n\n\n\nCausal Inference\n\n\n\nPartial Identification Problem: Non-Compliance\n\n\n\n\n\nJan 12, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 8. Partial Identification (Part 1)\n\n\n\nCausal Inference\n\n\n\nPartial Identification Problem: Natural Bounds\n\n\n\n\n\nJan 11, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 4. Confounding and Backdoor (Part 5)\n\n\n\nCausal Inference\n\n\n\nSimpson’s Paradox Simulation Using DoWhy: Stratification & Refutation\n\n\n\n\n\nJan 8, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 4. Confounding and Backdoor (Part 4)\n\n\n\nCausal Inference\n\n\n\nInverse Probability Weighting\n\n\n\n\n\nJan 7, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 4. Confounding and Backdoor (Part 3)\n\n\n\nCausal Inference\n\n\n\nBack-door Criterion\n\n\n\n\n\nJan 7, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 4. Confounding and Backdoor (Part 2)\n\n\n\nCausal Inference\n\n\n\nCounfounding Bias\n\n\n\n\n\nJan 7, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 4. Confounding and Backdoor (Part 1)\n\n\n\nCausal Inference\n\n\n\nIndentifiable and Non-identifiable Effects\n\n\n\n\n\nJan 7, 2026\n\n\n유성현\n\n\n\n\n\n\n\n\n\n\n\n\n[Causal Inference] 1. Introduction\n\n\n\nCausal Inference\n\n\n\nIntroduction to Causal Inference\n\n\n\n\n\nJan 6, 2026\n\n\n유성현\n\n\n\n\n\nNo matching items"
  }
]